{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b21a3f5",
   "metadata": {},
   "source": [
    "## Categorizing into simple, medium and hard categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b9dc95de",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'sampled_100_entries.yml'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01myaml\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Load the already sampled 100 entries\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msampled_100_entries.yml\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m      5\u001b[0m     sampled_data \u001b[38;5;241m=\u001b[39m yaml\u001b[38;5;241m.\u001b[39msafe_load(f)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Prepare YAML block scalar formatting\u001b[39;00m\n",
      "File \u001b[0;32m/tmp/conda_envs/hf_env/lib/python3.9/site-packages/IPython/core/interactiveshell.py:310\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    303\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    304\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    305\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    306\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    307\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    308\u001b[0m     )\n\u001b[0;32m--> 310\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'sampled_100_entries.yml'"
     ]
    }
   ],
   "source": [
    "import yaml\n",
    "\n",
    "# Load the already sampled 100 entries\n",
    "with open(\"sampled_100_entries.yml\", \"r\") as f:\n",
    "    sampled_data = yaml.safe_load(f)\n",
    "\n",
    "# Prepare YAML block scalar formatting\n",
    "class LiteralStr(str): pass\n",
    "\n",
    "def str_presenter(dumper, data):\n",
    "    return dumper.represent_scalar('tag:yaml.org,2002:str', data, style='|')\n",
    "\n",
    "yaml.add_representer(LiteralStr, str_presenter)\n",
    "\n",
    "# Create buckets\n",
    "simple, medium, complex = [], [], []\n",
    "\n",
    "# Classify by code line count\n",
    "for entry in sampled_data:\n",
    "    code = entry.get(\"code\", \"\")\n",
    "    formatted_code = code.replace(\"\\\\n\", \"\\n\").replace(\"\\\\\\\\\", \"\\\\\")\n",
    "    entry[\"code\"] = LiteralStr(formatted_code)\n",
    "\n",
    "    line_count = formatted_code.count(\"\\n\") + 1\n",
    "    if line_count <= 9:\n",
    "        simple.append(entry)\n",
    "    elif line_count <= 13:\n",
    "        medium.append(entry)\n",
    "    else:\n",
    "        complex.append(entry)\n",
    "\n",
    "# Save categorized datasets\n",
    "with open(\"simple.yml\", \"w\") as f:\n",
    "    yaml.dump(simple, f, sort_keys=False)\n",
    "\n",
    "with open(\"medium.yml\", \"w\") as f:\n",
    "    yaml.dump(medium, f, sort_keys=False)\n",
    "\n",
    "with open(\"complex.yml\", \"w\") as f:\n",
    "    yaml.dump(complex, f, sort_keys=False)\n",
    "\n",
    "print(\"Categorized into simple.yml, medium.yml, and complex.yml.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6866f99f",
   "metadata": {},
   "source": [
    "### lengths of files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94c2b91c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simple complexity: 35 code blocks\n",
      "Medium complexity: 35 code blocks\n",
      "Complex complexity: 30 code blocks\n"
     ]
    }
   ],
   "source": [
    "import yaml\n",
    "\n",
    "# Filenames\n",
    "files = {\n",
    "    \"Simple\": \"simple.yml\",\n",
    "    \"Medium\": \"medium.yml\",\n",
    "    \"Complex\": \"complex.yml\"\n",
    "}\n",
    "\n",
    "# Count entries in each file\n",
    "for label, filename in files.items():\n",
    "    with open(filename, \"r\") as f:\n",
    "        data = yaml.safe_load(f)\n",
    "        print(f\"{label} complexity: {len(data)} code blocks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb34c32c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simple codes: 35 code blocks\n",
      "Medium codes: 35 code blocks\n",
      "Complex codes: 30 code blocks\n"
     ]
    }
   ],
   "source": [
    "import yaml\n",
    "\n",
    "# Filenames\n",
    "files = {\n",
    "    \"Simple\": \"simple@1/simple@1_variations.yml\",\n",
    "    \"Medium\": \"medium@1/medium@1_variations.yml\",\n",
    "    \"Complex\": \"complex@1/complex@1_variations.yml\"\n",
    "}\n",
    "\n",
    "# Count entries in each file\n",
    "for label, filename in files.items():\n",
    "    with open(filename, \"r\") as f:\n",
    "        data = yaml.safe_load(f)\n",
    "        print(f\"{label} codes: {len(data)} code blocks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84520a5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simple codes: 35 code blocks\n",
      "Medium codes: 35 code blocks\n",
      "Complex codes: 30 code blocks\n"
     ]
    }
   ],
   "source": [
    "import yaml\n",
    "\n",
    "# Filenames\n",
    "files = {\n",
    "    \"Simple\": \"simple@1_variations_gpt.yml\",\n",
    "    \"Medium\": \"medium@1_variations_gpt.yml\",\n",
    "    \"Complex\": \"complex@1_variations_gpt.yml\"\n",
    "}\n",
    "\n",
    "# Count entries in each file\n",
    "for label, filename in files.items():\n",
    "    with open(filename, \"r\") as f:\n",
    "        data = yaml.safe_load(f)\n",
    "        print(f\"{label} codes: {len(data)} code blocks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b85e595",
   "metadata": {},
   "source": [
    "### Validate a single code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ece277bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📄 Loaded 30 code snippets from complex_variations_gpt.yml\n",
      "\n",
      "🎯 Validating index 29 from complex_variations_gpt.yml\n",
      "✅ PASS\n",
      "\n",
      "STDOUT:\n",
      "- V e r i l a t i o n   R e p o r t: Verilator 5.036 2025-04-27 rev UNKNOWN.REV\n",
      "- Verilator: Built from 0.027 MB sources in 2 modules, into 0.008 MB in 4 C++ files needing 0.000 MB\n",
      "- Verilator: Walltime 0.013 s (elab=0.002, cvt=0.003, bld=0.000); cpu 0.006 s on 1 threads\n"
     ]
    }
   ],
   "source": [
    "import yaml\n",
    "import subprocess\n",
    "import tempfile\n",
    "from pathlib import Path\n",
    "\n",
    "# === CONFIGURATION ===\n",
    "yaml_file = \"complex_variations_gpt.yml\"  # Change to your file path\n",
    "\n",
    "# === LOAD YAML FILE ===\n",
    "try:\n",
    "    with open(yaml_file, \"r\") as f:\n",
    "        codes = yaml.safe_load(f)\n",
    "except FileNotFoundError:\n",
    "    print(f\"❌ YAML file not found: {yaml_file}\")\n",
    "    exit(1)\n",
    "\n",
    "if not isinstance(codes, list) or not all(\"code\" in item for item in codes):\n",
    "    print(\"❌ Invalid YAML format. Expected a list of items with 'code' fields.\")\n",
    "    exit(1)\n",
    "\n",
    "# === PROMPT FOR INDEX ===\n",
    "max_index = len(codes) - 1\n",
    "print(f\"📄 Loaded {len(codes)} code snippets from {yaml_file}\")\n",
    "\n",
    "try:\n",
    "    index = int(input(f\"🔢 Enter index to validate (0–{max_index}): \"))\n",
    "except ValueError:\n",
    "    print(\"❌ Invalid input. Please enter an integer.\")\n",
    "    exit(1)\n",
    "\n",
    "if not (0 <= index <= max_index):\n",
    "    print(f\"❌ Invalid index. Must be between 0 and {max_index}.\")\n",
    "    exit(1)\n",
    "\n",
    "code = codes[index][\"code\"].strip()\n",
    "\n",
    "# === WRITE TO TEMP FILE ===\n",
    "with tempfile.NamedTemporaryFile(delete=False, suffix=\".sv\", mode=\"w\") as tmp_file:\n",
    "    tmp_file.write(code)\n",
    "    tmp_file_path = Path(tmp_file.name)\n",
    "\n",
    "# === RUN VERILATOR ===\n",
    "try:\n",
    "    print(f\"\\n🎯 Validating index {index} from {yaml_file}\")\n",
    "    result = subprocess.run(\n",
    "        [\"verilator\", \"--lint-only\", \"--timing\", str(tmp_file_path)],\n",
    "        stdout=subprocess.PIPE,\n",
    "        stderr=subprocess.PIPE,\n",
    "        timeout=10\n",
    "    )\n",
    "    passed = result.returncode == 0\n",
    "    print(\"✅ PASS\" if passed else \"❌ FAIL\")\n",
    "\n",
    "    if result.stdout:\n",
    "        print(\"\\nSTDOUT:\")\n",
    "        print(result.stdout.decode().strip())\n",
    "    if result.stderr:\n",
    "        print(\"\\nSTDERR:\")\n",
    "        print(result.stderr.decode().strip())\n",
    "\n",
    "except subprocess.TimeoutExpired:\n",
    "    print(\"❌ FAIL — Timeout while linting\")\n",
    "except FileNotFoundError:\n",
    "    print(\"❌ FAIL — Verilator not found. Is it installed and in PATH?\")\n",
    "finally:\n",
    "    tmp_file_path.unlink(missing_ok=True)  # Clean up temp file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f57720f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📄 Loaded 30 code snippets from complex_variations_gpt.yml\n",
      "\n",
      "🎯 Validating index 0 from complex_variations_gpt.yml\n",
      "✅ PASS\n",
      "\n",
      "STDOUT:\n",
      "- V e r i l a t i o n   R e p o r t: Verilator 5.036 2025-04-27 rev UNKNOWN.REV\n",
      "- Verilator: Built from 0.028 MB sources in 3 modules, into 0.018 MB in 6 C++ files needing 0.000 MB\n",
      "- Verilator: Walltime 0.005 s (elab=0.000, cvt=0.003, bld=0.000); cpu 0.005 s on 1 threads\n"
     ]
    }
   ],
   "source": [
    "import yaml\n",
    "import subprocess\n",
    "import tempfile\n",
    "from pathlib import Path\n",
    "\n",
    "# === CONFIGURATION ===\n",
    "yaml_file = \"complex_variations_gpt.yml\"  # Change to your file path\n",
    "\n",
    "# === LOAD YAML FILE ===\n",
    "try:\n",
    "    with open(yaml_file, \"r\") as f:\n",
    "        codes = yaml.safe_load(f)\n",
    "except FileNotFoundError:\n",
    "    print(f\"❌ YAML file not found: {yaml_file}\")\n",
    "    exit(1)\n",
    "\n",
    "if not isinstance(codes, list) or not all(\"code\" in item for item in codes):\n",
    "    print(\"❌ Invalid YAML format. Expected a list of items with 'code' fields.\")\n",
    "    exit(1)\n",
    "\n",
    "# === PROMPT FOR INDEX ===\n",
    "max_index = len(codes) - 1\n",
    "print(f\"📄 Loaded {len(codes)} code snippets from {yaml_file}\")\n",
    "\n",
    "try:\n",
    "    index = int(input(f\"🔢 Enter index to validate (0–{max_index}): \"))\n",
    "except ValueError:\n",
    "    print(\"❌ Invalid input. Please enter an integer.\")\n",
    "    exit(1)\n",
    "\n",
    "if not (0 <= index <= max_index):\n",
    "    print(f\"❌ Invalid index. Must be between 0 and {max_index}.\")\n",
    "    exit(1)\n",
    "\n",
    "code = codes[index][\"code\"].strip()\n",
    "\n",
    "# === WRITE TO TEMP FILE ===\n",
    "with tempfile.NamedTemporaryFile(delete=False, suffix=\".sv\", mode=\"w\") as tmp_file:\n",
    "    tmp_file.write(code)\n",
    "    tmp_file_path = Path(tmp_file.name)\n",
    "\n",
    "# === RUN VERILATOR ===\n",
    "try:\n",
    "    print(f\"\\n🎯 Validating index {index} from {yaml_file}\")\n",
    "    result = subprocess.run(\n",
    "        [\"verilator\", \"--lint-only\", \"--timing\", str(tmp_file_path)],\n",
    "        stdout=subprocess.PIPE,\n",
    "        stderr=subprocess.PIPE,\n",
    "        timeout=10\n",
    "    )\n",
    "    passed = result.returncode == 0\n",
    "    print(\"✅ PASS\" if passed else \"❌ FAIL\")\n",
    "\n",
    "    if result.stdout:\n",
    "        print(\"\\nSTDOUT:\")\n",
    "        print(result.stdout.decode().strip())\n",
    "    if result.stderr:\n",
    "        print(\"\\nSTDERR:\")\n",
    "        print(result.stderr.decode().strip())\n",
    "\n",
    "except subprocess.TimeoutExpired:\n",
    "    print(\"❌ FAIL — Timeout while linting\")\n",
    "except FileNotFoundError:\n",
    "    print(\"❌ FAIL — Verilator not found. Is it installed and in PATH?\")\n",
    "finally:\n",
    "    tmp_file_path.unlink(missing_ok=True)  # Clean up temp file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "876de6ce",
   "metadata": {},
   "source": [
    "## Validating GPT codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "912dabaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Verilator Validation Summary:\n",
      "Total Files Checked: 30\n",
      "Passed: 26\n",
      "Failed: 4\n",
      "\n",
      "✅ PASS — complex_variations_gpt.yml [index 0]\n",
      "✅ PASS — complex_variations_gpt.yml [index 1]\n",
      "✅ PASS — complex_variations_gpt.yml [index 2]\n",
      "✅ PASS — complex_variations_gpt.yml [index 3]\n",
      "✅ PASS — complex_variations_gpt.yml [index 4]\n",
      "❌ FAIL — complex_variations_gpt.yml [index 5]\n",
      "%Warning-WIDTHEXPAND: /var/folders/v6/jvlsn67j71g102qybf_hsvxr0000gn/T/tmp69t4l4qm.sv:15:15: Operator INSIDE expects 2 bits on the Inside expression, but Inside expression's VARREF 'w' generates 1 bits.\n",
      "                                                                                           : ... note: In instance 'mealy'\n",
      "   15 |         if (w inside {wlist[0], wlist[1]})\n",
      "      |               ^~~~~~\n",
      "                      ... For warning description see https://verilator.org/warn/WIDTHEXPAND?v=5.036\n",
      "                      ... Use \"/* verilator lint_off WIDTHEXPAND */\" and lint_on around source to disable this message.\n",
      "%Error-UNSUPPORTED: /var/folders/v6/jvlsn67j71g102qybf_hsvxr0000gn/T/tmp69t4l4qm.sv:15:28: Unsupported: RHS of ==? or !=? is fourstate but not a constant\n",
      "                                                                                         : ... note: In instance 'mealy'\n",
      "   15 |         if (w inside {wlist[0], wlist[1]})\n",
      "      |                            ^\n",
      "                    ... For error description see https://verilator.org/warn/UNSUPPORTED?v=5.036\n",
      "%Error-UNSUPPORTED: /var/folders/v6/jvlsn67j71g102qybf_hsvxr0000gn/T/tmp69t4l4qm.sv:15:38: Unsupported: RHS of ==? or !=? is fourstate but not a constant\n",
      "                                                                                         : ... note: In instance 'mealy'\n",
      "   15 |         if (w inside {wlist[0], wlist[1]})\n",
      "      |                                      ^\n",
      "%Error: Exiting due to 2 error(s), 1 warning(s)\n",
      "\n",
      "✅ PASS — complex_variations_gpt.yml [index 6]\n",
      "✅ PASS — complex_variations_gpt.yml [index 7]\n",
      "✅ PASS — complex_variations_gpt.yml [index 8]\n",
      "✅ PASS — complex_variations_gpt.yml [index 9]\n",
      "✅ PASS — complex_variations_gpt.yml [index 10]\n",
      "✅ PASS — complex_variations_gpt.yml [index 11]\n",
      "✅ PASS — complex_variations_gpt.yml [index 12]\n",
      "❌ FAIL — complex_variations_gpt.yml [index 13]\n",
      "%Error: /var/folders/v6/jvlsn67j71g102qybf_hsvxr0000gn/T/tmpdoquip9i.sv:9:29: syntax error, unexpected IDENTIFIER, expecting '{'\n",
      "    9 |         if ({s1, s0} inside ctrlvals[0]) begin\n",
      "      |                             ^~~~~~~~\n",
      "        ... See the manual at https://verilator.org/verilator_doc.html?v=5.036 for more assistance.\n",
      "%Error: Cannot continue\n",
      "        ... This fatal error may be caused by the earlier error(s); resolve those first.\n",
      "\n",
      "✅ PASS — complex_variations_gpt.yml [index 14]\n",
      "❌ FAIL — complex_variations_gpt.yml [index 15]\n",
      "%Error: /var/folders/v6/jvlsn67j71g102qybf_hsvxr0000gn/T/tmpfolcn1se.sv:6:5: Unknown verilator comment: '/*verilator -compatible manual check*/'\n",
      "    6 |     /*verilator -compatible manual check*/ \n",
      "      |     ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "        ... See the manual at https://verilator.org/verilator_doc.html?v=5.036 for more assistance.\n",
      "%Error: Exiting due to 1 error(s)\n",
      "\n",
      "✅ PASS — complex_variations_gpt.yml [index 16]\n",
      "✅ PASS — complex_variations_gpt.yml [index 17]\n",
      "✅ PASS — complex_variations_gpt.yml [index 18]\n",
      "✅ PASS — complex_variations_gpt.yml [index 19]\n",
      "✅ PASS — complex_variations_gpt.yml [index 20]\n",
      "✅ PASS — complex_variations_gpt.yml [index 21]\n",
      "✅ PASS — complex_variations_gpt.yml [index 22]\n",
      "✅ PASS — complex_variations_gpt.yml [index 23]\n",
      "✅ PASS — complex_variations_gpt.yml [index 24]\n",
      "❌ FAIL — complex_variations_gpt.yml [index 25]\n",
      "%Error: /var/folders/v6/jvlsn67j71g102qybf_hsvxr0000gn/T/tmph1mubrx5.sv:5:24: syntax error, unexpected table, expecting IDENTIFIER or randomize\n",
      "    5 | localparam logic [3:0] table [0:3] = '{4'd1, 4'd3, 4'd7, 4'd15};\n",
      "      |                        ^~~~~\n",
      "        ... See the manual at https://verilator.org/verilator_doc.html?v=5.036 for more assistance.\n",
      "%Error: /var/folders/v6/jvlsn67j71g102qybf_hsvxr0000gn/T/tmph1mubrx5.sv:5:30: Missing verilog.l rule: Default rule invoked in state 13 '['\n",
      "    5 | localparam logic [3:0] table [0:3] = '{4'd1, 4'd3, 4'd7, 4'd15};\n",
      "      |                              ^\n",
      "%Error: /var/folders/v6/jvlsn67j71g102qybf_hsvxr0000gn/T/tmph1mubrx5.sv:5:33: Missing verilog.l rule: Default rule invoked in state 13 '3'\n",
      "    5 | localparam logic [3:0] table [0:3] = '{4'd1, 4'd3, 4'd7, 4'd15};\n",
      "      |                                 ^\n",
      "%Error: /var/folders/v6/jvlsn67j71g102qybf_hsvxr0000gn/T/tmph1mubrx5.sv:5:34: Missing verilog.l rule: Default rule invoked in state 13 ']'\n",
      "    5 | localparam logic [3:0] table [0:3] = '{4'd1, 4'd3, 4'd7, 4'd15};\n",
      "      |                                  ^\n",
      "%Error: /var/folders/v6/jvlsn67j71g102qybf_hsvxr0000gn/T/tmph1mubrx5.sv:5:36: Missing verilog.l rule: Default rule invoked in state 13 '='\n",
      "    5 | localparam logic [3:0] table [0:3] = '{4'd1, 4'd3, 4'd7, 4'd15};\n",
      "      |                                    ^\n",
      "%Error: /var/folders/v6/jvlsn67j71g102qybf_hsvxr0000gn/T/tmph1mubrx5.sv:5:38: Missing verilog.l rule: Default rule invoked in state 13 '''\n",
      "    5 | localparam logic [3:0] table [0:3] = '{4'd1, 4'd3, 4'd7, 4'd15};\n",
      "      |                                      ^\n",
      "%Error: /var/folders/v6/jvlsn67j71g102qybf_hsvxr0000gn/T/tmph1mubrx5.sv:5:39: Missing verilog.l rule: Default rule invoked in state 13 '{'\n",
      "    5 | localparam logic [3:0] table [0:3] = '{4'd1, 4'd3, 4'd7, 4'd15};\n",
      "      |                                       ^\n",
      "%Error: /var/folders/v6/jvlsn67j71g102qybf_hsvxr0000gn/T/tmph1mubrx5.sv:5:40: Missing verilog.l rule: Default rule invoked in state 13 '4'\n",
      "    5 | localparam logic [3:0] table [0:3] = '{4'd1, 4'd3, 4'd7, 4'd15};\n",
      "      |                                        ^\n",
      "%Error: /var/folders/v6/jvlsn67j71g102qybf_hsvxr0000gn/T/tmph1mubrx5.sv:5:41: Missing verilog.l rule: Default rule invoked in state 13 '''\n",
      "    5 | localparam logic [3:0] table [0:3] = '{4'd1, 4'd3, 4'd7, 4'd15};\n",
      "      |                                         ^\n",
      "%Error: /var/folders/v6/jvlsn67j71g102qybf_hsvxr0000gn/T/tmph1mubrx5.sv:5:42: Missing verilog.l rule: Default rule invoked in state 13 'd'\n",
      "    5 | localparam logic [3:0] table [0:3] = '{4'd1, 4'd3, 4'd7, 4'd15};\n",
      "      |                                          ^\n",
      "%Error: /var/folders/v6/jvlsn67j71g102qybf_hsvxr0000gn/T/tmph1mubrx5.sv:5:44: Missing verilog.l rule: Default rule invoked in state 13 ','\n",
      "    5 | localparam logic [3:0] table [0:3] = '{4'd1, 4'd3, 4'd7, 4'd15};\n",
      "      |                                            ^\n",
      "%Error: /var/folders/v6/jvlsn67j71g102qybf_hsvxr0000gn/T/tmph1mubrx5.sv:5:46: Missing verilog.l rule: Default rule invoked in state 13 '4'\n",
      "    5 | localparam logic [3:0] table [0:3] = '{4'd1, 4'd3, 4'd7, 4'd15};\n",
      "      |                                              ^\n",
      "%Error: /var/folders/v6/jvlsn67j71g102qybf_hsvxr0000gn/T/tmph1mubrx5.sv:5:47: Missing verilog.l rule: Default rule invoked in state 13 '''\n",
      "    5 | localparam logic [3:0] table [0:3] = '{4'd1, 4'd3, 4'd7, 4'd15};\n",
      "      |                                               ^\n",
      "%Error: /var/folders/v6/jvlsn67j71g102qybf_hsvxr0000gn/T/tmph1mubrx5.sv:5:48: Missing verilog.l rule: Default rule invoked in state 13 'd'\n",
      "    5 | localparam logic [3:0] table [0:3] = '{4'd1, 4'd3, 4'd7, 4'd15};\n",
      "      |                                                ^\n",
      "%Error: /var/folders/v6/jvlsn67j71g102qybf_hsvxr0000gn/T/tmph1mubrx5.sv:5:49: Missing verilog.l rule: Default rule invoked in state 13 '3'\n",
      "    5 | localparam logic [3:0] table [0:3] = '{4'd1, 4'd3, 4'd7, 4'd15};\n",
      "      |                                                 ^\n",
      "%Error: /var/folders/v6/jvlsn67j71g102qybf_hsvxr0000gn/T/tmph1mubrx5.sv:5:50: Missing verilog.l rule: Default rule invoked in state 13 ','\n",
      "    5 | localparam logic [3:0] table [0:3] = '{4'd1, 4'd3, 4'd7, 4'd15};\n",
      "      |                                                  ^\n",
      "%Error: /var/folders/v6/jvlsn67j71g102qybf_hsvxr0000gn/T/tmph1mubrx5.sv:5:52: Missing verilog.l rule: Default rule invoked in state 13 '4'\n",
      "    5 | localparam logic [3:0] table [0:3] = '{4'd1, 4'd3, 4'd7, 4'd15};\n",
      "      |                                                    ^\n",
      "%Error: /var/folders/v6/jvlsn67j71g102qybf_hsvxr0000gn/T/tmph1mubrx5.sv:5:53: Missing verilog.l rule: Default rule invoked in state 13 '''\n",
      "    5 | localparam logic [3:0] table [0:3] = '{4'd1, 4'd3, 4'd7, 4'd15};\n",
      "      |                                                     ^\n",
      "%Error: /var/folders/v6/jvlsn67j71g102qybf_hsvxr0000gn/T/tmph1mubrx5.sv:5:54: Missing verilog.l rule: Default rule invoked in state 13 'd'\n",
      "    5 | localparam logic [3:0] table [0:3] = '{4'd1, 4'd3, 4'd7, 4'd15};\n",
      "      |                                                      ^\n",
      "%Error: /var/folders/v6/jvlsn67j71g102qybf_hsvxr0000gn/T/tmph1mubrx5.sv:5:55: Missing verilog.l rule: Default rule invoked in state 13 '7'\n",
      "    5 | localparam logic [3:0] table [0:3] = '{4'd1, 4'd3, 4'd7, 4'd15};\n",
      "      |                                                       ^\n",
      "%Error: /var/folders/v6/jvlsn67j71g102qybf_hsvxr0000gn/T/tmph1mubrx5.sv:5:56: Missing verilog.l rule: Default rule invoked in state 13 ','\n",
      "    5 | localparam logic [3:0] table [0:3] = '{4'd1, 4'd3, 4'd7, 4'd15};\n",
      "      |                                                        ^\n",
      "%Error: /var/folders/v6/jvlsn67j71g102qybf_hsvxr0000gn/T/tmph1mubrx5.sv:5:58: Missing verilog.l rule: Default rule invoked in state 13 '4'\n",
      "    5 | localparam logic [3:0] table [0:3] = '{4'd1, 4'd3, 4'd7, 4'd15};\n",
      "      |                                                          ^\n",
      "%Error: /var/folders/v6/jvlsn67j71g102qybf_hsvxr0000gn/T/tmph1mubrx5.sv:5:59: Missing verilog.l rule: Default rule invoked in state 13 '''\n",
      "    5 | localparam logic [3:0] table [0:3] = '{4'd1, 4'd3, 4'd7, 4'd15};\n",
      "      |                                                           ^\n",
      "%Error: /var/folders/v6/jvlsn67j71g102qybf_hsvxr0000gn/T/tmph1mubrx5.sv:5:60: Missing verilog.l rule: Default rule invoked in state 13 'd'\n",
      "    5 | localparam logic [3:0] table [0:3] = '{4'd1, 4'd3, 4'd7, 4'd15};\n",
      "      |                                                            ^\n",
      "%Error: /var/folders/v6/jvlsn67j71g102qybf_hsvxr0000gn/T/tmph1mubrx5.sv:5:62: Missing verilog.l rule: Default rule invoked in state 13 '5'\n",
      "    5 | localparam logic [3:0] table [0:3] = '{4'd1, 4'd3, 4'd7, 4'd15};\n",
      "      |                                                              ^\n",
      "%Error: /var/folders/v6/jvlsn67j71g102qybf_hsvxr0000gn/T/tmph1mubrx5.sv:5:63: Missing verilog.l rule: Default rule invoked in state 13 '}'\n",
      "    5 | localparam logic [3:0] table [0:3] = '{4'd1, 4'd3, 4'd7, 4'd15};\n",
      "      |                                                               ^\n",
      "%Error: /var/folders/v6/jvlsn67j71g102qybf_hsvxr0000gn/T/tmph1mubrx5.sv:7:1: Missing verilog.l rule: Default rule invoked in state 13 'i'\n",
      "    7 | initial begin\n",
      "      | ^\n",
      "%Error: /var/folders/v6/jvlsn67j71g102qybf_hsvxr0000gn/T/tmph1mubrx5.sv:7:3: Missing verilog.l rule: Default rule invoked in state 13 'i'\n",
      "    7 | initial begin\n",
      "      |   ^\n",
      "%Error: /var/folders/v6/jvlsn67j71g102qybf_hsvxr0000gn/T/tmph1mubrx5.sv:7:4: Missing verilog.l rule: Default rule invoked in state 13 't'\n",
      "    7 | initial begin\n",
      "      |    ^\n",
      "%Error: /var/folders/v6/jvlsn67j71g102qybf_hsvxr0000gn/T/tmph1mubrx5.sv:7:5: Missing verilog.l rule: Default rule invoked in state 13 'i'\n",
      "    7 | initial begin\n",
      "      |     ^\n",
      "%Error: /var/folders/v6/jvlsn67j71g102qybf_hsvxr0000gn/T/tmph1mubrx5.sv:7:6: Missing verilog.l rule: Default rule invoked in state 13 'a'\n",
      "    7 | initial begin\n",
      "      |      ^\n",
      "%Error: /var/folders/v6/jvlsn67j71g102qybf_hsvxr0000gn/T/tmph1mubrx5.sv:7:7: Missing verilog.l rule: Default rule invoked in state 13 'l'\n",
      "    7 | initial begin\n",
      "      |       ^\n",
      "%Error: /var/folders/v6/jvlsn67j71g102qybf_hsvxr0000gn/T/tmph1mubrx5.sv:7:10: Missing verilog.l rule: Default rule invoked in state 13 'e'\n",
      "    7 | initial begin\n",
      "      |          ^\n",
      "%Error: /var/folders/v6/jvlsn67j71g102qybf_hsvxr0000gn/T/tmph1mubrx5.sv:7:11: Missing verilog.l rule: Default rule invoked in state 13 'g'\n",
      "    7 | initial begin\n",
      "      |           ^\n",
      "%Error: /var/folders/v6/jvlsn67j71g102qybf_hsvxr0000gn/T/tmph1mubrx5.sv:7:12: Missing verilog.l rule: Default rule invoked in state 13 'i'\n",
      "    7 | initial begin\n",
      "      |            ^\n",
      "%Error: /var/folders/v6/jvlsn67j71g102qybf_hsvxr0000gn/T/tmph1mubrx5.sv:8:4: Missing verilog.l rule: Default rule invoked in state 13 'C'\n",
      "    8 |   BCD_in = 4'd3;  \n",
      "      |    ^\n",
      "%Error: /var/folders/v6/jvlsn67j71g102qybf_hsvxr0000gn/T/tmph1mubrx5.sv:8:5: Missing verilog.l rule: Default rule invoked in state 13 'D'\n",
      "    8 |   BCD_in = 4'd3;  \n",
      "      |     ^\n",
      "%Error: /var/folders/v6/jvlsn67j71g102qybf_hsvxr0000gn/T/tmph1mubrx5.sv:8:6: Missing verilog.l rule: Default rule invoked in state 13 '_'\n",
      "    8 |   BCD_in = 4'd3;  \n",
      "      |      ^\n",
      "%Error: /var/folders/v6/jvlsn67j71g102qybf_hsvxr0000gn/T/tmph1mubrx5.sv:8:7: Missing verilog.l rule: Default rule invoked in state 13 'i'\n",
      "    8 |   BCD_in = 4'd3;  \n",
      "      |       ^\n",
      "%Error: /var/folders/v6/jvlsn67j71g102qybf_hsvxr0000gn/T/tmph1mubrx5.sv:8:10: Missing verilog.l rule: Default rule invoked in state 13 '='\n",
      "    8 |   BCD_in = 4'd3;  \n",
      "      |          ^\n",
      "%Error: /var/folders/v6/jvlsn67j71g102qybf_hsvxr0000gn/T/tmph1mubrx5.sv:8:12: Missing verilog.l rule: Default rule invoked in state 13 '4'\n",
      "    8 |   BCD_in = 4'd3;  \n",
      "      |            ^\n",
      "%Error: /var/folders/v6/jvlsn67j71g102qybf_hsvxr0000gn/T/tmph1mubrx5.sv:8:13: Missing verilog.l rule: Default rule invoked in state 13 '''\n",
      "    8 |   BCD_in = 4'd3;  \n",
      "      |             ^\n",
      "%Error: /var/folders/v6/jvlsn67j71g102qybf_hsvxr0000gn/T/tmph1mubrx5.sv:8:14: Missing verilog.l rule: Default rule invoked in state 13 'd'\n",
      "    8 |   BCD_in = 4'd3;  \n",
      "      |              ^\n",
      "%Error: /var/folders/v6/jvlsn67j71g102qybf_hsvxr0000gn/T/tmph1mubrx5.sv:8:15: Missing verilog.l rule: Default rule invoked in state 13 '3'\n",
      "    8 |   BCD_in = 4'd3;  \n",
      "      |               ^\n",
      "%Error: /var/folders/v6/jvlsn67j71g102qybf_hsvxr0000gn/T/tmph1mubrx5.sv:11:3: Missing verilog.l rule: Default rule invoked in state 13 'i'\n",
      "   11 |   if (BCD_in == table[0] ||\n",
      "      |   ^\n",
      "%Error: /var/folders/v6/jvlsn67j71g102qybf_hsvxr0000gn/T/tmph1mubrx5.sv:11:8: Missing verilog.l rule: Default rule invoked in state 13 'C'\n",
      "   11 |   if (BCD_in == table[0] ||\n",
      "      |        ^\n",
      "%Error: /var/folders/v6/jvlsn67j71g102qybf_hsvxr0000gn/T/tmph1mubrx5.sv:11:9: Missing verilog.l rule: Default rule invoked in state 13 'D'\n",
      "   11 |   if (BCD_in == table[0] ||\n",
      "      |         ^\n",
      "%Error: /var/folders/v6/jvlsn67j71g102qybf_hsvxr0000gn/T/tmph1mubrx5.sv:11:10: Missing verilog.l rule: Default rule invoked in state 13 '_'\n",
      "   11 |   if (BCD_in == table[0] ||\n",
      "      |          ^\n",
      "%Error: /var/folders/v6/jvlsn67j71g102qybf_hsvxr0000gn/T/tmph1mubrx5.sv:11:11: Missing verilog.l rule: Default rule invoked in state 13 'i'\n",
      "   11 |   if (BCD_in == table[0] ||\n",
      "      |           ^\n",
      "%Error: /var/folders/v6/jvlsn67j71g102qybf_hsvxr0000gn/T/tmph1mubrx5.sv:11:14: Missing verilog.l rule: Default rule invoked in state 13 '='\n",
      "   11 |   if (BCD_in == table[0] ||\n",
      "      |              ^\n",
      "%Error: Exiting due to too many errors encountered; --error-limit=50\n",
      "\n",
      "✅ PASS — complex_variations_gpt.yml [index 26]\n",
      "✅ PASS — complex_variations_gpt.yml [index 27]\n",
      "✅ PASS — complex_variations_gpt.yml [index 28]\n",
      "✅ PASS — complex_variations_gpt.yml [index 29]\n"
     ]
    }
   ],
   "source": [
    "import yaml\n",
    "import subprocess\n",
    "import tempfile\n",
    "from pathlib import Path\n",
    "\n",
    "# === CONFIG ===\n",
    "input_files = [\n",
    "    \"complex_variations_gpt.yml\",\n",
    "    # \"medium@1_variations_gpt.yml\",\n",
    "    # \"complex@1_variations_gpt.yml\"\n",
    "]\n",
    "\n",
    "results = []\n",
    "\n",
    "for file in input_files:\n",
    "    with open(file, \"r\") as f:\n",
    "        codes = yaml.safe_load(f)\n",
    "\n",
    "    for i, item in enumerate(codes):\n",
    "        code = item[\"code\"]\n",
    "        # Create a temporary .sv file for each code block\n",
    "        with tempfile.NamedTemporaryFile(delete=False, suffix=\".sv\", mode=\"w\") as tmp_file:\n",
    "            tmp_file.write(code)\n",
    "            tmp_file_path = Path(tmp_file.name)\n",
    "\n",
    "        # Run verilator lint check\n",
    "        try:\n",
    "            result = subprocess.run(\n",
    "                [\"verilator\", \"--lint-only\", \"--timing\", tmp_file_path],\n",
    "                stdout=subprocess.PIPE,\n",
    "                stderr=subprocess.PIPE,\n",
    "                timeout=10\n",
    "            )\n",
    "            passed = result.returncode == 0\n",
    "            results.append({\n",
    "                \"file\": file,\n",
    "                \"index\": i,\n",
    "                \"passed\": passed,\n",
    "                \"stdout\": result.stdout.decode(),\n",
    "                \"stderr\": result.stderr.decode()\n",
    "            })\n",
    "        except subprocess.TimeoutExpired:\n",
    "            results.append({\n",
    "                \"file\": file,\n",
    "                \"index\": i,\n",
    "                \"passed\": False,\n",
    "                \"stderr\": \"❌ Timeout while linting\",\n",
    "                \"stdout\": \"\"\n",
    "            })\n",
    "\n",
    "        # Clean up temp file\n",
    "        tmp_file_path.unlink()\n",
    "\n",
    "# === REPORT ===\n",
    "total = len(results)\n",
    "passed = sum(1 for r in results if r[\"passed\"])\n",
    "failed = total - passed\n",
    "\n",
    "print(f\"\\n✅ Verilator Validation Summary:\")\n",
    "print(f\"Total Files Checked: {total}\")\n",
    "print(f\"Passed: {passed}\")\n",
    "print(f\"Failed: {failed}\\n\")\n",
    "\n",
    "for r in results:\n",
    "    status = \"✅ PASS\" if r[\"passed\"] else \"❌ FAIL\"\n",
    "    print(f\"{status} — {r['file']} [index {r['index']}]\")\n",
    "    if not r[\"passed\"]:\n",
    "        print(r[\"stderr\"].strip())\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a07298f7",
   "metadata": {},
   "source": [
    "## Batch inferencing (simple@5, star-coder-3b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec17db06",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Using StarCoder\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ [1/35] Variation 1/5 generated\n",
      "✅ [1/35] Variation 2/5 generated\n",
      "✅ [1/35] Variation 3/5 generated\n",
      "✅ [1/35] Variation 4/5 generated\n",
      "✅ [1/35] Variation 5/5 generated\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ [2/35] Variation 1/5 generated\n",
      "✅ [2/35] Variation 2/5 generated\n",
      "✅ [2/35] Variation 3/5 generated\n",
      "✅ [2/35] Variation 4/5 generated\n",
      "✅ [2/35] Variation 5/5 generated\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ [3/35] Variation 1/5 generated\n",
      "✅ [3/35] Variation 2/5 generated\n",
      "✅ [3/35] Variation 3/5 generated\n",
      "✅ [3/35] Variation 4/5 generated\n",
      "✅ [3/35] Variation 5/5 generated\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ [4/35] Variation 1/5 generated\n",
      "✅ [4/35] Variation 2/5 generated\n",
      "✅ [4/35] Variation 3/5 generated\n",
      "✅ [4/35] Variation 4/5 generated\n",
      "✅ [4/35] Variation 5/5 generated\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ [5/35] Variation 1/5 generated\n",
      "✅ [5/35] Variation 2/5 generated\n",
      "✅ [5/35] Variation 3/5 generated\n",
      "✅ [5/35] Variation 4/5 generated\n",
      "✅ [5/35] Variation 5/5 generated\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ [6/35] Variation 1/5 generated\n",
      "✅ [6/35] Variation 2/5 generated\n",
      "✅ [6/35] Variation 3/5 generated\n",
      "✅ [6/35] Variation 4/5 generated\n",
      "✅ [6/35] Variation 5/5 generated\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ [7/35] Variation 1/5 generated\n",
      "✅ [7/35] Variation 2/5 generated\n",
      "✅ [7/35] Variation 3/5 generated\n",
      "✅ [7/35] Variation 4/5 generated\n",
      "✅ [7/35] Variation 5/5 generated\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ [8/35] Variation 1/5 generated\n",
      "✅ [8/35] Variation 2/5 generated\n",
      "✅ [8/35] Variation 3/5 generated\n",
      "✅ [8/35] Variation 4/5 generated\n",
      "✅ [8/35] Variation 5/5 generated\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ [9/35] Variation 1/5 generated\n",
      "✅ [9/35] Variation 2/5 generated\n",
      "✅ [9/35] Variation 3/5 generated\n",
      "✅ [9/35] Variation 4/5 generated\n",
      "✅ [9/35] Variation 5/5 generated\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ [10/35] Variation 1/5 generated\n",
      "✅ [10/35] Variation 2/5 generated\n",
      "✅ [10/35] Variation 3/5 generated\n",
      "✅ [10/35] Variation 4/5 generated\n",
      "✅ [10/35] Variation 5/5 generated\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ [11/35] Variation 1/5 generated\n",
      "✅ [11/35] Variation 2/5 generated\n",
      "✅ [11/35] Variation 3/5 generated\n",
      "✅ [11/35] Variation 4/5 generated\n",
      "✅ [11/35] Variation 5/5 generated\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ [12/35] Variation 1/5 generated\n",
      "✅ [12/35] Variation 2/5 generated\n",
      "✅ [12/35] Variation 3/5 generated\n",
      "✅ [12/35] Variation 4/5 generated\n",
      "✅ [12/35] Variation 5/5 generated\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ [13/35] Variation 1/5 generated\n",
      "✅ [13/35] Variation 2/5 generated\n",
      "✅ [13/35] Variation 3/5 generated\n",
      "✅ [13/35] Variation 4/5 generated\n",
      "✅ [13/35] Variation 5/5 generated\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ [14/35] Variation 1/5 generated\n",
      "✅ [14/35] Variation 2/5 generated\n",
      "✅ [14/35] Variation 3/5 generated\n",
      "✅ [14/35] Variation 4/5 generated\n",
      "✅ [14/35] Variation 5/5 generated\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ [15/35] Variation 1/5 generated\n",
      "✅ [15/35] Variation 2/5 generated\n",
      "✅ [15/35] Variation 3/5 generated\n",
      "✅ [15/35] Variation 4/5 generated\n",
      "✅ [15/35] Variation 5/5 generated\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ [16/35] Variation 1/5 generated\n",
      "✅ [16/35] Variation 2/5 generated\n",
      "✅ [16/35] Variation 3/5 generated\n",
      "✅ [16/35] Variation 4/5 generated\n",
      "✅ [16/35] Variation 5/5 generated\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ [17/35] Variation 1/5 generated\n",
      "✅ [17/35] Variation 2/5 generated\n",
      "✅ [17/35] Variation 3/5 generated\n",
      "✅ [17/35] Variation 4/5 generated\n",
      "✅ [17/35] Variation 5/5 generated\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ [18/35] Variation 1/5 generated\n",
      "✅ [18/35] Variation 2/5 generated\n",
      "✅ [18/35] Variation 3/5 generated\n",
      "✅ [18/35] Variation 4/5 generated\n",
      "✅ [18/35] Variation 5/5 generated\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ [19/35] Variation 1/5 generated\n",
      "✅ [19/35] Variation 2/5 generated\n",
      "✅ [19/35] Variation 3/5 generated\n",
      "✅ [19/35] Variation 4/5 generated\n",
      "✅ [19/35] Variation 5/5 generated\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ [20/35] Variation 1/5 generated\n",
      "✅ [20/35] Variation 2/5 generated\n",
      "✅ [20/35] Variation 3/5 generated\n",
      "✅ [20/35] Variation 4/5 generated\n",
      "✅ [20/35] Variation 5/5 generated\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ [21/35] Variation 1/5 generated\n",
      "✅ [21/35] Variation 2/5 generated\n",
      "✅ [21/35] Variation 3/5 generated\n",
      "✅ [21/35] Variation 4/5 generated\n",
      "✅ [21/35] Variation 5/5 generated\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ [22/35] Variation 1/5 generated\n",
      "✅ [22/35] Variation 2/5 generated\n",
      "✅ [22/35] Variation 3/5 generated\n",
      "✅ [22/35] Variation 4/5 generated\n",
      "✅ [22/35] Variation 5/5 generated\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ [23/35] Variation 1/5 generated\n",
      "✅ [23/35] Variation 2/5 generated\n",
      "✅ [23/35] Variation 3/5 generated\n",
      "✅ [23/35] Variation 4/5 generated\n",
      "✅ [23/35] Variation 5/5 generated\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ [24/35] Variation 1/5 generated\n",
      "✅ [24/35] Variation 2/5 generated\n",
      "✅ [24/35] Variation 3/5 generated\n",
      "✅ [24/35] Variation 4/5 generated\n",
      "✅ [24/35] Variation 5/5 generated\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ [25/35] Variation 1/5 generated\n",
      "✅ [25/35] Variation 2/5 generated\n",
      "✅ [25/35] Variation 3/5 generated\n",
      "✅ [25/35] Variation 4/5 generated\n",
      "✅ [25/35] Variation 5/5 generated\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ [26/35] Variation 1/5 generated\n",
      "✅ [26/35] Variation 2/5 generated\n",
      "✅ [26/35] Variation 3/5 generated\n",
      "✅ [26/35] Variation 4/5 generated\n",
      "✅ [26/35] Variation 5/5 generated\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ [27/35] Variation 1/5 generated\n",
      "✅ [27/35] Variation 2/5 generated\n",
      "✅ [27/35] Variation 3/5 generated\n",
      "✅ [27/35] Variation 4/5 generated\n",
      "✅ [27/35] Variation 5/5 generated\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ [28/35] Variation 1/5 generated\n",
      "✅ [28/35] Variation 2/5 generated\n",
      "✅ [28/35] Variation 3/5 generated\n",
      "✅ [28/35] Variation 4/5 generated\n",
      "✅ [28/35] Variation 5/5 generated\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ [29/35] Variation 1/5 generated\n",
      "✅ [29/35] Variation 2/5 generated\n",
      "✅ [29/35] Variation 3/5 generated\n",
      "✅ [29/35] Variation 4/5 generated\n",
      "✅ [29/35] Variation 5/5 generated\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ [30/35] Variation 1/5 generated\n",
      "✅ [30/35] Variation 2/5 generated\n",
      "✅ [30/35] Variation 3/5 generated\n",
      "✅ [30/35] Variation 4/5 generated\n",
      "✅ [30/35] Variation 5/5 generated\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ [31/35] Variation 1/5 generated\n",
      "✅ [31/35] Variation 2/5 generated\n",
      "✅ [31/35] Variation 3/5 generated\n",
      "✅ [31/35] Variation 4/5 generated\n",
      "✅ [31/35] Variation 5/5 generated\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ [32/35] Variation 1/5 generated\n",
      "✅ [32/35] Variation 2/5 generated\n",
      "✅ [32/35] Variation 3/5 generated\n",
      "✅ [32/35] Variation 4/5 generated\n",
      "✅ [32/35] Variation 5/5 generated\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ [33/35] Variation 1/5 generated\n",
      "✅ [33/35] Variation 2/5 generated\n",
      "✅ [33/35] Variation 3/5 generated\n",
      "✅ [33/35] Variation 4/5 generated\n",
      "✅ [33/35] Variation 5/5 generated\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ [34/35] Variation 1/5 generated\n",
      "✅ [34/35] Variation 2/5 generated\n",
      "✅ [34/35] Variation 3/5 generated\n",
      "✅ [34/35] Variation 4/5 generated\n",
      "✅ [34/35] Variation 5/5 generated\n",
      "✅ [35/35] Variation 1/5 generated\n",
      "✅ [35/35] Variation 2/5 generated\n",
      "✅ [35/35] Variation 3/5 generated\n",
      "✅ [35/35] Variation 4/5 generated\n",
      "✅ [35/35] Variation 5/5 generated\n",
      "\n",
      "✅ All 175 variations saved to /tmp/simple@5_variations.yml\n"
     ]
    }
   ],
   "source": [
    "import yaml\n",
    "from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "# === CONFIG ===\n",
    "code_file = \"/tmp/simple_codes.yml\"\n",
    "prompt_file = \"/tmp/simple_prompts.yml\"\n",
    "output_file = \"/tmp/simple@5_variations.yml\"\n",
    "num_variations_per_prompt = 5\n",
    "\n",
    "# === LOAD MODEL ===\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bigcode/starcoder2-3b\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"bigcode/starcoder2-3b\", torch_dtype=torch.float16)\n",
    "print(\"✅ Using StarCoder\")\n",
    "\n",
    "generator = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, device=0)\n",
    "\n",
    "# === LOAD INPUT DATA ===\n",
    "with open(code_file, \"r\") as f:\n",
    "    codes = yaml.safe_load(f)\n",
    "\n",
    "with open(prompt_file, \"r\") as f:\n",
    "    prompts = yaml.safe_load(f)\n",
    "\n",
    "assert len(codes) == len(prompts), \"❌ Code and prompt counts do not match!\"\n",
    "\n",
    "# === BUILD PROMPT BATCH ===\n",
    "batch_prompts = [\n",
    "    f\"### Base Verilog Code:\\n{c['code'].strip()}\\n### Variation Instruction:\\n{p['prompt'].strip()}\\n### Modified Code:\\n\"\n",
    "    for c, p in zip(codes, prompts)\n",
    "]\n",
    "\n",
    "# === RUN MULTIPLE VARIATIONS PER PROMPT ===\n",
    "outputs = []\n",
    "\n",
    "for i, prompt in enumerate(batch_prompts):\n",
    "    responses = generator(\n",
    "        prompt,\n",
    "        max_new_tokens=2048,\n",
    "        do_sample=True,\n",
    "        temperature=0.8,\n",
    "        num_return_sequences=num_variations_per_prompt\n",
    "    )\n",
    "    for j, resp in enumerate(responses):\n",
    "        response = resp['generated_text']\n",
    "        variation = response.split(\"### Modified Code:\\n\")[-1].strip()\n",
    "        outputs.append({\"code\": variation})\n",
    "        print(f\"✅ [{i+1}/{len(batch_prompts)}] Variation {j+1}/{num_variations_per_prompt} generated\")\n",
    "\n",
    "# === SAVE OUTPUTS WITH BLOCK STYLE ===\n",
    "def str_presenter(dumper, data):\n",
    "    if '\\n' in data:\n",
    "        return dumper.represent_scalar('tag:yaml.org,2002:str', data, style='|')\n",
    "    return dumper.represent_scalar('tag:yaml.org,2002:str', data)\n",
    "\n",
    "yaml.add_representer(str, str_presenter)\n",
    "\n",
    "with open(output_file, \"w\") as f:\n",
    "    yaml.dump(outputs, f, sort_keys=False)\n",
    "\n",
    "print(f\"\\n✅ All {len(outputs)} variations saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c0cc6b8",
   "metadata": {},
   "source": [
    "## Batch inferencing (medium@5, star-coder-3b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01c94083",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import yaml\n",
    "# from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer\n",
    "# import torch\n",
    "\n",
    "# # === CONFIG ===\n",
    "# code_file = \"/tmp/medium_codes.yml\"\n",
    "# prompt_file = \"/tmp/medium_prompts.yml\"\n",
    "# output_file = \"/tmp/medium@5_variations.yml\"\n",
    "# num_variations_per_prompt = 5\n",
    "\n",
    "# # === LOAD MODEL ===\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"bigcode/starcoder2-3b\")\n",
    "# model = AutoModelForCausalLM.from_pretrained(\"bigcode/starcoder2-3b\", torch_dtype=torch.float16)\n",
    "# print(\"✅ Using StarCoder\")\n",
    "\n",
    "# generator = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, device=0)\n",
    "\n",
    "# # === LOAD INPUT DATA ===\n",
    "# with open(code_file, \"r\") as f:\n",
    "#     codes = yaml.safe_load(f)\n",
    "\n",
    "# with open(prompt_file, \"r\") as f:\n",
    "#     prompts = yaml.safe_load(f)\n",
    "\n",
    "# assert len(codes) == len(prompts), \"❌ Code and prompt counts do not match!\"\n",
    "\n",
    "# # === BUILD AND EXPAND PROMPT BATCH ===\n",
    "# batch_prompts = []\n",
    "# source_indices = []\n",
    "\n",
    "# for i, (c, p) in enumerate(zip(codes, prompts)):\n",
    "#     prompt_text = f\"### Base Verilog Code:\\n{c['code'].strip()}\\n### Variation Instruction:\\n{p['prompt'].strip()}\\n### Modified Code:\\n\"\n",
    "#     for _ in range(num_variations_per_prompt):\n",
    "#         batch_prompts.append(prompt_text)\n",
    "#         source_indices.append(i)\n",
    "\n",
    "# # === RUN BATCH INFERENCE ===\n",
    "# responses = generator(\n",
    "#     batch_prompts,\n",
    "#     max_new_tokens=2048,\n",
    "#     do_sample=True,\n",
    "#     temperature=0.8\n",
    "# )\n",
    "\n",
    "# # === EXTRACT VARIATIONS ===\n",
    "# outputs = []\n",
    "# counts = [0] * len(codes)\n",
    "\n",
    "# for i, resp in enumerate(responses):\n",
    "#     response_text = resp['generated_text']\n",
    "#     variation = response_text.split(\"### Modified Code:\\n\")[-1].strip()\n",
    "#     outputs.append({\"code\": variation})\n",
    "#     src_idx = source_indices[i]\n",
    "#     counts[src_idx] += 1\n",
    "#     print(f\"✅ [{src_idx+1}/{len(codes)}] Variation {counts[src_idx]}/{num_variations_per_prompt} generated\")\n",
    "\n",
    "# # === SAVE OUTPUTS WITH BLOCK STYLE ===\n",
    "# def str_presenter(dumper, data):\n",
    "#     if '\\n' in data:\n",
    "#         return dumper.represent_scalar('tag:yaml.org,2002:str', data, style='|')\n",
    "#     return dumper.represent_scalar('tag:yaml.org,2002:str', data)\n",
    "\n",
    "# yaml.add_representer(str, str_presenter)\n",
    "\n",
    "# with open(output_file, \"w\") as f:\n",
    "#     yaml.dump(outputs, f, sort_keys=False)\n",
    "\n",
    "# print(f\"\\n✅ All {len(outputs)} variations saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40bd8e9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Using StarCoder\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ [1/35] Variation 1/5 generated\n",
      "✅ [1/35] Variation 2/5 generated\n",
      "✅ [1/35] Variation 3/5 generated\n",
      "✅ [1/35] Variation 4/5 generated\n",
      "✅ [1/35] Variation 5/5 generated\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ [2/35] Variation 1/5 generated\n",
      "✅ [2/35] Variation 2/5 generated\n",
      "✅ [2/35] Variation 3/5 generated\n",
      "✅ [2/35] Variation 4/5 generated\n",
      "✅ [2/35] Variation 5/5 generated\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ [3/35] Variation 1/5 generated\n",
      "✅ [3/35] Variation 2/5 generated\n",
      "✅ [3/35] Variation 3/5 generated\n",
      "✅ [3/35] Variation 4/5 generated\n",
      "✅ [3/35] Variation 5/5 generated\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ [4/35] Variation 1/5 generated\n",
      "✅ [4/35] Variation 2/5 generated\n",
      "✅ [4/35] Variation 3/5 generated\n",
      "✅ [4/35] Variation 4/5 generated\n",
      "✅ [4/35] Variation 5/5 generated\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ [5/35] Variation 1/5 generated\n",
      "✅ [5/35] Variation 2/5 generated\n",
      "✅ [5/35] Variation 3/5 generated\n",
      "✅ [5/35] Variation 4/5 generated\n",
      "✅ [5/35] Variation 5/5 generated\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ [6/35] Variation 1/5 generated\n",
      "✅ [6/35] Variation 2/5 generated\n",
      "✅ [6/35] Variation 3/5 generated\n",
      "✅ [6/35] Variation 4/5 generated\n",
      "✅ [6/35] Variation 5/5 generated\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ [7/35] Variation 1/5 generated\n",
      "✅ [7/35] Variation 2/5 generated\n",
      "✅ [7/35] Variation 3/5 generated\n",
      "✅ [7/35] Variation 4/5 generated\n",
      "✅ [7/35] Variation 5/5 generated\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ [8/35] Variation 1/5 generated\n",
      "✅ [8/35] Variation 2/5 generated\n",
      "✅ [8/35] Variation 3/5 generated\n",
      "✅ [8/35] Variation 4/5 generated\n",
      "✅ [8/35] Variation 5/5 generated\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ [9/35] Variation 1/5 generated\n",
      "✅ [9/35] Variation 2/5 generated\n",
      "✅ [9/35] Variation 3/5 generated\n",
      "✅ [9/35] Variation 4/5 generated\n",
      "✅ [9/35] Variation 5/5 generated\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ [10/35] Variation 1/5 generated\n",
      "✅ [10/35] Variation 2/5 generated\n",
      "✅ [10/35] Variation 3/5 generated\n",
      "✅ [10/35] Variation 4/5 generated\n",
      "✅ [10/35] Variation 5/5 generated\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ [11/35] Variation 1/5 generated\n",
      "✅ [11/35] Variation 2/5 generated\n",
      "✅ [11/35] Variation 3/5 generated\n",
      "✅ [11/35] Variation 4/5 generated\n",
      "✅ [11/35] Variation 5/5 generated\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ [12/35] Variation 1/5 generated\n",
      "✅ [12/35] Variation 2/5 generated\n",
      "✅ [12/35] Variation 3/5 generated\n",
      "✅ [12/35] Variation 4/5 generated\n",
      "✅ [12/35] Variation 5/5 generated\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ [13/35] Variation 1/5 generated\n",
      "✅ [13/35] Variation 2/5 generated\n",
      "✅ [13/35] Variation 3/5 generated\n",
      "✅ [13/35] Variation 4/5 generated\n",
      "✅ [13/35] Variation 5/5 generated\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ [14/35] Variation 1/5 generated\n",
      "✅ [14/35] Variation 2/5 generated\n",
      "✅ [14/35] Variation 3/5 generated\n",
      "✅ [14/35] Variation 4/5 generated\n",
      "✅ [14/35] Variation 5/5 generated\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ [15/35] Variation 1/5 generated\n",
      "✅ [15/35] Variation 2/5 generated\n",
      "✅ [15/35] Variation 3/5 generated\n",
      "✅ [15/35] Variation 4/5 generated\n",
      "✅ [15/35] Variation 5/5 generated\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ [16/35] Variation 1/5 generated\n",
      "✅ [16/35] Variation 2/5 generated\n",
      "✅ [16/35] Variation 3/5 generated\n",
      "✅ [16/35] Variation 4/5 generated\n",
      "✅ [16/35] Variation 5/5 generated\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ [17/35] Variation 1/5 generated\n",
      "✅ [17/35] Variation 2/5 generated\n",
      "✅ [17/35] Variation 3/5 generated\n",
      "✅ [17/35] Variation 4/5 generated\n",
      "✅ [17/35] Variation 5/5 generated\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ [18/35] Variation 1/5 generated\n",
      "✅ [18/35] Variation 2/5 generated\n",
      "✅ [18/35] Variation 3/5 generated\n",
      "✅ [18/35] Variation 4/5 generated\n",
      "✅ [18/35] Variation 5/5 generated\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ [19/35] Variation 1/5 generated\n",
      "✅ [19/35] Variation 2/5 generated\n",
      "✅ [19/35] Variation 3/5 generated\n",
      "✅ [19/35] Variation 4/5 generated\n",
      "✅ [19/35] Variation 5/5 generated\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ [20/35] Variation 1/5 generated\n",
      "✅ [20/35] Variation 2/5 generated\n",
      "✅ [20/35] Variation 3/5 generated\n",
      "✅ [20/35] Variation 4/5 generated\n",
      "✅ [20/35] Variation 5/5 generated\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ [21/35] Variation 1/5 generated\n",
      "✅ [21/35] Variation 2/5 generated\n",
      "✅ [21/35] Variation 3/5 generated\n",
      "✅ [21/35] Variation 4/5 generated\n",
      "✅ [21/35] Variation 5/5 generated\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ [22/35] Variation 1/5 generated\n",
      "✅ [22/35] Variation 2/5 generated\n",
      "✅ [22/35] Variation 3/5 generated\n",
      "✅ [22/35] Variation 4/5 generated\n",
      "✅ [22/35] Variation 5/5 generated\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ [23/35] Variation 1/5 generated\n",
      "✅ [23/35] Variation 2/5 generated\n",
      "✅ [23/35] Variation 3/5 generated\n",
      "✅ [23/35] Variation 4/5 generated\n",
      "✅ [23/35] Variation 5/5 generated\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ [24/35] Variation 1/5 generated\n",
      "✅ [24/35] Variation 2/5 generated\n",
      "✅ [24/35] Variation 3/5 generated\n",
      "✅ [24/35] Variation 4/5 generated\n",
      "✅ [24/35] Variation 5/5 generated\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ [25/35] Variation 1/5 generated\n",
      "✅ [25/35] Variation 2/5 generated\n",
      "✅ [25/35] Variation 3/5 generated\n",
      "✅ [25/35] Variation 4/5 generated\n",
      "✅ [25/35] Variation 5/5 generated\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ [26/35] Variation 1/5 generated\n",
      "✅ [26/35] Variation 2/5 generated\n",
      "✅ [26/35] Variation 3/5 generated\n",
      "✅ [26/35] Variation 4/5 generated\n",
      "✅ [26/35] Variation 5/5 generated\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ [27/35] Variation 1/5 generated\n",
      "✅ [27/35] Variation 2/5 generated\n",
      "✅ [27/35] Variation 3/5 generated\n",
      "✅ [27/35] Variation 4/5 generated\n",
      "✅ [27/35] Variation 5/5 generated\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ [28/35] Variation 1/5 generated\n",
      "✅ [28/35] Variation 2/5 generated\n",
      "✅ [28/35] Variation 3/5 generated\n",
      "✅ [28/35] Variation 4/5 generated\n",
      "✅ [28/35] Variation 5/5 generated\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ [29/35] Variation 1/5 generated\n",
      "✅ [29/35] Variation 2/5 generated\n",
      "✅ [29/35] Variation 3/5 generated\n",
      "✅ [29/35] Variation 4/5 generated\n",
      "✅ [29/35] Variation 5/5 generated\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ [30/35] Variation 1/5 generated\n",
      "✅ [30/35] Variation 2/5 generated\n",
      "✅ [30/35] Variation 3/5 generated\n",
      "✅ [30/35] Variation 4/5 generated\n",
      "✅ [30/35] Variation 5/5 generated\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ [31/35] Variation 1/5 generated\n",
      "✅ [31/35] Variation 2/5 generated\n",
      "✅ [31/35] Variation 3/5 generated\n",
      "✅ [31/35] Variation 4/5 generated\n",
      "✅ [31/35] Variation 5/5 generated\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ [32/35] Variation 1/5 generated\n",
      "✅ [32/35] Variation 2/5 generated\n",
      "✅ [32/35] Variation 3/5 generated\n",
      "✅ [32/35] Variation 4/5 generated\n",
      "✅ [32/35] Variation 5/5 generated\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ [33/35] Variation 1/5 generated\n",
      "✅ [33/35] Variation 2/5 generated\n",
      "✅ [33/35] Variation 3/5 generated\n",
      "✅ [33/35] Variation 4/5 generated\n",
      "✅ [33/35] Variation 5/5 generated\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ [34/35] Variation 1/5 generated\n",
      "✅ [34/35] Variation 2/5 generated\n",
      "✅ [34/35] Variation 3/5 generated\n",
      "✅ [34/35] Variation 4/5 generated\n",
      "✅ [34/35] Variation 5/5 generated\n",
      "✅ [35/35] Variation 1/5 generated\n",
      "✅ [35/35] Variation 2/5 generated\n",
      "✅ [35/35] Variation 3/5 generated\n",
      "✅ [35/35] Variation 4/5 generated\n",
      "✅ [35/35] Variation 5/5 generated\n",
      "\n",
      "✅ All 175 variations saved to /tmp/medium@5_variations.yml\n"
     ]
    }
   ],
   "source": [
    "import yaml\n",
    "from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "# === CONFIG ===\n",
    "code_file = \"/tmp/medium_codes.yml\"\n",
    "prompt_file = \"/tmp/medium_prompts.yml\"\n",
    "output_file = \"/tmp/medium@5_variations.yml\"\n",
    "num_variations_per_prompt = 5\n",
    "\n",
    "# === LOAD MODEL ===\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bigcode/starcoder2-3b\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"bigcode/starcoder2-3b\", torch_dtype=torch.float16)\n",
    "print(\"✅ Using StarCoder\")\n",
    "\n",
    "generator = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, device=0)\n",
    "\n",
    "# === LOAD INPUT DATA ===\n",
    "with open(code_file, \"r\") as f:\n",
    "    codes = yaml.safe_load(f)\n",
    "\n",
    "with open(prompt_file, \"r\") as f:\n",
    "    prompts = yaml.safe_load(f)\n",
    "\n",
    "assert len(codes) == len(prompts), \"❌ Code and prompt counts do not match!\"\n",
    "\n",
    "# === BUILD PROMPT BATCH ===\n",
    "batch_prompts = [\n",
    "    f\"### Base Verilog Code:\\n{c['code'].strip()}\\n### Variation Instruction:\\n{p['prompt'].strip()}\\n### Modified Code:\\n\"\n",
    "    for c, p in zip(codes, prompts)\n",
    "]\n",
    "\n",
    "# === RUN MULTIPLE VARIATIONS PER PROMPT ===\n",
    "outputs = []\n",
    "\n",
    "for i, prompt in enumerate(batch_prompts):\n",
    "    responses = generator(\n",
    "        prompt,\n",
    "        max_new_tokens=2048,\n",
    "        do_sample=True,\n",
    "        temperature=0.8,\n",
    "        num_return_sequences=num_variations_per_prompt\n",
    "    )\n",
    "    for j, resp in enumerate(responses):\n",
    "        response = resp['generated_text']\n",
    "        variation = response.split(\"### Modified Code:\\n\")[-1].strip()\n",
    "        outputs.append({\"code\": variation})\n",
    "        print(f\"✅ [{i+1}/{len(batch_prompts)}] Variation {j+1}/{num_variations_per_prompt} generated\")\n",
    "\n",
    "# === SAVE OUTPUTS WITH BLOCK STYLE ===\n",
    "def str_presenter(dumper, data):\n",
    "    if '\\n' in data:\n",
    "        return dumper.represent_scalar('tag:yaml.org,2002:str', data, style='|')\n",
    "    return dumper.represent_scalar('tag:yaml.org,2002:str', data)\n",
    "\n",
    "yaml.add_representer(str, str_presenter)\n",
    "\n",
    "with open(output_file, \"w\") as f:\n",
    "    yaml.dump(outputs, f, sort_keys=False)\n",
    "\n",
    "print(f\"\\n✅ All {len(outputs)} variations saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "107c70ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8dee979",
   "metadata": {},
   "source": [
    "## Batch inferencing (complex@5, star-coder-3b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df082a5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Using StarCoder\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ [1/30] Variation 1/5 generated\n",
      "✅ [1/30] Variation 2/5 generated\n",
      "✅ [1/30] Variation 3/5 generated\n",
      "✅ [1/30] Variation 4/5 generated\n",
      "✅ [1/30] Variation 5/5 generated\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ [2/30] Variation 1/5 generated\n",
      "✅ [2/30] Variation 2/5 generated\n",
      "✅ [2/30] Variation 3/5 generated\n",
      "✅ [2/30] Variation 4/5 generated\n",
      "✅ [2/30] Variation 5/5 generated\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ [3/30] Variation 1/5 generated\n",
      "✅ [3/30] Variation 2/5 generated\n",
      "✅ [3/30] Variation 3/5 generated\n",
      "✅ [3/30] Variation 4/5 generated\n",
      "✅ [3/30] Variation 5/5 generated\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ [4/30] Variation 1/5 generated\n",
      "✅ [4/30] Variation 2/5 generated\n",
      "✅ [4/30] Variation 3/5 generated\n",
      "✅ [4/30] Variation 4/5 generated\n",
      "✅ [4/30] Variation 5/5 generated\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ [5/30] Variation 1/5 generated\n",
      "✅ [5/30] Variation 2/5 generated\n",
      "✅ [5/30] Variation 3/5 generated\n",
      "✅ [5/30] Variation 4/5 generated\n",
      "✅ [5/30] Variation 5/5 generated\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ [6/30] Variation 1/5 generated\n",
      "✅ [6/30] Variation 2/5 generated\n",
      "✅ [6/30] Variation 3/5 generated\n",
      "✅ [6/30] Variation 4/5 generated\n",
      "✅ [6/30] Variation 5/5 generated\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ [7/30] Variation 1/5 generated\n",
      "✅ [7/30] Variation 2/5 generated\n",
      "✅ [7/30] Variation 3/5 generated\n",
      "✅ [7/30] Variation 4/5 generated\n",
      "✅ [7/30] Variation 5/5 generated\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ [8/30] Variation 1/5 generated\n",
      "✅ [8/30] Variation 2/5 generated\n",
      "✅ [8/30] Variation 3/5 generated\n",
      "✅ [8/30] Variation 4/5 generated\n",
      "✅ [8/30] Variation 5/5 generated\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ [9/30] Variation 1/5 generated\n",
      "✅ [9/30] Variation 2/5 generated\n",
      "✅ [9/30] Variation 3/5 generated\n",
      "✅ [9/30] Variation 4/5 generated\n",
      "✅ [9/30] Variation 5/5 generated\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ [10/30] Variation 1/5 generated\n",
      "✅ [10/30] Variation 2/5 generated\n",
      "✅ [10/30] Variation 3/5 generated\n",
      "✅ [10/30] Variation 4/5 generated\n",
      "✅ [10/30] Variation 5/5 generated\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ [11/30] Variation 1/5 generated\n",
      "✅ [11/30] Variation 2/5 generated\n",
      "✅ [11/30] Variation 3/5 generated\n",
      "✅ [11/30] Variation 4/5 generated\n",
      "✅ [11/30] Variation 5/5 generated\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ [12/30] Variation 1/5 generated\n",
      "✅ [12/30] Variation 2/5 generated\n",
      "✅ [12/30] Variation 3/5 generated\n",
      "✅ [12/30] Variation 4/5 generated\n",
      "✅ [12/30] Variation 5/5 generated\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ [13/30] Variation 1/5 generated\n",
      "✅ [13/30] Variation 2/5 generated\n",
      "✅ [13/30] Variation 3/5 generated\n",
      "✅ [13/30] Variation 4/5 generated\n",
      "✅ [13/30] Variation 5/5 generated\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ [14/30] Variation 1/5 generated\n",
      "✅ [14/30] Variation 2/5 generated\n",
      "✅ [14/30] Variation 3/5 generated\n",
      "✅ [14/30] Variation 4/5 generated\n",
      "✅ [14/30] Variation 5/5 generated\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ [15/30] Variation 1/5 generated\n",
      "✅ [15/30] Variation 2/5 generated\n",
      "✅ [15/30] Variation 3/5 generated\n",
      "✅ [15/30] Variation 4/5 generated\n",
      "✅ [15/30] Variation 5/5 generated\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ [16/30] Variation 1/5 generated\n",
      "✅ [16/30] Variation 2/5 generated\n",
      "✅ [16/30] Variation 3/5 generated\n",
      "✅ [16/30] Variation 4/5 generated\n",
      "✅ [16/30] Variation 5/5 generated\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ [17/30] Variation 1/5 generated\n",
      "✅ [17/30] Variation 2/5 generated\n",
      "✅ [17/30] Variation 3/5 generated\n",
      "✅ [17/30] Variation 4/5 generated\n",
      "✅ [17/30] Variation 5/5 generated\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ [18/30] Variation 1/5 generated\n",
      "✅ [18/30] Variation 2/5 generated\n",
      "✅ [18/30] Variation 3/5 generated\n",
      "✅ [18/30] Variation 4/5 generated\n",
      "✅ [18/30] Variation 5/5 generated\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ [19/30] Variation 1/5 generated\n",
      "✅ [19/30] Variation 2/5 generated\n",
      "✅ [19/30] Variation 3/5 generated\n",
      "✅ [19/30] Variation 4/5 generated\n",
      "✅ [19/30] Variation 5/5 generated\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ [20/30] Variation 1/5 generated\n",
      "✅ [20/30] Variation 2/5 generated\n",
      "✅ [20/30] Variation 3/5 generated\n",
      "✅ [20/30] Variation 4/5 generated\n",
      "✅ [20/30] Variation 5/5 generated\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ [21/30] Variation 1/5 generated\n",
      "✅ [21/30] Variation 2/5 generated\n",
      "✅ [21/30] Variation 3/5 generated\n",
      "✅ [21/30] Variation 4/5 generated\n",
      "✅ [21/30] Variation 5/5 generated\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ [22/30] Variation 1/5 generated\n",
      "✅ [22/30] Variation 2/5 generated\n",
      "✅ [22/30] Variation 3/5 generated\n",
      "✅ [22/30] Variation 4/5 generated\n",
      "✅ [22/30] Variation 5/5 generated\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ [23/30] Variation 1/5 generated\n",
      "✅ [23/30] Variation 2/5 generated\n",
      "✅ [23/30] Variation 3/5 generated\n",
      "✅ [23/30] Variation 4/5 generated\n",
      "✅ [23/30] Variation 5/5 generated\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ [24/30] Variation 1/5 generated\n",
      "✅ [24/30] Variation 2/5 generated\n",
      "✅ [24/30] Variation 3/5 generated\n",
      "✅ [24/30] Variation 4/5 generated\n",
      "✅ [24/30] Variation 5/5 generated\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ [25/30] Variation 1/5 generated\n",
      "✅ [25/30] Variation 2/5 generated\n",
      "✅ [25/30] Variation 3/5 generated\n",
      "✅ [25/30] Variation 4/5 generated\n",
      "✅ [25/30] Variation 5/5 generated\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ [26/30] Variation 1/5 generated\n",
      "✅ [26/30] Variation 2/5 generated\n",
      "✅ [26/30] Variation 3/5 generated\n",
      "✅ [26/30] Variation 4/5 generated\n",
      "✅ [26/30] Variation 5/5 generated\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ [27/30] Variation 1/5 generated\n",
      "✅ [27/30] Variation 2/5 generated\n",
      "✅ [27/30] Variation 3/5 generated\n",
      "✅ [27/30] Variation 4/5 generated\n",
      "✅ [27/30] Variation 5/5 generated\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ [28/30] Variation 1/5 generated\n",
      "✅ [28/30] Variation 2/5 generated\n",
      "✅ [28/30] Variation 3/5 generated\n",
      "✅ [28/30] Variation 4/5 generated\n",
      "✅ [28/30] Variation 5/5 generated\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ [29/30] Variation 1/5 generated\n",
      "✅ [29/30] Variation 2/5 generated\n",
      "✅ [29/30] Variation 3/5 generated\n",
      "✅ [29/30] Variation 4/5 generated\n",
      "✅ [29/30] Variation 5/5 generated\n",
      "✅ [30/30] Variation 1/5 generated\n",
      "✅ [30/30] Variation 2/5 generated\n",
      "✅ [30/30] Variation 3/5 generated\n",
      "✅ [30/30] Variation 4/5 generated\n",
      "✅ [30/30] Variation 5/5 generated\n",
      "\n",
      "✅ All 150 variations saved to /tmp/complex@5_variations.yml\n"
     ]
    }
   ],
   "source": [
    "import yaml\n",
    "from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "# === CONFIG ===\n",
    "code_file = \"/tmp/complex_codes.yml\"\n",
    "prompt_file = \"/tmp/complex_prompts.yml\"\n",
    "output_file = \"/tmp/complex@5_variations.yml\"\n",
    "num_variations_per_prompt = 5\n",
    "\n",
    "# === LOAD MODEL ===\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bigcode/starcoder2-3b\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"bigcode/starcoder2-3b\", torch_dtype=torch.float16)\n",
    "print(\"✅ Using StarCoder\")\n",
    "\n",
    "generator = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, device=0)\n",
    "\n",
    "# === LOAD INPUT DATA ===\n",
    "with open(code_file, \"r\") as f:\n",
    "    codes = yaml.safe_load(f)\n",
    "\n",
    "with open(prompt_file, \"r\") as f:\n",
    "    prompts = yaml.safe_load(f)\n",
    "\n",
    "assert len(codes) == len(prompts), \"❌ Code and prompt counts do not match!\"\n",
    "\n",
    "# === BUILD PROMPT BATCH ===\n",
    "batch_prompts = [\n",
    "    f\"### Base Verilog Code:\\n{c['code'].strip()}\\n### Variation Instruction:\\n{p['prompt'].strip()}\\n### Modified Code:\\n\"\n",
    "    for c, p in zip(codes, prompts)\n",
    "]\n",
    "\n",
    "# === RUN MULTIPLE VARIATIONS PER PROMPT ===\n",
    "outputs = []\n",
    "\n",
    "for i, prompt in enumerate(batch_prompts):\n",
    "    responses = generator(\n",
    "        prompt,\n",
    "        max_new_tokens=2048,\n",
    "        do_sample=True,\n",
    "        temperature=0.8,\n",
    "        num_return_sequences=num_variations_per_prompt\n",
    "    )\n",
    "    for j, resp in enumerate(responses):\n",
    "        response = resp['generated_text']\n",
    "        variation = response.split(\"### Modified Code:\\n\")[-1].strip()\n",
    "        outputs.append({\"code\": variation})\n",
    "        print(f\"✅ [{i+1}/{len(batch_prompts)}] Variation {j+1}/{num_variations_per_prompt} generated\")\n",
    "\n",
    "# === SAVE OUTPUTS WITH BLOCK STYLE ===\n",
    "def str_presenter(dumper, data):\n",
    "    if '\\n' in data:\n",
    "        return dumper.represent_scalar('tag:yaml.org,2002:str', data, style='|')\n",
    "    return dumper.represent_scalar('tag:yaml.org,2002:str', data)\n",
    "\n",
    "yaml.add_representer(str, str_presenter)\n",
    "\n",
    "with open(output_file, \"w\") as f:\n",
    "    yaml.dump(outputs, f, sort_keys=False)\n",
    "\n",
    "print(f\"\\n✅ All {len(outputs)} variations saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "744dfc1b",
   "metadata": {},
   "source": [
    "## Batch Inferencing (Simple@10, star-coder-3b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d17e8577",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Using StarCoder\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/tmp/simple_codes.yml'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 19\u001b[0m\n\u001b[1;32m     16\u001b[0m generator \u001b[38;5;241m=\u001b[39m pipeline(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext-generation\u001b[39m\u001b[38;5;124m\"\u001b[39m, model\u001b[38;5;241m=\u001b[39mmodel, tokenizer\u001b[38;5;241m=\u001b[39mtokenizer, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# === LOAD INPUT DATA ===\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcode_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m     20\u001b[0m     codes \u001b[38;5;241m=\u001b[39m yaml\u001b[38;5;241m.\u001b[39msafe_load(f)\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(prompt_file, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n",
      "File \u001b[0;32m/tmp/conda_envs/hf_env/lib/python3.9/site-packages/IPython/core/interactiveshell.py:310\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    303\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    304\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    305\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    306\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    307\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    308\u001b[0m     )\n\u001b[0;32m--> 310\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/tmp/simple_codes.yml'"
     ]
    }
   ],
   "source": [
    "import yaml\n",
    "from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "# === CONFIG ===\n",
    "code_file = \"/tmp/simple_codes.yml\"\n",
    "prompt_file = \"/tmp/simple_prompts.yml\"\n",
    "output_file = \"/tmp/simple@10_variations.yml\"\n",
    "num_variations_per_prompt = 10\n",
    "\n",
    "# === LOAD MODEL ===\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bigcode/starcoder2-3b\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"bigcode/starcoder2-3b\", torch_dtype=torch.float16)\n",
    "print(\"✅ Using StarCoder\")\n",
    "\n",
    "generator = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, device=0)\n",
    "\n",
    "# === LOAD INPUT DATA ===\n",
    "with open(code_file, \"r\") as f:\n",
    "    codes = yaml.safe_load(f)\n",
    "\n",
    "with open(prompt_file, \"r\") as f:\n",
    "    prompts = yaml.safe_load(f)\n",
    "\n",
    "assert len(codes) == len(prompts), \"❌ Code and prompt counts do not match!\"\n",
    "\n",
    "# === BUILD PROMPT BATCH ===\n",
    "batch_prompts = [\n",
    "    f\"### Base Verilog Code:\\n{c['code'].strip()}\\n### Variation Instruction:\\n{p['prompt'].strip()}\\n### Modified Code:\\n\"\n",
    "    for c, p in zip(codes, prompts)\n",
    "]\n",
    "\n",
    "# === RUN MULTIPLE VARIATIONS PER PROMPT ===\n",
    "outputs = []\n",
    "\n",
    "for i, prompt in enumerate(batch_prompts):\n",
    "    responses = generator(\n",
    "        prompt,\n",
    "        max_new_tokens=2048,\n",
    "        do_sample=True,\n",
    "        temperature=0.8,\n",
    "        num_return_sequences=num_variations_per_prompt\n",
    "    )\n",
    "    for j, resp in enumerate(responses):\n",
    "        response = resp['generated_text']\n",
    "        variation = response.split(\"### Modified Code:\\n\")[-1].strip()\n",
    "        outputs.append({\"code\": variation})\n",
    "        print(f\"✅ [{i+1}/{len(batch_prompts)}] Variation {j+1}/{num_variations_per_prompt} generated\")\n",
    "\n",
    "# === SAVE OUTPUTS WITH BLOCK STYLE ===\n",
    "def str_presenter(dumper, data):\n",
    "    if '\\n' in data:\n",
    "        return dumper.represent_scalar('tag:yaml.org,2002:str', data, style='|')\n",
    "    return dumper.represent_scalar('tag:yaml.org,2002:str', data)\n",
    "\n",
    "yaml.add_representer(str, str_presenter)\n",
    "\n",
    "with open(output_file, \"w\") as f:\n",
    "    yaml.dump(outputs, f, sort_keys=False)\n",
    "\n",
    "print(f\"\\n✅ All {len(outputs)} variations saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23ecd676",
   "metadata": {},
   "source": [
    "## Batch Inferencing (medium@10, star-coder-3b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df7a4a62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "# === CONFIG ===\n",
    "code_file = \"/tmp/medium_codes.yml\"\n",
    "prompt_file = \"/tmp/medium_prompts.yml\"\n",
    "output_file = \"/tmp/medium@10_variations.yml\"\n",
    "num_variations_per_prompt = 10\n",
    "\n",
    "# === LOAD MODEL ===\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bigcode/starcoder2-3b\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"bigcode/starcoder2-3b\", torch_dtype=torch.float16)\n",
    "print(\"✅ Using StarCoder\")\n",
    "\n",
    "generator = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, device=0)\n",
    "\n",
    "# === LOAD INPUT DATA ===\n",
    "with open(code_file, \"r\") as f:\n",
    "    codes = yaml.safe_load(f)\n",
    "\n",
    "with open(prompt_file, \"r\") as f:\n",
    "    prompts = yaml.safe_load(f)\n",
    "\n",
    "assert len(codes) == len(prompts), \"❌ Code and prompt counts do not match!\"\n",
    "\n",
    "# === BUILD PROMPT BATCH ===\n",
    "batch_prompts = [\n",
    "    f\"### Base Verilog Code:\\n{c['code'].strip()}\\n### Variation Instruction:\\n{p['prompt'].strip()}\\n### Modified Code:\\n\"\n",
    "    for c, p in zip(codes, prompts)\n",
    "]\n",
    "\n",
    "# === RUN MULTIPLE VARIATIONS PER PROMPT ===\n",
    "outputs = []\n",
    "\n",
    "for i, prompt in enumerate(batch_prompts):\n",
    "    responses = generator(\n",
    "        prompt,\n",
    "        max_new_tokens=2048,\n",
    "        do_sample=True,\n",
    "        temperature=0.8,\n",
    "        num_return_sequences=num_variations_per_prompt\n",
    "    )\n",
    "    for j, resp in enumerate(responses):\n",
    "        response = resp['generated_text']\n",
    "        variation = response.split(\"### Modified Code:\\n\")[-1].strip()\n",
    "        outputs.append({\"code\": variation})\n",
    "        print(f\"✅ [{i+1}/{len(batch_prompts)}] Variation {j+1}/{num_variations_per_prompt} generated\")\n",
    "\n",
    "# === SAVE OUTPUTS WITH BLOCK STYLE ===\n",
    "def str_presenter(dumper, data):\n",
    "    if '\\n' in data:\n",
    "        return dumper.represent_scalar('tag:yaml.org,2002:str', data, style='|')\n",
    "    return dumper.represent_scalar('tag:yaml.org,2002:str', data)\n",
    "\n",
    "yaml.add_representer(str, str_presenter)\n",
    "\n",
    "with open(output_file, \"w\") as f:\n",
    "    yaml.dump(outputs, f, sort_keys=False)\n",
    "\n",
    "print(f\"\\n✅ All {len(outputs)} variations saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "901866d2",
   "metadata": {},
   "source": [
    "## Batch Inferencing (complex@10, star-coder-3b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "409a92fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "# === CONFIG ===\n",
    "code_file = \"/tmp/complex_codes.yml\"\n",
    "prompt_file = \"/tmp/complex_prompts.yml\"\n",
    "output_file = \"/tmp/complex@10_variations.yml\"\n",
    "num_variations_per_prompt = 10\n",
    "\n",
    "# === LOAD MODEL ===\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bigcode/starcoder2-3b\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"bigcode/starcoder2-3b\", torch_dtype=torch.float16)\n",
    "print(\"✅ Using StarCoder\")\n",
    "\n",
    "generator = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, device=0)\n",
    "\n",
    "# === LOAD INPUT DATA ===\n",
    "with open(code_file, \"r\") as f:\n",
    "    codes = yaml.safe_load(f)\n",
    "\n",
    "with open(prompt_file, \"r\") as f:\n",
    "    prompts = yaml.safe_load(f)\n",
    "\n",
    "assert len(codes) == len(prompts), \"❌ Code and prompt counts do not match!\"\n",
    "\n",
    "# === BUILD PROMPT BATCH ===\n",
    "batch_prompts = [\n",
    "    f\"### Base Verilog Code:\\n{c['code'].strip()}\\n### Variation Instruction:\\n{p['prompt'].strip()}\\n### Modified Code:\\n\"\n",
    "    for c, p in zip(codes, prompts)\n",
    "]\n",
    "\n",
    "# === RUN MULTIPLE VARIATIONS PER PROMPT ===\n",
    "outputs = []\n",
    "\n",
    "for i, prompt in enumerate(batch_prompts):\n",
    "    responses = generator(\n",
    "        prompt,\n",
    "        max_new_tokens=2048,\n",
    "        do_sample=True,\n",
    "        temperature=0.8,\n",
    "        num_return_sequences=num_variations_per_prompt\n",
    "    )\n",
    "    for j, resp in enumerate(responses):\n",
    "        response = resp['generated_text']\n",
    "        variation = response.split(\"### Modified Code:\\n\")[-1].strip()\n",
    "        outputs.append({\"code\": variation})\n",
    "        print(f\"✅ [{i+1}/{len(batch_prompts)}] Variation {j+1}/{num_variations_per_prompt} generated\")\n",
    "\n",
    "# === SAVE OUTPUTS WITH BLOCK STYLE ===\n",
    "def str_presenter(dumper, data):\n",
    "    if '\\n' in data:\n",
    "        return dumper.represent_scalar('tag:yaml.org,2002:str', data, style='|')\n",
    "    return dumper.represent_scalar('tag:yaml.org,2002:str', data)\n",
    "\n",
    "yaml.add_representer(str, str_presenter)\n",
    "\n",
    "with open(output_file, \"w\") as f:\n",
    "    yaml.dump(outputs, f, sort_keys=False)\n",
    "\n",
    "print(f\"\\n✅ All {len(outputs)} variations saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98232d19",
   "metadata": {},
   "source": [
    "## Batch Inferencing (RTLCoder-DeepSeek-6.7B, simple@1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "526c46b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/conda_envs/hf_env/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [03:18<00:00, 99.41s/it] \n",
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Using DeepSeek Coder 6.7B Base\n",
      "✅ [1/35] Variation 1/1 generated\n",
      "✅ [2/35] Variation 1/1 generated\n",
      "✅ [3/35] Variation 1/1 generated\n",
      "✅ [4/35] Variation 1/1 generated\n",
      "✅ [5/35] Variation 1/1 generated\n",
      "✅ [6/35] Variation 1/1 generated\n",
      "✅ [7/35] Variation 1/1 generated\n",
      "✅ [8/35] Variation 1/1 generated\n",
      "✅ [9/35] Variation 1/1 generated\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ [10/35] Variation 1/1 generated\n",
      "✅ [11/35] Variation 1/1 generated\n",
      "✅ [12/35] Variation 1/1 generated\n",
      "✅ [13/35] Variation 1/1 generated\n",
      "✅ [14/35] Variation 1/1 generated\n",
      "✅ [15/35] Variation 1/1 generated\n",
      "✅ [16/35] Variation 1/1 generated\n",
      "✅ [17/35] Variation 1/1 generated\n",
      "✅ [18/35] Variation 1/1 generated\n",
      "✅ [19/35] Variation 1/1 generated\n",
      "✅ [20/35] Variation 1/1 generated\n",
      "✅ [21/35] Variation 1/1 generated\n",
      "✅ [22/35] Variation 1/1 generated\n",
      "✅ [23/35] Variation 1/1 generated\n",
      "✅ [24/35] Variation 1/1 generated\n",
      "✅ [25/35] Variation 1/1 generated\n",
      "✅ [26/35] Variation 1/1 generated\n",
      "✅ [27/35] Variation 1/1 generated\n",
      "✅ [28/35] Variation 1/1 generated\n",
      "✅ [29/35] Variation 1/1 generated\n",
      "✅ [30/35] Variation 1/1 generated\n",
      "✅ [31/35] Variation 1/1 generated\n",
      "✅ [32/35] Variation 1/1 generated\n",
      "✅ [33/35] Variation 1/1 generated\n",
      "✅ [34/35] Variation 1/1 generated\n",
      "✅ [35/35] Variation 1/1 generated\n",
      "\n",
      "✅ All 35 variations saved to /tmp/simple@10_variations.yml\n"
     ]
    }
   ],
   "source": [
    "import yaml\n",
    "from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "# === CONFIG ===\n",
    "code_file = \"/tmp/simple_codes.yml\"\n",
    "prompt_file = \"/tmp/simple_prompts.yml\"\n",
    "output_file = \"/tmp/simple@1_variations.yml\"\n",
    "num_variations_per_prompt = 1\n",
    "\n",
    "# === LOAD MODEL ===\n",
    "model_name = \"deepseek-ai/deepseek-coder-6.7b-base\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.float16,\n",
    "    trust_remote_code=True\n",
    ").to(\"cuda\")  # manually move to GPU 0\n",
    "\n",
    "print(\"✅ Using DeepSeek Coder 6.7B Base\")\n",
    "\n",
    "generator = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, device=0)\n",
    "\n",
    "# === LOAD INPUT DATA ===\n",
    "with open(code_file, \"r\") as f:\n",
    "    codes = yaml.safe_load(f)\n",
    "\n",
    "with open(prompt_file, \"r\") as f:\n",
    "    prompts = yaml.safe_load(f)\n",
    "\n",
    "assert len(codes) == len(prompts), \"❌ Code and prompt counts do not match!\"\n",
    "\n",
    "# === BUILD PROMPT BATCH ===\n",
    "batch_prompts = [\n",
    "    f\"### Base Verilog Code:\\n{c['code'].strip()}\\n### Variation Instruction:\\n{p['prompt'].strip()}\\n### Modified Code:\\n\"\n",
    "    for c, p in zip(codes, prompts)\n",
    "]\n",
    "\n",
    "# === RUN MULTIPLE VARIATIONS PER PROMPT ===\n",
    "outputs = []\n",
    "\n",
    "for i, prompt in enumerate(batch_prompts):\n",
    "    responses = generator(\n",
    "        prompt,\n",
    "        max_new_tokens=2048,\n",
    "        do_sample=True,\n",
    "        temperature=0.8,\n",
    "        num_return_sequences=num_variations_per_prompt\n",
    "    )\n",
    "    for j, resp in enumerate(responses):\n",
    "        response = resp['generated_text']\n",
    "        variation = response.split(\"### Modified Code:\\n\")[-1].strip()\n",
    "        outputs.append({\"code\": variation})\n",
    "        print(f\"✅ [{i+1}/{len(batch_prompts)}] Variation {j+1}/{num_variations_per_prompt} generated\")\n",
    "\n",
    "# === SAVE OUTPUTS WITH BLOCK STYLE ===\n",
    "def str_presenter(dumper, data):\n",
    "    if '\\n' in data:\n",
    "        return dumper.represent_scalar('tag:yaml.org,2002:str', data, style='|')\n",
    "    return dumper.represent_scalar('tag:yaml.org,2002:str', data)\n",
    "\n",
    "yaml.add_representer(str, str_presenter)\n",
    "\n",
    "with open(output_file, \"w\") as f:\n",
    "    yaml.dump(outputs, f, sort_keys=False)\n",
    "\n",
    "print(f\"\\n✅ All {len(outputs)} variations saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5605bcf",
   "metadata": {},
   "source": [
    "## Batch Inferencing (RTLCoder-DeepSeek-6.7B, medium@1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0465003b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/conda_envs/hf_env/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [02:33<00:00, 76.71s/it] \n",
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Using DeepSeek Coder 6.7B Base\n",
      "✅ [1/35] Variation 1/1 generated\n",
      "✅ [2/35] Variation 1/1 generated\n",
      "✅ [3/35] Variation 1/1 generated\n",
      "✅ [4/35] Variation 1/1 generated\n",
      "✅ [5/35] Variation 1/1 generated\n",
      "✅ [6/35] Variation 1/1 generated\n",
      "✅ [7/35] Variation 1/1 generated\n",
      "✅ [8/35] Variation 1/1 generated\n",
      "✅ [9/35] Variation 1/1 generated\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ [10/35] Variation 1/1 generated\n",
      "✅ [11/35] Variation 1/1 generated\n",
      "✅ [12/35] Variation 1/1 generated\n",
      "✅ [13/35] Variation 1/1 generated\n",
      "✅ [14/35] Variation 1/1 generated\n",
      "✅ [15/35] Variation 1/1 generated\n",
      "✅ [16/35] Variation 1/1 generated\n",
      "✅ [17/35] Variation 1/1 generated\n",
      "✅ [18/35] Variation 1/1 generated\n",
      "✅ [19/35] Variation 1/1 generated\n",
      "✅ [20/35] Variation 1/1 generated\n",
      "✅ [21/35] Variation 1/1 generated\n",
      "✅ [22/35] Variation 1/1 generated\n",
      "✅ [23/35] Variation 1/1 generated\n",
      "✅ [24/35] Variation 1/1 generated\n",
      "✅ [25/35] Variation 1/1 generated\n",
      "✅ [26/35] Variation 1/1 generated\n",
      "✅ [27/35] Variation 1/1 generated\n",
      "✅ [28/35] Variation 1/1 generated\n",
      "✅ [29/35] Variation 1/1 generated\n",
      "✅ [30/35] Variation 1/1 generated\n",
      "✅ [31/35] Variation 1/1 generated\n",
      "✅ [32/35] Variation 1/1 generated\n",
      "✅ [33/35] Variation 1/1 generated\n",
      "✅ [34/35] Variation 1/1 generated\n",
      "✅ [35/35] Variation 1/1 generated\n",
      "\n",
      "✅ All 35 variations saved to /tmp/medium@1_variations.yml\n"
     ]
    }
   ],
   "source": [
    "import yaml\n",
    "from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "# === CONFIG ===\n",
    "code_file = \"/tmp/medium_codes.yml\"\n",
    "prompt_file = \"/tmp/medium_prompts.yml\"\n",
    "output_file = \"/tmp/medium@1_variations.yml\"\n",
    "num_variations_per_prompt = 1\n",
    "\n",
    "# === LOAD MODEL ===\n",
    "model_name = \"deepseek-ai/deepseek-coder-6.7b-base\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.float16,\n",
    "    trust_remote_code=True\n",
    ").to(\"cuda\")  # manually move to GPU 0\n",
    "\n",
    "print(\"✅ Using DeepSeek Coder 6.7B Base\")\n",
    "\n",
    "generator = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, device=0)\n",
    "\n",
    "# === LOAD INPUT DATA ===\n",
    "with open(code_file, \"r\") as f:\n",
    "    codes = yaml.safe_load(f)\n",
    "\n",
    "with open(prompt_file, \"r\") as f:\n",
    "    prompts = yaml.safe_load(f)\n",
    "\n",
    "assert len(codes) == len(prompts), \"❌ Code and prompt counts do not match!\"\n",
    "\n",
    "# === BUILD PROMPT BATCH ===\n",
    "batch_prompts = [\n",
    "    f\"### Base Verilog Code:\\n{c['code'].strip()}\\n### Variation Instruction:\\n{p['prompt'].strip()}\\n### Modified Code:\\n\"\n",
    "    for c, p in zip(codes, prompts)\n",
    "]\n",
    "\n",
    "# === RUN MULTIPLE VARIATIONS PER PROMPT ===\n",
    "outputs = []\n",
    "\n",
    "for i, prompt in enumerate(batch_prompts):\n",
    "    responses = generator(\n",
    "        prompt,\n",
    "        max_new_tokens=2048,\n",
    "        do_sample=True,\n",
    "        temperature=0.8,\n",
    "        num_return_sequences=num_variations_per_prompt\n",
    "    )\n",
    "    for j, resp in enumerate(responses):\n",
    "        response = resp['generated_text']\n",
    "        variation = response.split(\"### Modified Code:\\n\")[-1].strip()\n",
    "        outputs.append({\"code\": variation})\n",
    "        print(f\"✅ [{i+1}/{len(batch_prompts)}] Variation {j+1}/{num_variations_per_prompt} generated\")\n",
    "\n",
    "# === SAVE OUTPUTS WITH BLOCK STYLE ===\n",
    "def str_presenter(dumper, data):\n",
    "    if '\\n' in data:\n",
    "        return dumper.represent_scalar('tag:yaml.org,2002:str', data, style='|')\n",
    "    return dumper.represent_scalar('tag:yaml.org,2002:str', data)\n",
    "\n",
    "yaml.add_representer(str, str_presenter)\n",
    "\n",
    "with open(output_file, \"w\") as f:\n",
    "    yaml.dump(outputs, f, sort_keys=False)\n",
    "\n",
    "print(f\"\\n✅ All {len(outputs)} variations saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8a3052c",
   "metadata": {},
   "source": [
    "## Batch Inferencing (RTLCoder-DeepSeek-6.7B, complex@1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "13a7f411",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/conda_envs/hf_env/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [01:37<00:00, 48.77s/it]\n",
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Using DeepSeek Coder 6.7B Base\n",
      "✅ [1/30] Variation 1/1 generated\n",
      "✅ [2/30] Variation 1/1 generated\n",
      "✅ [3/30] Variation 1/1 generated\n",
      "✅ [4/30] Variation 1/1 generated\n",
      "✅ [5/30] Variation 1/1 generated\n",
      "✅ [6/30] Variation 1/1 generated\n",
      "✅ [7/30] Variation 1/1 generated\n",
      "✅ [8/30] Variation 1/1 generated\n",
      "✅ [9/30] Variation 1/1 generated\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ [10/30] Variation 1/1 generated\n",
      "✅ [11/30] Variation 1/1 generated\n",
      "✅ [12/30] Variation 1/1 generated\n",
      "✅ [13/30] Variation 1/1 generated\n",
      "✅ [14/30] Variation 1/1 generated\n",
      "✅ [15/30] Variation 1/1 generated\n",
      "✅ [16/30] Variation 1/1 generated\n",
      "✅ [17/30] Variation 1/1 generated\n",
      "✅ [18/30] Variation 1/1 generated\n",
      "✅ [19/30] Variation 1/1 generated\n",
      "✅ [20/30] Variation 1/1 generated\n",
      "✅ [21/30] Variation 1/1 generated\n",
      "✅ [22/30] Variation 1/1 generated\n",
      "✅ [23/30] Variation 1/1 generated\n",
      "✅ [24/30] Variation 1/1 generated\n",
      "✅ [25/30] Variation 1/1 generated\n",
      "✅ [26/30] Variation 1/1 generated\n",
      "✅ [27/30] Variation 1/1 generated\n",
      "✅ [28/30] Variation 1/1 generated\n",
      "✅ [29/30] Variation 1/1 generated\n",
      "✅ [30/30] Variation 1/1 generated\n",
      "\n",
      "✅ All 30 variations saved to /tmp/complex@1_variations.yml\n"
     ]
    }
   ],
   "source": [
    "import yaml\n",
    "from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "# === CONFIG ===\n",
    "code_file = \"/tmp/complex_codes.yml\"\n",
    "prompt_file = \"/tmp/complex_prompts.yml\"\n",
    "output_file = \"/tmp/complex@1_variations.yml\"\n",
    "num_variations_per_prompt = 1\n",
    "\n",
    "# === LOAD MODEL ===\n",
    "model_name = \"deepseek-ai/deepseek-coder-6.7b-base\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.float16,\n",
    "    trust_remote_code=True\n",
    ").to(\"cuda\")  # manually move to GPU 0\n",
    "\n",
    "print(\"✅ Using DeepSeek Coder 6.7B Base\")\n",
    "\n",
    "generator = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, device=0)\n",
    "\n",
    "# === LOAD INPUT DATA ===\n",
    "with open(code_file, \"r\") as f:\n",
    "    codes = yaml.safe_load(f)\n",
    "\n",
    "with open(prompt_file, \"r\") as f:\n",
    "    prompts = yaml.safe_load(f)\n",
    "\n",
    "assert len(codes) == len(prompts), \"❌ Code and prompt counts do not match!\"\n",
    "\n",
    "# === BUILD PROMPT BATCH ===\n",
    "batch_prompts = [\n",
    "    f\"### Base Verilog Code:\\n{c['code'].strip()}\\n### Variation Instruction:\\n{p['prompt'].strip()}\\n### Modified Code:\\n\"\n",
    "    for c, p in zip(codes, prompts)\n",
    "]\n",
    "\n",
    "# === RUN MULTIPLE VARIATIONS PER PROMPT ===\n",
    "outputs = []\n",
    "\n",
    "for i, prompt in enumerate(batch_prompts):\n",
    "    responses = generator(\n",
    "        prompt,\n",
    "        max_new_tokens=2048,\n",
    "        do_sample=True,\n",
    "        temperature=0.8,\n",
    "        num_return_sequences=num_variations_per_prompt\n",
    "    )\n",
    "    for j, resp in enumerate(responses):\n",
    "        response = resp['generated_text']\n",
    "        variation = response.split(\"### Modified Code:\\n\")[-1].strip()\n",
    "        outputs.append({\"code\": variation})\n",
    "        print(f\"✅ [{i+1}/{len(batch_prompts)}] Variation {j+1}/{num_variations_per_prompt} generated\")\n",
    "\n",
    "# === SAVE OUTPUTS WITH BLOCK STYLE ===\n",
    "def str_presenter(dumper, data):\n",
    "    if '\\n' in data:\n",
    "        return dumper.represent_scalar('tag:yaml.org,2002:str', data, style='|')\n",
    "    return dumper.represent_scalar('tag:yaml.org,2002:str', data)\n",
    "\n",
    "yaml.add_representer(str, str_presenter)\n",
    "\n",
    "with open(output_file, \"w\") as f:\n",
    "    yaml.dump(outputs, f, sort_keys=False)\n",
    "\n",
    "print(f\"\\n✅ All {len(outputs)} variations saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "da34a033",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13a1a2cf",
   "metadata": {},
   "source": [
    "## Batch Inferencing (RTLCoder-DeepSeek-6.7B, simple@5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "458ea236",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/conda_envs/hf_env/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [02:33<00:00, 76.59s/it] \n",
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Using DeepSeek Coder 6.7B Base\n",
      "✅ [1/35] Variation 1/5 generated\n",
      "✅ [1/35] Variation 2/5 generated\n",
      "✅ [1/35] Variation 3/5 generated\n",
      "✅ [1/35] Variation 4/5 generated\n",
      "✅ [1/35] Variation 5/5 generated\n",
      "✅ [2/35] Variation 1/5 generated\n",
      "✅ [2/35] Variation 2/5 generated\n",
      "✅ [2/35] Variation 3/5 generated\n",
      "✅ [2/35] Variation 4/5 generated\n",
      "✅ [2/35] Variation 5/5 generated\n",
      "✅ [3/35] Variation 1/5 generated\n",
      "✅ [3/35] Variation 2/5 generated\n",
      "✅ [3/35] Variation 3/5 generated\n",
      "✅ [3/35] Variation 4/5 generated\n",
      "✅ [3/35] Variation 5/5 generated\n",
      "✅ [4/35] Variation 1/5 generated\n",
      "✅ [4/35] Variation 2/5 generated\n",
      "✅ [4/35] Variation 3/5 generated\n",
      "✅ [4/35] Variation 4/5 generated\n",
      "✅ [4/35] Variation 5/5 generated\n",
      "✅ [5/35] Variation 1/5 generated\n",
      "✅ [5/35] Variation 2/5 generated\n",
      "✅ [5/35] Variation 3/5 generated\n",
      "✅ [5/35] Variation 4/5 generated\n",
      "✅ [5/35] Variation 5/5 generated\n",
      "✅ [6/35] Variation 1/5 generated\n",
      "✅ [6/35] Variation 2/5 generated\n",
      "✅ [6/35] Variation 3/5 generated\n",
      "✅ [6/35] Variation 4/5 generated\n",
      "✅ [6/35] Variation 5/5 generated\n",
      "✅ [7/35] Variation 1/5 generated\n",
      "✅ [7/35] Variation 2/5 generated\n",
      "✅ [7/35] Variation 3/5 generated\n",
      "✅ [7/35] Variation 4/5 generated\n",
      "✅ [7/35] Variation 5/5 generated\n",
      "✅ [8/35] Variation 1/5 generated\n",
      "✅ [8/35] Variation 2/5 generated\n",
      "✅ [8/35] Variation 3/5 generated\n",
      "✅ [8/35] Variation 4/5 generated\n",
      "✅ [8/35] Variation 5/5 generated\n",
      "✅ [9/35] Variation 1/5 generated\n",
      "✅ [9/35] Variation 2/5 generated\n",
      "✅ [9/35] Variation 3/5 generated\n",
      "✅ [9/35] Variation 4/5 generated\n",
      "✅ [9/35] Variation 5/5 generated\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ [10/35] Variation 1/5 generated\n",
      "✅ [10/35] Variation 2/5 generated\n",
      "✅ [10/35] Variation 3/5 generated\n",
      "✅ [10/35] Variation 4/5 generated\n",
      "✅ [10/35] Variation 5/5 generated\n",
      "✅ [11/35] Variation 1/5 generated\n",
      "✅ [11/35] Variation 2/5 generated\n",
      "✅ [11/35] Variation 3/5 generated\n",
      "✅ [11/35] Variation 4/5 generated\n",
      "✅ [11/35] Variation 5/5 generated\n",
      "✅ [12/35] Variation 1/5 generated\n",
      "✅ [12/35] Variation 2/5 generated\n",
      "✅ [12/35] Variation 3/5 generated\n",
      "✅ [12/35] Variation 4/5 generated\n",
      "✅ [12/35] Variation 5/5 generated\n",
      "✅ [13/35] Variation 1/5 generated\n",
      "✅ [13/35] Variation 2/5 generated\n",
      "✅ [13/35] Variation 3/5 generated\n",
      "✅ [13/35] Variation 4/5 generated\n",
      "✅ [13/35] Variation 5/5 generated\n",
      "✅ [14/35] Variation 1/5 generated\n",
      "✅ [14/35] Variation 2/5 generated\n",
      "✅ [14/35] Variation 3/5 generated\n",
      "✅ [14/35] Variation 4/5 generated\n",
      "✅ [14/35] Variation 5/5 generated\n",
      "✅ [15/35] Variation 1/5 generated\n",
      "✅ [15/35] Variation 2/5 generated\n",
      "✅ [15/35] Variation 3/5 generated\n",
      "✅ [15/35] Variation 4/5 generated\n",
      "✅ [15/35] Variation 5/5 generated\n",
      "✅ [16/35] Variation 1/5 generated\n",
      "✅ [16/35] Variation 2/5 generated\n",
      "✅ [16/35] Variation 3/5 generated\n",
      "✅ [16/35] Variation 4/5 generated\n",
      "✅ [16/35] Variation 5/5 generated\n",
      "✅ [17/35] Variation 1/5 generated\n",
      "✅ [17/35] Variation 2/5 generated\n",
      "✅ [17/35] Variation 3/5 generated\n",
      "✅ [17/35] Variation 4/5 generated\n",
      "✅ [17/35] Variation 5/5 generated\n",
      "✅ [18/35] Variation 1/5 generated\n",
      "✅ [18/35] Variation 2/5 generated\n",
      "✅ [18/35] Variation 3/5 generated\n",
      "✅ [18/35] Variation 4/5 generated\n",
      "✅ [18/35] Variation 5/5 generated\n",
      "✅ [19/35] Variation 1/5 generated\n",
      "✅ [19/35] Variation 2/5 generated\n",
      "✅ [19/35] Variation 3/5 generated\n",
      "✅ [19/35] Variation 4/5 generated\n",
      "✅ [19/35] Variation 5/5 generated\n",
      "✅ [20/35] Variation 1/5 generated\n",
      "✅ [20/35] Variation 2/5 generated\n",
      "✅ [20/35] Variation 3/5 generated\n",
      "✅ [20/35] Variation 4/5 generated\n",
      "✅ [20/35] Variation 5/5 generated\n",
      "✅ [21/35] Variation 1/5 generated\n",
      "✅ [21/35] Variation 2/5 generated\n",
      "✅ [21/35] Variation 3/5 generated\n",
      "✅ [21/35] Variation 4/5 generated\n",
      "✅ [21/35] Variation 5/5 generated\n",
      "✅ [22/35] Variation 1/5 generated\n",
      "✅ [22/35] Variation 2/5 generated\n",
      "✅ [22/35] Variation 3/5 generated\n",
      "✅ [22/35] Variation 4/5 generated\n",
      "✅ [22/35] Variation 5/5 generated\n",
      "✅ [23/35] Variation 1/5 generated\n",
      "✅ [23/35] Variation 2/5 generated\n",
      "✅ [23/35] Variation 3/5 generated\n",
      "✅ [23/35] Variation 4/5 generated\n",
      "✅ [23/35] Variation 5/5 generated\n",
      "✅ [24/35] Variation 1/5 generated\n",
      "✅ [24/35] Variation 2/5 generated\n",
      "✅ [24/35] Variation 3/5 generated\n",
      "✅ [24/35] Variation 4/5 generated\n",
      "✅ [24/35] Variation 5/5 generated\n",
      "✅ [25/35] Variation 1/5 generated\n",
      "✅ [25/35] Variation 2/5 generated\n",
      "✅ [25/35] Variation 3/5 generated\n",
      "✅ [25/35] Variation 4/5 generated\n",
      "✅ [25/35] Variation 5/5 generated\n",
      "✅ [26/35] Variation 1/5 generated\n",
      "✅ [26/35] Variation 2/5 generated\n",
      "✅ [26/35] Variation 3/5 generated\n",
      "✅ [26/35] Variation 4/5 generated\n",
      "✅ [26/35] Variation 5/5 generated\n",
      "✅ [27/35] Variation 1/5 generated\n",
      "✅ [27/35] Variation 2/5 generated\n",
      "✅ [27/35] Variation 3/5 generated\n",
      "✅ [27/35] Variation 4/5 generated\n",
      "✅ [27/35] Variation 5/5 generated\n",
      "✅ [28/35] Variation 1/5 generated\n",
      "✅ [28/35] Variation 2/5 generated\n",
      "✅ [28/35] Variation 3/5 generated\n",
      "✅ [28/35] Variation 4/5 generated\n",
      "✅ [28/35] Variation 5/5 generated\n",
      "✅ [29/35] Variation 1/5 generated\n",
      "✅ [29/35] Variation 2/5 generated\n",
      "✅ [29/35] Variation 3/5 generated\n",
      "✅ [29/35] Variation 4/5 generated\n",
      "✅ [29/35] Variation 5/5 generated\n",
      "✅ [30/35] Variation 1/5 generated\n",
      "✅ [30/35] Variation 2/5 generated\n",
      "✅ [30/35] Variation 3/5 generated\n",
      "✅ [30/35] Variation 4/5 generated\n",
      "✅ [30/35] Variation 5/5 generated\n",
      "✅ [31/35] Variation 1/5 generated\n",
      "✅ [31/35] Variation 2/5 generated\n",
      "✅ [31/35] Variation 3/5 generated\n",
      "✅ [31/35] Variation 4/5 generated\n",
      "✅ [31/35] Variation 5/5 generated\n",
      "✅ [32/35] Variation 1/5 generated\n",
      "✅ [32/35] Variation 2/5 generated\n",
      "✅ [32/35] Variation 3/5 generated\n",
      "✅ [32/35] Variation 4/5 generated\n",
      "✅ [32/35] Variation 5/5 generated\n",
      "✅ [33/35] Variation 1/5 generated\n",
      "✅ [33/35] Variation 2/5 generated\n",
      "✅ [33/35] Variation 3/5 generated\n",
      "✅ [33/35] Variation 4/5 generated\n",
      "✅ [33/35] Variation 5/5 generated\n",
      "✅ [34/35] Variation 1/5 generated\n",
      "✅ [34/35] Variation 2/5 generated\n",
      "✅ [34/35] Variation 3/5 generated\n",
      "✅ [34/35] Variation 4/5 generated\n",
      "✅ [34/35] Variation 5/5 generated\n",
      "✅ [35/35] Variation 1/5 generated\n",
      "✅ [35/35] Variation 2/5 generated\n",
      "✅ [35/35] Variation 3/5 generated\n",
      "✅ [35/35] Variation 4/5 generated\n",
      "✅ [35/35] Variation 5/5 generated\n",
      "\n",
      "✅ All 175 variations saved to /tmp/simple@5_variations.yml\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/conda_envs/hf_env/lib/python3.9/site-packages/IPython/core/interactiveshell.py:3558: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import yaml\n",
    "from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "# === CONFIG ===\n",
    "code_file = \"/tmp/simple_codes.yml\"\n",
    "prompt_file = \"/tmp/simple_prompts.yml\"\n",
    "output_file = \"/tmp/simple@5_variations.yml\"\n",
    "num_variations_per_prompt = 5\n",
    "\n",
    "# === LOAD MODEL ===\n",
    "model_name = \"deepseek-ai/deepseek-coder-6.7b-base\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.float16,\n",
    "    trust_remote_code=True\n",
    ").to(\"cuda\")  # manually move to GPU 0\n",
    "\n",
    "print(\"✅ Using DeepSeek Coder 6.7B Base\")\n",
    "\n",
    "generator = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, device=0)\n",
    "\n",
    "# === LOAD INPUT DATA ===\n",
    "with open(code_file, \"r\") as f:\n",
    "    codes = yaml.safe_load(f)\n",
    "\n",
    "with open(prompt_file, \"r\") as f:\n",
    "    prompts = yaml.safe_load(f)\n",
    "\n",
    "assert len(codes) == len(prompts), \"❌ Code and prompt counts do not match!\"\n",
    "\n",
    "# === BUILD PROMPT BATCH ===\n",
    "batch_prompts = [\n",
    "    f\"### Base Verilog Code:\\n{c['code'].strip()}\\n### Variation Instruction:\\n{p['prompt'].strip()}\\n### Modified Code:\\n\"\n",
    "    for c, p in zip(codes, prompts)\n",
    "]\n",
    "\n",
    "# === RUN MULTIPLE VARIATIONS PER PROMPT ===\n",
    "outputs = []\n",
    "\n",
    "for i, prompt in enumerate(batch_prompts):\n",
    "    responses = generator(\n",
    "        prompt,\n",
    "        max_new_tokens=2048,\n",
    "        do_sample=True,\n",
    "        temperature=0.8,\n",
    "        num_return_sequences=num_variations_per_prompt\n",
    "    )\n",
    "    for j, resp in enumerate(responses):\n",
    "        response = resp['generated_text']\n",
    "        variation = response.split(\"### Modified Code:\\n\")[-1].strip()\n",
    "        outputs.append({\"code\": variation})\n",
    "        print(f\"✅ [{i+1}/{len(batch_prompts)}] Variation {j+1}/{num_variations_per_prompt} generated\")\n",
    "\n",
    "# === SAVE OUTPUTS WITH BLOCK STYLE ===\n",
    "def str_presenter(dumper, data):\n",
    "    if '\\n' in data:\n",
    "        return dumper.represent_scalar('tag:yaml.org,2002:str', data, style='|')\n",
    "    return dumper.represent_scalar('tag:yaml.org,2002:str', data)\n",
    "\n",
    "yaml.add_representer(str, str_presenter)\n",
    "\n",
    "with open(output_file, \"w\") as f:\n",
    "    yaml.dump(outputs, f, sort_keys=False)\n",
    "\n",
    "print(f\"\\n✅ All {len(outputs)} variations saved to {output_file}\")\n",
    "\n",
    "sys.exit(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47332a05",
   "metadata": {},
   "source": [
    "## Batch Inferencing (RTLCoder-DeepSeek-6.7B, medium@5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "977dd0bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/conda_envs/hf_env/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [06:10<00:00, 185.25s/it]\n",
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Using DeepSeek Coder 6.7B Base\n",
      "✅ [1/35] Variation 1/5 generated\n",
      "✅ [1/35] Variation 2/5 generated\n",
      "✅ [1/35] Variation 3/5 generated\n",
      "✅ [1/35] Variation 4/5 generated\n",
      "✅ [1/35] Variation 5/5 generated\n",
      "✅ [2/35] Variation 1/5 generated\n",
      "✅ [2/35] Variation 2/5 generated\n",
      "✅ [2/35] Variation 3/5 generated\n",
      "✅ [2/35] Variation 4/5 generated\n",
      "✅ [2/35] Variation 5/5 generated\n",
      "✅ [3/35] Variation 1/5 generated\n",
      "✅ [3/35] Variation 2/5 generated\n",
      "✅ [3/35] Variation 3/5 generated\n",
      "✅ [3/35] Variation 4/5 generated\n",
      "✅ [3/35] Variation 5/5 generated\n",
      "✅ [4/35] Variation 1/5 generated\n",
      "✅ [4/35] Variation 2/5 generated\n",
      "✅ [4/35] Variation 3/5 generated\n",
      "✅ [4/35] Variation 4/5 generated\n",
      "✅ [4/35] Variation 5/5 generated\n",
      "✅ [5/35] Variation 1/5 generated\n",
      "✅ [5/35] Variation 2/5 generated\n",
      "✅ [5/35] Variation 3/5 generated\n",
      "✅ [5/35] Variation 4/5 generated\n",
      "✅ [5/35] Variation 5/5 generated\n",
      "✅ [6/35] Variation 1/5 generated\n",
      "✅ [6/35] Variation 2/5 generated\n",
      "✅ [6/35] Variation 3/5 generated\n",
      "✅ [6/35] Variation 4/5 generated\n",
      "✅ [6/35] Variation 5/5 generated\n",
      "✅ [7/35] Variation 1/5 generated\n",
      "✅ [7/35] Variation 2/5 generated\n",
      "✅ [7/35] Variation 3/5 generated\n",
      "✅ [7/35] Variation 4/5 generated\n",
      "✅ [7/35] Variation 5/5 generated\n",
      "✅ [8/35] Variation 1/5 generated\n",
      "✅ [8/35] Variation 2/5 generated\n",
      "✅ [8/35] Variation 3/5 generated\n",
      "✅ [8/35] Variation 4/5 generated\n",
      "✅ [8/35] Variation 5/5 generated\n",
      "✅ [9/35] Variation 1/5 generated\n",
      "✅ [9/35] Variation 2/5 generated\n",
      "✅ [9/35] Variation 3/5 generated\n",
      "✅ [9/35] Variation 4/5 generated\n",
      "✅ [9/35] Variation 5/5 generated\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ [10/35] Variation 1/5 generated\n",
      "✅ [10/35] Variation 2/5 generated\n",
      "✅ [10/35] Variation 3/5 generated\n",
      "✅ [10/35] Variation 4/5 generated\n",
      "✅ [10/35] Variation 5/5 generated\n",
      "✅ [11/35] Variation 1/5 generated\n",
      "✅ [11/35] Variation 2/5 generated\n",
      "✅ [11/35] Variation 3/5 generated\n",
      "✅ [11/35] Variation 4/5 generated\n",
      "✅ [11/35] Variation 5/5 generated\n",
      "✅ [12/35] Variation 1/5 generated\n",
      "✅ [12/35] Variation 2/5 generated\n",
      "✅ [12/35] Variation 3/5 generated\n",
      "✅ [12/35] Variation 4/5 generated\n",
      "✅ [12/35] Variation 5/5 generated\n",
      "✅ [13/35] Variation 1/5 generated\n",
      "✅ [13/35] Variation 2/5 generated\n",
      "✅ [13/35] Variation 3/5 generated\n",
      "✅ [13/35] Variation 4/5 generated\n",
      "✅ [13/35] Variation 5/5 generated\n",
      "✅ [14/35] Variation 1/5 generated\n",
      "✅ [14/35] Variation 2/5 generated\n",
      "✅ [14/35] Variation 3/5 generated\n",
      "✅ [14/35] Variation 4/5 generated\n",
      "✅ [14/35] Variation 5/5 generated\n",
      "✅ [15/35] Variation 1/5 generated\n",
      "✅ [15/35] Variation 2/5 generated\n",
      "✅ [15/35] Variation 3/5 generated\n",
      "✅ [15/35] Variation 4/5 generated\n",
      "✅ [15/35] Variation 5/5 generated\n",
      "✅ [16/35] Variation 1/5 generated\n",
      "✅ [16/35] Variation 2/5 generated\n",
      "✅ [16/35] Variation 3/5 generated\n",
      "✅ [16/35] Variation 4/5 generated\n",
      "✅ [16/35] Variation 5/5 generated\n",
      "✅ [17/35] Variation 1/5 generated\n",
      "✅ [17/35] Variation 2/5 generated\n",
      "✅ [17/35] Variation 3/5 generated\n",
      "✅ [17/35] Variation 4/5 generated\n",
      "✅ [17/35] Variation 5/5 generated\n",
      "✅ [18/35] Variation 1/5 generated\n",
      "✅ [18/35] Variation 2/5 generated\n",
      "✅ [18/35] Variation 3/5 generated\n",
      "✅ [18/35] Variation 4/5 generated\n",
      "✅ [18/35] Variation 5/5 generated\n",
      "✅ [19/35] Variation 1/5 generated\n",
      "✅ [19/35] Variation 2/5 generated\n",
      "✅ [19/35] Variation 3/5 generated\n",
      "✅ [19/35] Variation 4/5 generated\n",
      "✅ [19/35] Variation 5/5 generated\n",
      "✅ [20/35] Variation 1/5 generated\n",
      "✅ [20/35] Variation 2/5 generated\n",
      "✅ [20/35] Variation 3/5 generated\n",
      "✅ [20/35] Variation 4/5 generated\n",
      "✅ [20/35] Variation 5/5 generated\n",
      "✅ [21/35] Variation 1/5 generated\n",
      "✅ [21/35] Variation 2/5 generated\n",
      "✅ [21/35] Variation 3/5 generated\n",
      "✅ [21/35] Variation 4/5 generated\n",
      "✅ [21/35] Variation 5/5 generated\n",
      "✅ [22/35] Variation 1/5 generated\n",
      "✅ [22/35] Variation 2/5 generated\n",
      "✅ [22/35] Variation 3/5 generated\n",
      "✅ [22/35] Variation 4/5 generated\n",
      "✅ [22/35] Variation 5/5 generated\n",
      "✅ [23/35] Variation 1/5 generated\n",
      "✅ [23/35] Variation 2/5 generated\n",
      "✅ [23/35] Variation 3/5 generated\n",
      "✅ [23/35] Variation 4/5 generated\n",
      "✅ [23/35] Variation 5/5 generated\n",
      "✅ [24/35] Variation 1/5 generated\n",
      "✅ [24/35] Variation 2/5 generated\n",
      "✅ [24/35] Variation 3/5 generated\n",
      "✅ [24/35] Variation 4/5 generated\n",
      "✅ [24/35] Variation 5/5 generated\n",
      "✅ [25/35] Variation 1/5 generated\n",
      "✅ [25/35] Variation 2/5 generated\n",
      "✅ [25/35] Variation 3/5 generated\n",
      "✅ [25/35] Variation 4/5 generated\n",
      "✅ [25/35] Variation 5/5 generated\n",
      "✅ [26/35] Variation 1/5 generated\n",
      "✅ [26/35] Variation 2/5 generated\n",
      "✅ [26/35] Variation 3/5 generated\n",
      "✅ [26/35] Variation 4/5 generated\n",
      "✅ [26/35] Variation 5/5 generated\n",
      "✅ [27/35] Variation 1/5 generated\n",
      "✅ [27/35] Variation 2/5 generated\n",
      "✅ [27/35] Variation 3/5 generated\n",
      "✅ [27/35] Variation 4/5 generated\n",
      "✅ [27/35] Variation 5/5 generated\n",
      "✅ [28/35] Variation 1/5 generated\n",
      "✅ [28/35] Variation 2/5 generated\n",
      "✅ [28/35] Variation 3/5 generated\n",
      "✅ [28/35] Variation 4/5 generated\n",
      "✅ [28/35] Variation 5/5 generated\n",
      "✅ [29/35] Variation 1/5 generated\n",
      "✅ [29/35] Variation 2/5 generated\n",
      "✅ [29/35] Variation 3/5 generated\n",
      "✅ [29/35] Variation 4/5 generated\n",
      "✅ [29/35] Variation 5/5 generated\n",
      "✅ [30/35] Variation 1/5 generated\n",
      "✅ [30/35] Variation 2/5 generated\n",
      "✅ [30/35] Variation 3/5 generated\n",
      "✅ [30/35] Variation 4/5 generated\n",
      "✅ [30/35] Variation 5/5 generated\n",
      "✅ [31/35] Variation 1/5 generated\n",
      "✅ [31/35] Variation 2/5 generated\n",
      "✅ [31/35] Variation 3/5 generated\n",
      "✅ [31/35] Variation 4/5 generated\n",
      "✅ [31/35] Variation 5/5 generated\n",
      "✅ [32/35] Variation 1/5 generated\n",
      "✅ [32/35] Variation 2/5 generated\n",
      "✅ [32/35] Variation 3/5 generated\n",
      "✅ [32/35] Variation 4/5 generated\n",
      "✅ [32/35] Variation 5/5 generated\n",
      "✅ [33/35] Variation 1/5 generated\n",
      "✅ [33/35] Variation 2/5 generated\n",
      "✅ [33/35] Variation 3/5 generated\n",
      "✅ [33/35] Variation 4/5 generated\n",
      "✅ [33/35] Variation 5/5 generated\n",
      "✅ [34/35] Variation 1/5 generated\n",
      "✅ [34/35] Variation 2/5 generated\n",
      "✅ [34/35] Variation 3/5 generated\n",
      "✅ [34/35] Variation 4/5 generated\n",
      "✅ [34/35] Variation 5/5 generated\n",
      "✅ [35/35] Variation 1/5 generated\n",
      "✅ [35/35] Variation 2/5 generated\n",
      "✅ [35/35] Variation 3/5 generated\n",
      "✅ [35/35] Variation 4/5 generated\n",
      "✅ [35/35] Variation 5/5 generated\n",
      "\n",
      "✅ All 175 variations saved to /tmp/medium@5_variations.yml\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import yaml\n",
    "from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "# === CONFIG ===\n",
    "code_file = \"/tmp/medium_codes.yml\"\n",
    "prompt_file = \"/tmp/medium_prompts.yml\"\n",
    "output_file = \"/tmp/medium@5_variations.yml\"\n",
    "num_variations_per_prompt = 5\n",
    "\n",
    "# === LOAD MODEL ===\n",
    "model_name = \"deepseek-ai/deepseek-coder-6.7b-base\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.float16,\n",
    "    trust_remote_code=True\n",
    ").to(\"cuda\")  # manually move to GPU 0\n",
    "\n",
    "print(\"✅ Using DeepSeek Coder 6.7B Base\")\n",
    "\n",
    "generator = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, device=0)\n",
    "\n",
    "# === LOAD INPUT DATA ===\n",
    "with open(code_file, \"r\") as f:\n",
    "    codes = yaml.safe_load(f)\n",
    "\n",
    "with open(prompt_file, \"r\") as f:\n",
    "    prompts = yaml.safe_load(f)\n",
    "\n",
    "assert len(codes) == len(prompts), \"❌ Code and prompt counts do not match!\"\n",
    "\n",
    "# === BUILD PROMPT BATCH ===\n",
    "batch_prompts = [\n",
    "    f\"### Base Verilog Code:\\n{c['code'].strip()}\\n### Variation Instruction:\\n{p['prompt'].strip()}\\n### Modified Code:\\n\"\n",
    "    for c, p in zip(codes, prompts)\n",
    "]\n",
    "\n",
    "# === RUN MULTIPLE VARIATIONS PER PROMPT ===\n",
    "outputs = []\n",
    "\n",
    "for i, prompt in enumerate(batch_prompts):\n",
    "    responses = generator(\n",
    "        prompt,\n",
    "        max_new_tokens=2048,\n",
    "        do_sample=True,\n",
    "        temperature=0.8,\n",
    "        num_return_sequences=num_variations_per_prompt\n",
    "    )\n",
    "    for j, resp in enumerate(responses):\n",
    "        response = resp['generated_text']\n",
    "        variation = response.split(\"### Modified Code:\\n\")[-1].strip()\n",
    "        outputs.append({\"code\": variation})\n",
    "        print(f\"✅ [{i+1}/{len(batch_prompts)}] Variation {j+1}/{num_variations_per_prompt} generated\")\n",
    "\n",
    "# === SAVE OUTPUTS WITH BLOCK STYLE ===\n",
    "def str_presenter(dumper, data):\n",
    "    if '\\n' in data:\n",
    "        return dumper.represent_scalar('tag:yaml.org,2002:str', data, style='|')\n",
    "    return dumper.represent_scalar('tag:yaml.org,2002:str', data)\n",
    "\n",
    "yaml.add_representer(str, str_presenter)\n",
    "\n",
    "with open(output_file, \"w\") as f:\n",
    "    yaml.dump(outputs, f, sort_keys=False)\n",
    "\n",
    "print(f\"\\n✅ All {len(outputs)} variations saved to {output_file}\")\n",
    "\n",
    "exit(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22c7e5f4",
   "metadata": {},
   "source": [
    "## Batch Inferencing (RTLCoder-DeepSeek-6.7B, complex@5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93ed7349",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "# === CONFIG ===\n",
    "code_file = \"/tmp/complex_codes.yml\"\n",
    "prompt_file = \"/tmp/complex_prompts.yml\"\n",
    "output_file = \"/tmp/complex@5_variations.yml\"\n",
    "num_variations_per_prompt = 5\n",
    "\n",
    "# === LOAD MODEL ===\n",
    "model_name = \"deepseek-ai/deepseek-coder-6.7b-base\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.float16,\n",
    "    trust_remote_code=True\n",
    ").to(\"cuda\")  # manually move to GPU 0\n",
    "\n",
    "print(\"✅ Using DeepSeek Coder 6.7B Base\")\n",
    "\n",
    "generator = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, device=0)\n",
    "\n",
    "# === LOAD INPUT DATA ===\n",
    "with open(code_file, \"r\") as f:\n",
    "    codes = yaml.safe_load(f)\n",
    "\n",
    "with open(prompt_file, \"r\") as f:\n",
    "    prompts = yaml.safe_load(f)\n",
    "\n",
    "assert len(codes) == len(prompts), \"❌ Code and prompt counts do not match!\"\n",
    "\n",
    "# === BUILD PROMPT BATCH ===\n",
    "batch_prompts = [\n",
    "    f\"### Base Verilog Code:\\n{c['code'].strip()}\\n### Variation Instruction:\\n{p['prompt'].strip()}\\n### Modified Code:\\n\"\n",
    "    for c, p in zip(codes, prompts)\n",
    "]\n",
    "\n",
    "# === RUN MULTIPLE VARIATIONS PER PROMPT ===\n",
    "outputs = []\n",
    "\n",
    "for i, prompt in enumerate(batch_prompts):\n",
    "    responses = generator(\n",
    "        prompt,\n",
    "        max_new_tokens=2048,\n",
    "        do_sample=True,\n",
    "        temperature=0.8,\n",
    "        num_return_sequences=num_variations_per_prompt\n",
    "    )\n",
    "    for j, resp in enumerate(responses):\n",
    "        response = resp['generated_text']\n",
    "        variation = response.split(\"### Modified Code:\\n\")[-1].strip()\n",
    "        outputs.append({\"code\": variation})\n",
    "        print(f\"✅ [{i+1}/{len(batch_prompts)}] Variation {j+1}/{num_variations_per_prompt} generated\")\n",
    "\n",
    "# === SAVE OUTPUTS WITH BLOCK STYLE ===\n",
    "def str_presenter(dumper, data):\n",
    "    if '\\n' in data:\n",
    "        return dumper.represent_scalar('tag:yaml.org,2002:str', data, style='|')\n",
    "    return dumper.represent_scalar('tag:yaml.org,2002:str', data)\n",
    "\n",
    "yaml.add_representer(str, str_presenter)\n",
    "\n",
    "with open(output_file, \"w\") as f:\n",
    "    yaml.dump(outputs, f, sort_keys=False)\n",
    "\n",
    "print(f\"\\n✅ All {len(outputs)} variations saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "072377b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "592721e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ec8820c7",
   "metadata": {},
   "source": [
    "## Batch Inferencing (RTLCoder-DeepSeek-6.7B, simple@10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "29d6bc85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "618271f86b8f4b5380587f08c618cd23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Using DeepSeek Coder 6.7B Base\n",
      "✅ [1/35] Variation 1/10 generated\n",
      "✅ [1/35] Variation 2/10 generated\n",
      "✅ [1/35] Variation 3/10 generated\n",
      "✅ [1/35] Variation 4/10 generated\n",
      "✅ [1/35] Variation 5/10 generated\n",
      "✅ [1/35] Variation 6/10 generated\n",
      "✅ [1/35] Variation 7/10 generated\n",
      "✅ [1/35] Variation 8/10 generated\n",
      "✅ [1/35] Variation 9/10 generated\n",
      "✅ [1/35] Variation 10/10 generated\n",
      "✅ [2/35] Variation 1/10 generated\n",
      "✅ [2/35] Variation 2/10 generated\n",
      "✅ [2/35] Variation 3/10 generated\n",
      "✅ [2/35] Variation 4/10 generated\n",
      "✅ [2/35] Variation 5/10 generated\n",
      "✅ [2/35] Variation 6/10 generated\n",
      "✅ [2/35] Variation 7/10 generated\n",
      "✅ [2/35] Variation 8/10 generated\n",
      "✅ [2/35] Variation 9/10 generated\n",
      "✅ [2/35] Variation 10/10 generated\n",
      "✅ [3/35] Variation 1/10 generated\n",
      "✅ [3/35] Variation 2/10 generated\n",
      "✅ [3/35] Variation 3/10 generated\n",
      "✅ [3/35] Variation 4/10 generated\n",
      "✅ [3/35] Variation 5/10 generated\n",
      "✅ [3/35] Variation 6/10 generated\n",
      "✅ [3/35] Variation 7/10 generated\n",
      "✅ [3/35] Variation 8/10 generated\n",
      "✅ [3/35] Variation 9/10 generated\n",
      "✅ [3/35] Variation 10/10 generated\n",
      "✅ [4/35] Variation 1/10 generated\n",
      "✅ [4/35] Variation 2/10 generated\n",
      "✅ [4/35] Variation 3/10 generated\n",
      "✅ [4/35] Variation 4/10 generated\n",
      "✅ [4/35] Variation 5/10 generated\n",
      "✅ [4/35] Variation 6/10 generated\n",
      "✅ [4/35] Variation 7/10 generated\n",
      "✅ [4/35] Variation 8/10 generated\n",
      "✅ [4/35] Variation 9/10 generated\n",
      "✅ [4/35] Variation 10/10 generated\n",
      "✅ [5/35] Variation 1/10 generated\n",
      "✅ [5/35] Variation 2/10 generated\n",
      "✅ [5/35] Variation 3/10 generated\n",
      "✅ [5/35] Variation 4/10 generated\n",
      "✅ [5/35] Variation 5/10 generated\n",
      "✅ [5/35] Variation 6/10 generated\n",
      "✅ [5/35] Variation 7/10 generated\n",
      "✅ [5/35] Variation 8/10 generated\n",
      "✅ [5/35] Variation 9/10 generated\n",
      "✅ [5/35] Variation 10/10 generated\n",
      "✅ [6/35] Variation 1/10 generated\n",
      "✅ [6/35] Variation 2/10 generated\n",
      "✅ [6/35] Variation 3/10 generated\n",
      "✅ [6/35] Variation 4/10 generated\n",
      "✅ [6/35] Variation 5/10 generated\n",
      "✅ [6/35] Variation 6/10 generated\n",
      "✅ [6/35] Variation 7/10 generated\n",
      "✅ [6/35] Variation 8/10 generated\n",
      "✅ [6/35] Variation 9/10 generated\n",
      "✅ [6/35] Variation 10/10 generated\n",
      "✅ [7/35] Variation 1/10 generated\n",
      "✅ [7/35] Variation 2/10 generated\n",
      "✅ [7/35] Variation 3/10 generated\n",
      "✅ [7/35] Variation 4/10 generated\n",
      "✅ [7/35] Variation 5/10 generated\n",
      "✅ [7/35] Variation 6/10 generated\n",
      "✅ [7/35] Variation 7/10 generated\n",
      "✅ [7/35] Variation 8/10 generated\n",
      "✅ [7/35] Variation 9/10 generated\n",
      "✅ [7/35] Variation 10/10 generated\n",
      "✅ [8/35] Variation 1/10 generated\n",
      "✅ [8/35] Variation 2/10 generated\n",
      "✅ [8/35] Variation 3/10 generated\n",
      "✅ [8/35] Variation 4/10 generated\n",
      "✅ [8/35] Variation 5/10 generated\n",
      "✅ [8/35] Variation 6/10 generated\n",
      "✅ [8/35] Variation 7/10 generated\n",
      "✅ [8/35] Variation 8/10 generated\n",
      "✅ [8/35] Variation 9/10 generated\n",
      "✅ [8/35] Variation 10/10 generated\n",
      "✅ [9/35] Variation 1/10 generated\n",
      "✅ [9/35] Variation 2/10 generated\n",
      "✅ [9/35] Variation 3/10 generated\n",
      "✅ [9/35] Variation 4/10 generated\n",
      "✅ [9/35] Variation 5/10 generated\n",
      "✅ [9/35] Variation 6/10 generated\n",
      "✅ [9/35] Variation 7/10 generated\n",
      "✅ [9/35] Variation 8/10 generated\n",
      "✅ [9/35] Variation 9/10 generated\n",
      "✅ [9/35] Variation 10/10 generated\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ [10/35] Variation 1/10 generated\n",
      "✅ [10/35] Variation 2/10 generated\n",
      "✅ [10/35] Variation 3/10 generated\n",
      "✅ [10/35] Variation 4/10 generated\n",
      "✅ [10/35] Variation 5/10 generated\n",
      "✅ [10/35] Variation 6/10 generated\n",
      "✅ [10/35] Variation 7/10 generated\n",
      "✅ [10/35] Variation 8/10 generated\n",
      "✅ [10/35] Variation 9/10 generated\n",
      "✅ [10/35] Variation 10/10 generated\n",
      "✅ [11/35] Variation 1/10 generated\n",
      "✅ [11/35] Variation 2/10 generated\n",
      "✅ [11/35] Variation 3/10 generated\n",
      "✅ [11/35] Variation 4/10 generated\n",
      "✅ [11/35] Variation 5/10 generated\n",
      "✅ [11/35] Variation 6/10 generated\n",
      "✅ [11/35] Variation 7/10 generated\n",
      "✅ [11/35] Variation 8/10 generated\n",
      "✅ [11/35] Variation 9/10 generated\n",
      "✅ [11/35] Variation 10/10 generated\n",
      "✅ [12/35] Variation 1/10 generated\n",
      "✅ [12/35] Variation 2/10 generated\n",
      "✅ [12/35] Variation 3/10 generated\n",
      "✅ [12/35] Variation 4/10 generated\n",
      "✅ [12/35] Variation 5/10 generated\n",
      "✅ [12/35] Variation 6/10 generated\n",
      "✅ [12/35] Variation 7/10 generated\n",
      "✅ [12/35] Variation 8/10 generated\n",
      "✅ [12/35] Variation 9/10 generated\n",
      "✅ [12/35] Variation 10/10 generated\n",
      "✅ [13/35] Variation 1/10 generated\n",
      "✅ [13/35] Variation 2/10 generated\n",
      "✅ [13/35] Variation 3/10 generated\n",
      "✅ [13/35] Variation 4/10 generated\n",
      "✅ [13/35] Variation 5/10 generated\n",
      "✅ [13/35] Variation 6/10 generated\n",
      "✅ [13/35] Variation 7/10 generated\n",
      "✅ [13/35] Variation 8/10 generated\n",
      "✅ [13/35] Variation 9/10 generated\n",
      "✅ [13/35] Variation 10/10 generated\n",
      "✅ [14/35] Variation 1/10 generated\n",
      "✅ [14/35] Variation 2/10 generated\n",
      "✅ [14/35] Variation 3/10 generated\n",
      "✅ [14/35] Variation 4/10 generated\n",
      "✅ [14/35] Variation 5/10 generated\n",
      "✅ [14/35] Variation 6/10 generated\n",
      "✅ [14/35] Variation 7/10 generated\n",
      "✅ [14/35] Variation 8/10 generated\n",
      "✅ [14/35] Variation 9/10 generated\n",
      "✅ [14/35] Variation 10/10 generated\n",
      "✅ [15/35] Variation 1/10 generated\n",
      "✅ [15/35] Variation 2/10 generated\n",
      "✅ [15/35] Variation 3/10 generated\n",
      "✅ [15/35] Variation 4/10 generated\n",
      "✅ [15/35] Variation 5/10 generated\n",
      "✅ [15/35] Variation 6/10 generated\n",
      "✅ [15/35] Variation 7/10 generated\n",
      "✅ [15/35] Variation 8/10 generated\n",
      "✅ [15/35] Variation 9/10 generated\n",
      "✅ [15/35] Variation 10/10 generated\n",
      "✅ [16/35] Variation 1/10 generated\n",
      "✅ [16/35] Variation 2/10 generated\n",
      "✅ [16/35] Variation 3/10 generated\n",
      "✅ [16/35] Variation 4/10 generated\n",
      "✅ [16/35] Variation 5/10 generated\n",
      "✅ [16/35] Variation 6/10 generated\n",
      "✅ [16/35] Variation 7/10 generated\n",
      "✅ [16/35] Variation 8/10 generated\n",
      "✅ [16/35] Variation 9/10 generated\n",
      "✅ [16/35] Variation 10/10 generated\n",
      "✅ [17/35] Variation 1/10 generated\n",
      "✅ [17/35] Variation 2/10 generated\n",
      "✅ [17/35] Variation 3/10 generated\n",
      "✅ [17/35] Variation 4/10 generated\n",
      "✅ [17/35] Variation 5/10 generated\n",
      "✅ [17/35] Variation 6/10 generated\n",
      "✅ [17/35] Variation 7/10 generated\n",
      "✅ [17/35] Variation 8/10 generated\n",
      "✅ [17/35] Variation 9/10 generated\n",
      "✅ [17/35] Variation 10/10 generated\n",
      "✅ [18/35] Variation 1/10 generated\n",
      "✅ [18/35] Variation 2/10 generated\n",
      "✅ [18/35] Variation 3/10 generated\n",
      "✅ [18/35] Variation 4/10 generated\n",
      "✅ [18/35] Variation 5/10 generated\n",
      "✅ [18/35] Variation 6/10 generated\n",
      "✅ [18/35] Variation 7/10 generated\n",
      "✅ [18/35] Variation 8/10 generated\n",
      "✅ [18/35] Variation 9/10 generated\n",
      "✅ [18/35] Variation 10/10 generated\n",
      "✅ [19/35] Variation 1/10 generated\n",
      "✅ [19/35] Variation 2/10 generated\n",
      "✅ [19/35] Variation 3/10 generated\n",
      "✅ [19/35] Variation 4/10 generated\n",
      "✅ [19/35] Variation 5/10 generated\n",
      "✅ [19/35] Variation 6/10 generated\n",
      "✅ [19/35] Variation 7/10 generated\n",
      "✅ [19/35] Variation 8/10 generated\n",
      "✅ [19/35] Variation 9/10 generated\n",
      "✅ [19/35] Variation 10/10 generated\n",
      "✅ [20/35] Variation 1/10 generated\n",
      "✅ [20/35] Variation 2/10 generated\n",
      "✅ [20/35] Variation 3/10 generated\n",
      "✅ [20/35] Variation 4/10 generated\n",
      "✅ [20/35] Variation 5/10 generated\n",
      "✅ [20/35] Variation 6/10 generated\n",
      "✅ [20/35] Variation 7/10 generated\n",
      "✅ [20/35] Variation 8/10 generated\n",
      "✅ [20/35] Variation 9/10 generated\n",
      "✅ [20/35] Variation 10/10 generated\n",
      "✅ [21/35] Variation 1/10 generated\n",
      "✅ [21/35] Variation 2/10 generated\n",
      "✅ [21/35] Variation 3/10 generated\n",
      "✅ [21/35] Variation 4/10 generated\n",
      "✅ [21/35] Variation 5/10 generated\n",
      "✅ [21/35] Variation 6/10 generated\n",
      "✅ [21/35] Variation 7/10 generated\n",
      "✅ [21/35] Variation 8/10 generated\n",
      "✅ [21/35] Variation 9/10 generated\n",
      "✅ [21/35] Variation 10/10 generated\n",
      "✅ [22/35] Variation 1/10 generated\n",
      "✅ [22/35] Variation 2/10 generated\n",
      "✅ [22/35] Variation 3/10 generated\n",
      "✅ [22/35] Variation 4/10 generated\n",
      "✅ [22/35] Variation 5/10 generated\n",
      "✅ [22/35] Variation 6/10 generated\n",
      "✅ [22/35] Variation 7/10 generated\n",
      "✅ [22/35] Variation 8/10 generated\n",
      "✅ [22/35] Variation 9/10 generated\n",
      "✅ [22/35] Variation 10/10 generated\n",
      "✅ [23/35] Variation 1/10 generated\n",
      "✅ [23/35] Variation 2/10 generated\n",
      "✅ [23/35] Variation 3/10 generated\n",
      "✅ [23/35] Variation 4/10 generated\n",
      "✅ [23/35] Variation 5/10 generated\n",
      "✅ [23/35] Variation 6/10 generated\n",
      "✅ [23/35] Variation 7/10 generated\n",
      "✅ [23/35] Variation 8/10 generated\n",
      "✅ [23/35] Variation 9/10 generated\n",
      "✅ [23/35] Variation 10/10 generated\n",
      "✅ [24/35] Variation 1/10 generated\n",
      "✅ [24/35] Variation 2/10 generated\n",
      "✅ [24/35] Variation 3/10 generated\n",
      "✅ [24/35] Variation 4/10 generated\n",
      "✅ [24/35] Variation 5/10 generated\n",
      "✅ [24/35] Variation 6/10 generated\n",
      "✅ [24/35] Variation 7/10 generated\n",
      "✅ [24/35] Variation 8/10 generated\n",
      "✅ [24/35] Variation 9/10 generated\n",
      "✅ [24/35] Variation 10/10 generated\n",
      "✅ [25/35] Variation 1/10 generated\n",
      "✅ [25/35] Variation 2/10 generated\n",
      "✅ [25/35] Variation 3/10 generated\n",
      "✅ [25/35] Variation 4/10 generated\n",
      "✅ [25/35] Variation 5/10 generated\n",
      "✅ [25/35] Variation 6/10 generated\n",
      "✅ [25/35] Variation 7/10 generated\n",
      "✅ [25/35] Variation 8/10 generated\n",
      "✅ [25/35] Variation 9/10 generated\n",
      "✅ [25/35] Variation 10/10 generated\n",
      "✅ [26/35] Variation 1/10 generated\n",
      "✅ [26/35] Variation 2/10 generated\n",
      "✅ [26/35] Variation 3/10 generated\n",
      "✅ [26/35] Variation 4/10 generated\n",
      "✅ [26/35] Variation 5/10 generated\n",
      "✅ [26/35] Variation 6/10 generated\n",
      "✅ [26/35] Variation 7/10 generated\n",
      "✅ [26/35] Variation 8/10 generated\n",
      "✅ [26/35] Variation 9/10 generated\n",
      "✅ [26/35] Variation 10/10 generated\n",
      "✅ [27/35] Variation 1/10 generated\n",
      "✅ [27/35] Variation 2/10 generated\n",
      "✅ [27/35] Variation 3/10 generated\n",
      "✅ [27/35] Variation 4/10 generated\n",
      "✅ [27/35] Variation 5/10 generated\n",
      "✅ [27/35] Variation 6/10 generated\n",
      "✅ [27/35] Variation 7/10 generated\n",
      "✅ [27/35] Variation 8/10 generated\n",
      "✅ [27/35] Variation 9/10 generated\n",
      "✅ [27/35] Variation 10/10 generated\n",
      "✅ [28/35] Variation 1/10 generated\n",
      "✅ [28/35] Variation 2/10 generated\n",
      "✅ [28/35] Variation 3/10 generated\n",
      "✅ [28/35] Variation 4/10 generated\n",
      "✅ [28/35] Variation 5/10 generated\n",
      "✅ [28/35] Variation 6/10 generated\n",
      "✅ [28/35] Variation 7/10 generated\n",
      "✅ [28/35] Variation 8/10 generated\n",
      "✅ [28/35] Variation 9/10 generated\n",
      "✅ [28/35] Variation 10/10 generated\n",
      "✅ [29/35] Variation 1/10 generated\n",
      "✅ [29/35] Variation 2/10 generated\n",
      "✅ [29/35] Variation 3/10 generated\n",
      "✅ [29/35] Variation 4/10 generated\n",
      "✅ [29/35] Variation 5/10 generated\n",
      "✅ [29/35] Variation 6/10 generated\n",
      "✅ [29/35] Variation 7/10 generated\n",
      "✅ [29/35] Variation 8/10 generated\n",
      "✅ [29/35] Variation 9/10 generated\n",
      "✅ [29/35] Variation 10/10 generated\n",
      "✅ [30/35] Variation 1/10 generated\n",
      "✅ [30/35] Variation 2/10 generated\n",
      "✅ [30/35] Variation 3/10 generated\n",
      "✅ [30/35] Variation 4/10 generated\n",
      "✅ [30/35] Variation 5/10 generated\n",
      "✅ [30/35] Variation 6/10 generated\n",
      "✅ [30/35] Variation 7/10 generated\n",
      "✅ [30/35] Variation 8/10 generated\n",
      "✅ [30/35] Variation 9/10 generated\n",
      "✅ [30/35] Variation 10/10 generated\n",
      "✅ [31/35] Variation 1/10 generated\n",
      "✅ [31/35] Variation 2/10 generated\n",
      "✅ [31/35] Variation 3/10 generated\n",
      "✅ [31/35] Variation 4/10 generated\n",
      "✅ [31/35] Variation 5/10 generated\n",
      "✅ [31/35] Variation 6/10 generated\n",
      "✅ [31/35] Variation 7/10 generated\n",
      "✅ [31/35] Variation 8/10 generated\n",
      "✅ [31/35] Variation 9/10 generated\n",
      "✅ [31/35] Variation 10/10 generated\n",
      "✅ [32/35] Variation 1/10 generated\n",
      "✅ [32/35] Variation 2/10 generated\n",
      "✅ [32/35] Variation 3/10 generated\n",
      "✅ [32/35] Variation 4/10 generated\n",
      "✅ [32/35] Variation 5/10 generated\n",
      "✅ [32/35] Variation 6/10 generated\n",
      "✅ [32/35] Variation 7/10 generated\n",
      "✅ [32/35] Variation 8/10 generated\n",
      "✅ [32/35] Variation 9/10 generated\n",
      "✅ [32/35] Variation 10/10 generated\n",
      "✅ [33/35] Variation 1/10 generated\n",
      "✅ [33/35] Variation 2/10 generated\n",
      "✅ [33/35] Variation 3/10 generated\n",
      "✅ [33/35] Variation 4/10 generated\n",
      "✅ [33/35] Variation 5/10 generated\n",
      "✅ [33/35] Variation 6/10 generated\n",
      "✅ [33/35] Variation 7/10 generated\n",
      "✅ [33/35] Variation 8/10 generated\n",
      "✅ [33/35] Variation 9/10 generated\n",
      "✅ [33/35] Variation 10/10 generated\n",
      "✅ [34/35] Variation 1/10 generated\n",
      "✅ [34/35] Variation 2/10 generated\n",
      "✅ [34/35] Variation 3/10 generated\n",
      "✅ [34/35] Variation 4/10 generated\n",
      "✅ [34/35] Variation 5/10 generated\n",
      "✅ [34/35] Variation 6/10 generated\n",
      "✅ [34/35] Variation 7/10 generated\n",
      "✅ [34/35] Variation 8/10 generated\n",
      "✅ [34/35] Variation 9/10 generated\n",
      "✅ [34/35] Variation 10/10 generated\n",
      "✅ [35/35] Variation 1/10 generated\n",
      "✅ [35/35] Variation 2/10 generated\n",
      "✅ [35/35] Variation 3/10 generated\n",
      "✅ [35/35] Variation 4/10 generated\n",
      "✅ [35/35] Variation 5/10 generated\n",
      "✅ [35/35] Variation 6/10 generated\n",
      "✅ [35/35] Variation 7/10 generated\n",
      "✅ [35/35] Variation 8/10 generated\n",
      "✅ [35/35] Variation 9/10 generated\n",
      "✅ [35/35] Variation 10/10 generated\n",
      "\n",
      "✅ All 350 variations saved to /tmp/simple@10_variations.yml\n"
     ]
    }
   ],
   "source": [
    "import yaml\n",
    "from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "# === CONFIG ===\n",
    "code_file = \"/tmp/simple_codes.yml\"\n",
    "prompt_file = \"/tmp/simple_prompts.yml\"\n",
    "output_file = \"/tmp/simple@10_variations.yml\"\n",
    "num_variations_per_prompt = 10\n",
    "\n",
    "# === LOAD MODEL ===\n",
    "model_name = \"deepseek-ai/deepseek-coder-6.7b-base\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.float16,\n",
    "    trust_remote_code=True\n",
    ").to(\"cuda\")  # manually move to GPU 0\n",
    "\n",
    "print(\"✅ Using DeepSeek Coder 6.7B Base\")\n",
    "\n",
    "generator = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, device=0)\n",
    "\n",
    "# === LOAD INPUT DATA ===\n",
    "with open(code_file, \"r\") as f:\n",
    "    codes = yaml.safe_load(f)\n",
    "\n",
    "with open(prompt_file, \"r\") as f:\n",
    "    prompts = yaml.safe_load(f)\n",
    "\n",
    "assert len(codes) == len(prompts), \"❌ Code and prompt counts do not match!\"\n",
    "\n",
    "# === BUILD PROMPT BATCH ===\n",
    "batch_prompts = [\n",
    "    f\"### Base Verilog Code:\\n{c['code'].strip()}\\n### Variation Instruction:\\n{p['prompt'].strip()}\\n### Modified Code:\\n\"\n",
    "    for c, p in zip(codes, prompts)\n",
    "]\n",
    "\n",
    "# === RUN MULTIPLE VARIATIONS PER PROMPT ===\n",
    "outputs = []\n",
    "\n",
    "for i, prompt in enumerate(batch_prompts):\n",
    "    responses = generator(\n",
    "        prompt,\n",
    "        max_new_tokens=512,\n",
    "        do_sample=True,\n",
    "        temperature=0.8,\n",
    "        num_return_sequences=num_variations_per_prompt\n",
    "    )\n",
    "    for j, resp in enumerate(responses):\n",
    "        response = resp['generated_text']\n",
    "        variation = response.split(\"### Modified Code:\\n\")[-1].strip()\n",
    "        outputs.append({\"code\": variation})\n",
    "        print(f\"✅ [{i+1}/{len(batch_prompts)}] Variation {j+1}/{num_variations_per_prompt} generated\")\n",
    "\n",
    "# === SAVE OUTPUTS WITH BLOCK STYLE ===\n",
    "def str_presenter(dumper, data):\n",
    "    if '\\n' in data:\n",
    "        return dumper.represent_scalar('tag:yaml.org,2002:str', data, style='|')\n",
    "    return dumper.represent_scalar('tag:yaml.org,2002:str', data)\n",
    "\n",
    "yaml.add_representer(str, str_presenter)\n",
    "\n",
    "with open(output_file, \"w\") as f:\n",
    "    yaml.dump(outputs, f, sort_keys=False)\n",
    "\n",
    "print(f\"\\n✅ All {len(outputs)} variations saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0928e93a",
   "metadata": {},
   "source": [
    "## Batch Inferencing (RTLCoder-DeepSeek-6.7B, medium@10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d58b6d12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd7c9ce1b287455093d14acea2170302",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Using DeepSeek Coder 6.7B Base\n",
      "✅ [1/35] Variation 1/10 generated\n",
      "✅ [1/35] Variation 2/10 generated\n",
      "✅ [1/35] Variation 3/10 generated\n",
      "✅ [1/35] Variation 4/10 generated\n",
      "✅ [1/35] Variation 5/10 generated\n",
      "✅ [1/35] Variation 6/10 generated\n",
      "✅ [1/35] Variation 7/10 generated\n",
      "✅ [1/35] Variation 8/10 generated\n",
      "✅ [1/35] Variation 9/10 generated\n",
      "✅ [1/35] Variation 10/10 generated\n",
      "✅ [2/35] Variation 1/10 generated\n",
      "✅ [2/35] Variation 2/10 generated\n",
      "✅ [2/35] Variation 3/10 generated\n",
      "✅ [2/35] Variation 4/10 generated\n",
      "✅ [2/35] Variation 5/10 generated\n",
      "✅ [2/35] Variation 6/10 generated\n",
      "✅ [2/35] Variation 7/10 generated\n",
      "✅ [2/35] Variation 8/10 generated\n",
      "✅ [2/35] Variation 9/10 generated\n",
      "✅ [2/35] Variation 10/10 generated\n",
      "✅ [3/35] Variation 1/10 generated\n",
      "✅ [3/35] Variation 2/10 generated\n",
      "✅ [3/35] Variation 3/10 generated\n",
      "✅ [3/35] Variation 4/10 generated\n",
      "✅ [3/35] Variation 5/10 generated\n",
      "✅ [3/35] Variation 6/10 generated\n",
      "✅ [3/35] Variation 7/10 generated\n",
      "✅ [3/35] Variation 8/10 generated\n",
      "✅ [3/35] Variation 9/10 generated\n",
      "✅ [3/35] Variation 10/10 generated\n",
      "✅ [4/35] Variation 1/10 generated\n",
      "✅ [4/35] Variation 2/10 generated\n",
      "✅ [4/35] Variation 3/10 generated\n",
      "✅ [4/35] Variation 4/10 generated\n",
      "✅ [4/35] Variation 5/10 generated\n",
      "✅ [4/35] Variation 6/10 generated\n",
      "✅ [4/35] Variation 7/10 generated\n",
      "✅ [4/35] Variation 8/10 generated\n",
      "✅ [4/35] Variation 9/10 generated\n",
      "✅ [4/35] Variation 10/10 generated\n",
      "✅ [5/35] Variation 1/10 generated\n",
      "✅ [5/35] Variation 2/10 generated\n",
      "✅ [5/35] Variation 3/10 generated\n",
      "✅ [5/35] Variation 4/10 generated\n",
      "✅ [5/35] Variation 5/10 generated\n",
      "✅ [5/35] Variation 6/10 generated\n",
      "✅ [5/35] Variation 7/10 generated\n",
      "✅ [5/35] Variation 8/10 generated\n",
      "✅ [5/35] Variation 9/10 generated\n",
      "✅ [5/35] Variation 10/10 generated\n",
      "✅ [6/35] Variation 1/10 generated\n",
      "✅ [6/35] Variation 2/10 generated\n",
      "✅ [6/35] Variation 3/10 generated\n",
      "✅ [6/35] Variation 4/10 generated\n",
      "✅ [6/35] Variation 5/10 generated\n",
      "✅ [6/35] Variation 6/10 generated\n",
      "✅ [6/35] Variation 7/10 generated\n",
      "✅ [6/35] Variation 8/10 generated\n",
      "✅ [6/35] Variation 9/10 generated\n",
      "✅ [6/35] Variation 10/10 generated\n",
      "✅ [7/35] Variation 1/10 generated\n",
      "✅ [7/35] Variation 2/10 generated\n",
      "✅ [7/35] Variation 3/10 generated\n",
      "✅ [7/35] Variation 4/10 generated\n",
      "✅ [7/35] Variation 5/10 generated\n",
      "✅ [7/35] Variation 6/10 generated\n",
      "✅ [7/35] Variation 7/10 generated\n",
      "✅ [7/35] Variation 8/10 generated\n",
      "✅ [7/35] Variation 9/10 generated\n",
      "✅ [7/35] Variation 10/10 generated\n",
      "✅ [8/35] Variation 1/10 generated\n",
      "✅ [8/35] Variation 2/10 generated\n",
      "✅ [8/35] Variation 3/10 generated\n",
      "✅ [8/35] Variation 4/10 generated\n",
      "✅ [8/35] Variation 5/10 generated\n",
      "✅ [8/35] Variation 6/10 generated\n",
      "✅ [8/35] Variation 7/10 generated\n",
      "✅ [8/35] Variation 8/10 generated\n",
      "✅ [8/35] Variation 9/10 generated\n",
      "✅ [8/35] Variation 10/10 generated\n",
      "✅ [9/35] Variation 1/10 generated\n",
      "✅ [9/35] Variation 2/10 generated\n",
      "✅ [9/35] Variation 3/10 generated\n",
      "✅ [9/35] Variation 4/10 generated\n",
      "✅ [9/35] Variation 5/10 generated\n",
      "✅ [9/35] Variation 6/10 generated\n",
      "✅ [9/35] Variation 7/10 generated\n",
      "✅ [9/35] Variation 8/10 generated\n",
      "✅ [9/35] Variation 9/10 generated\n",
      "✅ [9/35] Variation 10/10 generated\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ [10/35] Variation 1/10 generated\n",
      "✅ [10/35] Variation 2/10 generated\n",
      "✅ [10/35] Variation 3/10 generated\n",
      "✅ [10/35] Variation 4/10 generated\n",
      "✅ [10/35] Variation 5/10 generated\n",
      "✅ [10/35] Variation 6/10 generated\n",
      "✅ [10/35] Variation 7/10 generated\n",
      "✅ [10/35] Variation 8/10 generated\n",
      "✅ [10/35] Variation 9/10 generated\n",
      "✅ [10/35] Variation 10/10 generated\n",
      "✅ [11/35] Variation 1/10 generated\n",
      "✅ [11/35] Variation 2/10 generated\n",
      "✅ [11/35] Variation 3/10 generated\n",
      "✅ [11/35] Variation 4/10 generated\n",
      "✅ [11/35] Variation 5/10 generated\n",
      "✅ [11/35] Variation 6/10 generated\n",
      "✅ [11/35] Variation 7/10 generated\n",
      "✅ [11/35] Variation 8/10 generated\n",
      "✅ [11/35] Variation 9/10 generated\n",
      "✅ [11/35] Variation 10/10 generated\n",
      "✅ [12/35] Variation 1/10 generated\n",
      "✅ [12/35] Variation 2/10 generated\n",
      "✅ [12/35] Variation 3/10 generated\n",
      "✅ [12/35] Variation 4/10 generated\n",
      "✅ [12/35] Variation 5/10 generated\n",
      "✅ [12/35] Variation 6/10 generated\n",
      "✅ [12/35] Variation 7/10 generated\n",
      "✅ [12/35] Variation 8/10 generated\n",
      "✅ [12/35] Variation 9/10 generated\n",
      "✅ [12/35] Variation 10/10 generated\n",
      "✅ [13/35] Variation 1/10 generated\n",
      "✅ [13/35] Variation 2/10 generated\n",
      "✅ [13/35] Variation 3/10 generated\n",
      "✅ [13/35] Variation 4/10 generated\n",
      "✅ [13/35] Variation 5/10 generated\n",
      "✅ [13/35] Variation 6/10 generated\n",
      "✅ [13/35] Variation 7/10 generated\n",
      "✅ [13/35] Variation 8/10 generated\n",
      "✅ [13/35] Variation 9/10 generated\n",
      "✅ [13/35] Variation 10/10 generated\n",
      "✅ [14/35] Variation 1/10 generated\n",
      "✅ [14/35] Variation 2/10 generated\n",
      "✅ [14/35] Variation 3/10 generated\n",
      "✅ [14/35] Variation 4/10 generated\n",
      "✅ [14/35] Variation 5/10 generated\n",
      "✅ [14/35] Variation 6/10 generated\n",
      "✅ [14/35] Variation 7/10 generated\n",
      "✅ [14/35] Variation 8/10 generated\n",
      "✅ [14/35] Variation 9/10 generated\n",
      "✅ [14/35] Variation 10/10 generated\n",
      "✅ [15/35] Variation 1/10 generated\n",
      "✅ [15/35] Variation 2/10 generated\n",
      "✅ [15/35] Variation 3/10 generated\n",
      "✅ [15/35] Variation 4/10 generated\n",
      "✅ [15/35] Variation 5/10 generated\n",
      "✅ [15/35] Variation 6/10 generated\n",
      "✅ [15/35] Variation 7/10 generated\n",
      "✅ [15/35] Variation 8/10 generated\n",
      "✅ [15/35] Variation 9/10 generated\n",
      "✅ [15/35] Variation 10/10 generated\n",
      "✅ [16/35] Variation 1/10 generated\n",
      "✅ [16/35] Variation 2/10 generated\n",
      "✅ [16/35] Variation 3/10 generated\n",
      "✅ [16/35] Variation 4/10 generated\n",
      "✅ [16/35] Variation 5/10 generated\n",
      "✅ [16/35] Variation 6/10 generated\n",
      "✅ [16/35] Variation 7/10 generated\n",
      "✅ [16/35] Variation 8/10 generated\n",
      "✅ [16/35] Variation 9/10 generated\n",
      "✅ [16/35] Variation 10/10 generated\n",
      "✅ [17/35] Variation 1/10 generated\n",
      "✅ [17/35] Variation 2/10 generated\n",
      "✅ [17/35] Variation 3/10 generated\n",
      "✅ [17/35] Variation 4/10 generated\n",
      "✅ [17/35] Variation 5/10 generated\n",
      "✅ [17/35] Variation 6/10 generated\n",
      "✅ [17/35] Variation 7/10 generated\n",
      "✅ [17/35] Variation 8/10 generated\n",
      "✅ [17/35] Variation 9/10 generated\n",
      "✅ [17/35] Variation 10/10 generated\n",
      "✅ [18/35] Variation 1/10 generated\n",
      "✅ [18/35] Variation 2/10 generated\n",
      "✅ [18/35] Variation 3/10 generated\n",
      "✅ [18/35] Variation 4/10 generated\n",
      "✅ [18/35] Variation 5/10 generated\n",
      "✅ [18/35] Variation 6/10 generated\n",
      "✅ [18/35] Variation 7/10 generated\n",
      "✅ [18/35] Variation 8/10 generated\n",
      "✅ [18/35] Variation 9/10 generated\n",
      "✅ [18/35] Variation 10/10 generated\n",
      "✅ [19/35] Variation 1/10 generated\n",
      "✅ [19/35] Variation 2/10 generated\n",
      "✅ [19/35] Variation 3/10 generated\n",
      "✅ [19/35] Variation 4/10 generated\n",
      "✅ [19/35] Variation 5/10 generated\n",
      "✅ [19/35] Variation 6/10 generated\n",
      "✅ [19/35] Variation 7/10 generated\n",
      "✅ [19/35] Variation 8/10 generated\n",
      "✅ [19/35] Variation 9/10 generated\n",
      "✅ [19/35] Variation 10/10 generated\n",
      "✅ [20/35] Variation 1/10 generated\n",
      "✅ [20/35] Variation 2/10 generated\n",
      "✅ [20/35] Variation 3/10 generated\n",
      "✅ [20/35] Variation 4/10 generated\n",
      "✅ [20/35] Variation 5/10 generated\n",
      "✅ [20/35] Variation 6/10 generated\n",
      "✅ [20/35] Variation 7/10 generated\n",
      "✅ [20/35] Variation 8/10 generated\n",
      "✅ [20/35] Variation 9/10 generated\n",
      "✅ [20/35] Variation 10/10 generated\n",
      "✅ [21/35] Variation 1/10 generated\n",
      "✅ [21/35] Variation 2/10 generated\n",
      "✅ [21/35] Variation 3/10 generated\n",
      "✅ [21/35] Variation 4/10 generated\n",
      "✅ [21/35] Variation 5/10 generated\n",
      "✅ [21/35] Variation 6/10 generated\n",
      "✅ [21/35] Variation 7/10 generated\n",
      "✅ [21/35] Variation 8/10 generated\n",
      "✅ [21/35] Variation 9/10 generated\n",
      "✅ [21/35] Variation 10/10 generated\n",
      "✅ [22/35] Variation 1/10 generated\n",
      "✅ [22/35] Variation 2/10 generated\n",
      "✅ [22/35] Variation 3/10 generated\n",
      "✅ [22/35] Variation 4/10 generated\n",
      "✅ [22/35] Variation 5/10 generated\n",
      "✅ [22/35] Variation 6/10 generated\n",
      "✅ [22/35] Variation 7/10 generated\n",
      "✅ [22/35] Variation 8/10 generated\n",
      "✅ [22/35] Variation 9/10 generated\n",
      "✅ [22/35] Variation 10/10 generated\n",
      "✅ [23/35] Variation 1/10 generated\n",
      "✅ [23/35] Variation 2/10 generated\n",
      "✅ [23/35] Variation 3/10 generated\n",
      "✅ [23/35] Variation 4/10 generated\n",
      "✅ [23/35] Variation 5/10 generated\n",
      "✅ [23/35] Variation 6/10 generated\n",
      "✅ [23/35] Variation 7/10 generated\n",
      "✅ [23/35] Variation 8/10 generated\n",
      "✅ [23/35] Variation 9/10 generated\n",
      "✅ [23/35] Variation 10/10 generated\n",
      "✅ [24/35] Variation 1/10 generated\n",
      "✅ [24/35] Variation 2/10 generated\n",
      "✅ [24/35] Variation 3/10 generated\n",
      "✅ [24/35] Variation 4/10 generated\n",
      "✅ [24/35] Variation 5/10 generated\n",
      "✅ [24/35] Variation 6/10 generated\n",
      "✅ [24/35] Variation 7/10 generated\n",
      "✅ [24/35] Variation 8/10 generated\n",
      "✅ [24/35] Variation 9/10 generated\n",
      "✅ [24/35] Variation 10/10 generated\n",
      "✅ [25/35] Variation 1/10 generated\n",
      "✅ [25/35] Variation 2/10 generated\n",
      "✅ [25/35] Variation 3/10 generated\n",
      "✅ [25/35] Variation 4/10 generated\n",
      "✅ [25/35] Variation 5/10 generated\n",
      "✅ [25/35] Variation 6/10 generated\n",
      "✅ [25/35] Variation 7/10 generated\n",
      "✅ [25/35] Variation 8/10 generated\n",
      "✅ [25/35] Variation 9/10 generated\n",
      "✅ [25/35] Variation 10/10 generated\n",
      "✅ [26/35] Variation 1/10 generated\n",
      "✅ [26/35] Variation 2/10 generated\n",
      "✅ [26/35] Variation 3/10 generated\n",
      "✅ [26/35] Variation 4/10 generated\n",
      "✅ [26/35] Variation 5/10 generated\n",
      "✅ [26/35] Variation 6/10 generated\n",
      "✅ [26/35] Variation 7/10 generated\n",
      "✅ [26/35] Variation 8/10 generated\n",
      "✅ [26/35] Variation 9/10 generated\n",
      "✅ [26/35] Variation 10/10 generated\n",
      "✅ [27/35] Variation 1/10 generated\n",
      "✅ [27/35] Variation 2/10 generated\n",
      "✅ [27/35] Variation 3/10 generated\n",
      "✅ [27/35] Variation 4/10 generated\n",
      "✅ [27/35] Variation 5/10 generated\n",
      "✅ [27/35] Variation 6/10 generated\n",
      "✅ [27/35] Variation 7/10 generated\n",
      "✅ [27/35] Variation 8/10 generated\n",
      "✅ [27/35] Variation 9/10 generated\n",
      "✅ [27/35] Variation 10/10 generated\n",
      "✅ [28/35] Variation 1/10 generated\n",
      "✅ [28/35] Variation 2/10 generated\n",
      "✅ [28/35] Variation 3/10 generated\n",
      "✅ [28/35] Variation 4/10 generated\n",
      "✅ [28/35] Variation 5/10 generated\n",
      "✅ [28/35] Variation 6/10 generated\n",
      "✅ [28/35] Variation 7/10 generated\n",
      "✅ [28/35] Variation 8/10 generated\n",
      "✅ [28/35] Variation 9/10 generated\n",
      "✅ [28/35] Variation 10/10 generated\n",
      "✅ [29/35] Variation 1/10 generated\n",
      "✅ [29/35] Variation 2/10 generated\n",
      "✅ [29/35] Variation 3/10 generated\n",
      "✅ [29/35] Variation 4/10 generated\n",
      "✅ [29/35] Variation 5/10 generated\n",
      "✅ [29/35] Variation 6/10 generated\n",
      "✅ [29/35] Variation 7/10 generated\n",
      "✅ [29/35] Variation 8/10 generated\n",
      "✅ [29/35] Variation 9/10 generated\n",
      "✅ [29/35] Variation 10/10 generated\n",
      "✅ [30/35] Variation 1/10 generated\n",
      "✅ [30/35] Variation 2/10 generated\n",
      "✅ [30/35] Variation 3/10 generated\n",
      "✅ [30/35] Variation 4/10 generated\n",
      "✅ [30/35] Variation 5/10 generated\n",
      "✅ [30/35] Variation 6/10 generated\n",
      "✅ [30/35] Variation 7/10 generated\n",
      "✅ [30/35] Variation 8/10 generated\n",
      "✅ [30/35] Variation 9/10 generated\n",
      "✅ [30/35] Variation 10/10 generated\n",
      "✅ [31/35] Variation 1/10 generated\n",
      "✅ [31/35] Variation 2/10 generated\n",
      "✅ [31/35] Variation 3/10 generated\n",
      "✅ [31/35] Variation 4/10 generated\n",
      "✅ [31/35] Variation 5/10 generated\n",
      "✅ [31/35] Variation 6/10 generated\n",
      "✅ [31/35] Variation 7/10 generated\n",
      "✅ [31/35] Variation 8/10 generated\n",
      "✅ [31/35] Variation 9/10 generated\n",
      "✅ [31/35] Variation 10/10 generated\n",
      "✅ [32/35] Variation 1/10 generated\n",
      "✅ [32/35] Variation 2/10 generated\n",
      "✅ [32/35] Variation 3/10 generated\n",
      "✅ [32/35] Variation 4/10 generated\n",
      "✅ [32/35] Variation 5/10 generated\n",
      "✅ [32/35] Variation 6/10 generated\n",
      "✅ [32/35] Variation 7/10 generated\n",
      "✅ [32/35] Variation 8/10 generated\n",
      "✅ [32/35] Variation 9/10 generated\n",
      "✅ [32/35] Variation 10/10 generated\n",
      "✅ [33/35] Variation 1/10 generated\n",
      "✅ [33/35] Variation 2/10 generated\n",
      "✅ [33/35] Variation 3/10 generated\n",
      "✅ [33/35] Variation 4/10 generated\n",
      "✅ [33/35] Variation 5/10 generated\n",
      "✅ [33/35] Variation 6/10 generated\n",
      "✅ [33/35] Variation 7/10 generated\n",
      "✅ [33/35] Variation 8/10 generated\n",
      "✅ [33/35] Variation 9/10 generated\n",
      "✅ [33/35] Variation 10/10 generated\n",
      "✅ [34/35] Variation 1/10 generated\n",
      "✅ [34/35] Variation 2/10 generated\n",
      "✅ [34/35] Variation 3/10 generated\n",
      "✅ [34/35] Variation 4/10 generated\n",
      "✅ [34/35] Variation 5/10 generated\n",
      "✅ [34/35] Variation 6/10 generated\n",
      "✅ [34/35] Variation 7/10 generated\n",
      "✅ [34/35] Variation 8/10 generated\n",
      "✅ [34/35] Variation 9/10 generated\n",
      "✅ [34/35] Variation 10/10 generated\n",
      "✅ [35/35] Variation 1/10 generated\n",
      "✅ [35/35] Variation 2/10 generated\n",
      "✅ [35/35] Variation 3/10 generated\n",
      "✅ [35/35] Variation 4/10 generated\n",
      "✅ [35/35] Variation 5/10 generated\n",
      "✅ [35/35] Variation 6/10 generated\n",
      "✅ [35/35] Variation 7/10 generated\n",
      "✅ [35/35] Variation 8/10 generated\n",
      "✅ [35/35] Variation 9/10 generated\n",
      "✅ [35/35] Variation 10/10 generated\n",
      "\n",
      "✅ All 350 variations saved to /tmp/medium@10_variations.yml\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import yaml\n",
    "from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "# === CONFIG ===\n",
    "code_file = \"/tmp/medium_codes.yml\"\n",
    "prompt_file = \"/tmp/medium_prompts.yml\"\n",
    "output_file = \"/tmp/medium@10_variations.yml\"\n",
    "num_variations_per_prompt = 10\n",
    "\n",
    "# === LOAD MODEL ===\n",
    "model_name = \"deepseek-ai/deepseek-coder-6.7b-base\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.float16,\n",
    "    trust_remote_code=True\n",
    ").to(\"cuda\")  # manually move to GPU 0\n",
    "\n",
    "print(\"✅ Using DeepSeek Coder 6.7B Base\")\n",
    "\n",
    "generator = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, device=0)\n",
    "\n",
    "# === LOAD INPUT DATA ===\n",
    "with open(code_file, \"r\") as f:\n",
    "    codes = yaml.safe_load(f)\n",
    "\n",
    "with open(prompt_file, \"r\") as f:\n",
    "    prompts = yaml.safe_load(f)\n",
    "\n",
    "assert len(codes) == len(prompts), \"❌ Code and prompt counts do not match!\"\n",
    "\n",
    "# === BUILD PROMPT BATCH ===\n",
    "batch_prompts = [\n",
    "    f\"### Base Verilog Code:\\n{c['code'].strip()}\\n### Variation Instruction:\\n{p['prompt'].strip()}\\n### Modified Code:\\n\"\n",
    "    for c, p in zip(codes, prompts)\n",
    "]\n",
    "\n",
    "# === RUN MULTIPLE VARIATIONS PER PROMPT ===\n",
    "outputs = []\n",
    "\n",
    "for i, prompt in enumerate(batch_prompts):\n",
    "    responses = generator(\n",
    "        prompt,\n",
    "        max_new_tokens=512,\n",
    "        do_sample=True,\n",
    "        temperature=0.8,\n",
    "        num_return_sequences=num_variations_per_prompt\n",
    "    )\n",
    "    for j, resp in enumerate(responses):\n",
    "        response = resp['generated_text']\n",
    "        variation = response.split(\"### Modified Code:\\n\")[-1].strip()\n",
    "        outputs.append({\"code\": variation})\n",
    "        print(f\"✅ [{i+1}/{len(batch_prompts)}] Variation {j+1}/{num_variations_per_prompt} generated\")\n",
    "\n",
    "# === SAVE OUTPUTS WITH BLOCK STYLE ===\n",
    "def str_presenter(dumper, data):\n",
    "    if '\\n' in data:\n",
    "        return dumper.represent_scalar('tag:yaml.org,2002:str', data, style='|')\n",
    "    return dumper.represent_scalar('tag:yaml.org,2002:str', data)\n",
    "\n",
    "yaml.add_representer(str, str_presenter)\n",
    "\n",
    "with open(output_file, \"w\") as f:\n",
    "    yaml.dump(outputs, f, sort_keys=False)\n",
    "\n",
    "print(f\"\\n✅ All {len(outputs)} variations saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "673bfe93",
   "metadata": {},
   "source": [
    "## Batch Inferencing (RTLCoder-DeepSeek-6.7B, complex@10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57b6eb56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d423db44a40243e7958c6a587bd46069",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Using DeepSeek Coder 6.7B Base\n",
      "✅ [1/30] Variation 1/10 generated\n",
      "✅ [1/30] Variation 2/10 generated\n",
      "✅ [1/30] Variation 3/10 generated\n",
      "✅ [1/30] Variation 4/10 generated\n",
      "✅ [1/30] Variation 5/10 generated\n",
      "✅ [1/30] Variation 6/10 generated\n",
      "✅ [1/30] Variation 7/10 generated\n",
      "✅ [1/30] Variation 8/10 generated\n",
      "✅ [1/30] Variation 9/10 generated\n",
      "✅ [1/30] Variation 10/10 generated\n",
      "✅ [2/30] Variation 1/10 generated\n",
      "✅ [2/30] Variation 2/10 generated\n",
      "✅ [2/30] Variation 3/10 generated\n",
      "✅ [2/30] Variation 4/10 generated\n",
      "✅ [2/30] Variation 5/10 generated\n",
      "✅ [2/30] Variation 6/10 generated\n",
      "✅ [2/30] Variation 7/10 generated\n",
      "✅ [2/30] Variation 8/10 generated\n",
      "✅ [2/30] Variation 9/10 generated\n",
      "✅ [2/30] Variation 10/10 generated\n",
      "✅ [3/30] Variation 1/10 generated\n",
      "✅ [3/30] Variation 2/10 generated\n",
      "✅ [3/30] Variation 3/10 generated\n",
      "✅ [3/30] Variation 4/10 generated\n",
      "✅ [3/30] Variation 5/10 generated\n",
      "✅ [3/30] Variation 6/10 generated\n",
      "✅ [3/30] Variation 7/10 generated\n",
      "✅ [3/30] Variation 8/10 generated\n",
      "✅ [3/30] Variation 9/10 generated\n",
      "✅ [3/30] Variation 10/10 generated\n",
      "✅ [4/30] Variation 1/10 generated\n",
      "✅ [4/30] Variation 2/10 generated\n",
      "✅ [4/30] Variation 3/10 generated\n",
      "✅ [4/30] Variation 4/10 generated\n",
      "✅ [4/30] Variation 5/10 generated\n",
      "✅ [4/30] Variation 6/10 generated\n",
      "✅ [4/30] Variation 7/10 generated\n",
      "✅ [4/30] Variation 8/10 generated\n",
      "✅ [4/30] Variation 9/10 generated\n",
      "✅ [4/30] Variation 10/10 generated\n",
      "✅ [5/30] Variation 1/10 generated\n",
      "✅ [5/30] Variation 2/10 generated\n",
      "✅ [5/30] Variation 3/10 generated\n",
      "✅ [5/30] Variation 4/10 generated\n",
      "✅ [5/30] Variation 5/10 generated\n",
      "✅ [5/30] Variation 6/10 generated\n",
      "✅ [5/30] Variation 7/10 generated\n",
      "✅ [5/30] Variation 8/10 generated\n",
      "✅ [5/30] Variation 9/10 generated\n",
      "✅ [5/30] Variation 10/10 generated\n",
      "✅ [6/30] Variation 1/10 generated\n",
      "✅ [6/30] Variation 2/10 generated\n",
      "✅ [6/30] Variation 3/10 generated\n",
      "✅ [6/30] Variation 4/10 generated\n",
      "✅ [6/30] Variation 5/10 generated\n",
      "✅ [6/30] Variation 6/10 generated\n",
      "✅ [6/30] Variation 7/10 generated\n",
      "✅ [6/30] Variation 8/10 generated\n",
      "✅ [6/30] Variation 9/10 generated\n",
      "✅ [6/30] Variation 10/10 generated\n",
      "✅ [7/30] Variation 1/10 generated\n",
      "✅ [7/30] Variation 2/10 generated\n",
      "✅ [7/30] Variation 3/10 generated\n",
      "✅ [7/30] Variation 4/10 generated\n",
      "✅ [7/30] Variation 5/10 generated\n",
      "✅ [7/30] Variation 6/10 generated\n",
      "✅ [7/30] Variation 7/10 generated\n",
      "✅ [7/30] Variation 8/10 generated\n",
      "✅ [7/30] Variation 9/10 generated\n",
      "✅ [7/30] Variation 10/10 generated\n",
      "✅ [8/30] Variation 1/10 generated\n",
      "✅ [8/30] Variation 2/10 generated\n",
      "✅ [8/30] Variation 3/10 generated\n",
      "✅ [8/30] Variation 4/10 generated\n",
      "✅ [8/30] Variation 5/10 generated\n",
      "✅ [8/30] Variation 6/10 generated\n",
      "✅ [8/30] Variation 7/10 generated\n",
      "✅ [8/30] Variation 8/10 generated\n",
      "✅ [8/30] Variation 9/10 generated\n",
      "✅ [8/30] Variation 10/10 generated\n",
      "✅ [9/30] Variation 1/10 generated\n",
      "✅ [9/30] Variation 2/10 generated\n",
      "✅ [9/30] Variation 3/10 generated\n",
      "✅ [9/30] Variation 4/10 generated\n",
      "✅ [9/30] Variation 5/10 generated\n",
      "✅ [9/30] Variation 6/10 generated\n",
      "✅ [9/30] Variation 7/10 generated\n",
      "✅ [9/30] Variation 8/10 generated\n",
      "✅ [9/30] Variation 9/10 generated\n",
      "✅ [9/30] Variation 10/10 generated\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ [10/30] Variation 1/10 generated\n",
      "✅ [10/30] Variation 2/10 generated\n",
      "✅ [10/30] Variation 3/10 generated\n",
      "✅ [10/30] Variation 4/10 generated\n",
      "✅ [10/30] Variation 5/10 generated\n",
      "✅ [10/30] Variation 6/10 generated\n",
      "✅ [10/30] Variation 7/10 generated\n",
      "✅ [10/30] Variation 8/10 generated\n",
      "✅ [10/30] Variation 9/10 generated\n",
      "✅ [10/30] Variation 10/10 generated\n",
      "✅ [11/30] Variation 1/10 generated\n",
      "✅ [11/30] Variation 2/10 generated\n",
      "✅ [11/30] Variation 3/10 generated\n",
      "✅ [11/30] Variation 4/10 generated\n",
      "✅ [11/30] Variation 5/10 generated\n",
      "✅ [11/30] Variation 6/10 generated\n",
      "✅ [11/30] Variation 7/10 generated\n",
      "✅ [11/30] Variation 8/10 generated\n",
      "✅ [11/30] Variation 9/10 generated\n",
      "✅ [11/30] Variation 10/10 generated\n",
      "✅ [12/30] Variation 1/10 generated\n",
      "✅ [12/30] Variation 2/10 generated\n",
      "✅ [12/30] Variation 3/10 generated\n",
      "✅ [12/30] Variation 4/10 generated\n",
      "✅ [12/30] Variation 5/10 generated\n",
      "✅ [12/30] Variation 6/10 generated\n",
      "✅ [12/30] Variation 7/10 generated\n",
      "✅ [12/30] Variation 8/10 generated\n",
      "✅ [12/30] Variation 9/10 generated\n",
      "✅ [12/30] Variation 10/10 generated\n",
      "✅ [13/30] Variation 1/10 generated\n",
      "✅ [13/30] Variation 2/10 generated\n",
      "✅ [13/30] Variation 3/10 generated\n",
      "✅ [13/30] Variation 4/10 generated\n",
      "✅ [13/30] Variation 5/10 generated\n",
      "✅ [13/30] Variation 6/10 generated\n",
      "✅ [13/30] Variation 7/10 generated\n",
      "✅ [13/30] Variation 8/10 generated\n",
      "✅ [13/30] Variation 9/10 generated\n",
      "✅ [13/30] Variation 10/10 generated\n",
      "✅ [14/30] Variation 1/10 generated\n",
      "✅ [14/30] Variation 2/10 generated\n",
      "✅ [14/30] Variation 3/10 generated\n",
      "✅ [14/30] Variation 4/10 generated\n",
      "✅ [14/30] Variation 5/10 generated\n",
      "✅ [14/30] Variation 6/10 generated\n",
      "✅ [14/30] Variation 7/10 generated\n",
      "✅ [14/30] Variation 8/10 generated\n",
      "✅ [14/30] Variation 9/10 generated\n",
      "✅ [14/30] Variation 10/10 generated\n",
      "✅ [15/30] Variation 1/10 generated\n",
      "✅ [15/30] Variation 2/10 generated\n",
      "✅ [15/30] Variation 3/10 generated\n",
      "✅ [15/30] Variation 4/10 generated\n",
      "✅ [15/30] Variation 5/10 generated\n",
      "✅ [15/30] Variation 6/10 generated\n",
      "✅ [15/30] Variation 7/10 generated\n",
      "✅ [15/30] Variation 8/10 generated\n",
      "✅ [15/30] Variation 9/10 generated\n",
      "✅ [15/30] Variation 10/10 generated\n",
      "✅ [16/30] Variation 1/10 generated\n",
      "✅ [16/30] Variation 2/10 generated\n",
      "✅ [16/30] Variation 3/10 generated\n",
      "✅ [16/30] Variation 4/10 generated\n",
      "✅ [16/30] Variation 5/10 generated\n",
      "✅ [16/30] Variation 6/10 generated\n",
      "✅ [16/30] Variation 7/10 generated\n",
      "✅ [16/30] Variation 8/10 generated\n",
      "✅ [16/30] Variation 9/10 generated\n",
      "✅ [16/30] Variation 10/10 generated\n",
      "✅ [17/30] Variation 1/10 generated\n",
      "✅ [17/30] Variation 2/10 generated\n",
      "✅ [17/30] Variation 3/10 generated\n",
      "✅ [17/30] Variation 4/10 generated\n",
      "✅ [17/30] Variation 5/10 generated\n",
      "✅ [17/30] Variation 6/10 generated\n",
      "✅ [17/30] Variation 7/10 generated\n",
      "✅ [17/30] Variation 8/10 generated\n",
      "✅ [17/30] Variation 9/10 generated\n",
      "✅ [17/30] Variation 10/10 generated\n",
      "✅ [18/30] Variation 1/10 generated\n",
      "✅ [18/30] Variation 2/10 generated\n",
      "✅ [18/30] Variation 3/10 generated\n",
      "✅ [18/30] Variation 4/10 generated\n",
      "✅ [18/30] Variation 5/10 generated\n",
      "✅ [18/30] Variation 6/10 generated\n",
      "✅ [18/30] Variation 7/10 generated\n",
      "✅ [18/30] Variation 8/10 generated\n",
      "✅ [18/30] Variation 9/10 generated\n",
      "✅ [18/30] Variation 10/10 generated\n",
      "✅ [19/30] Variation 1/10 generated\n",
      "✅ [19/30] Variation 2/10 generated\n",
      "✅ [19/30] Variation 3/10 generated\n",
      "✅ [19/30] Variation 4/10 generated\n",
      "✅ [19/30] Variation 5/10 generated\n",
      "✅ [19/30] Variation 6/10 generated\n",
      "✅ [19/30] Variation 7/10 generated\n",
      "✅ [19/30] Variation 8/10 generated\n",
      "✅ [19/30] Variation 9/10 generated\n",
      "✅ [19/30] Variation 10/10 generated\n",
      "✅ [20/30] Variation 1/10 generated\n",
      "✅ [20/30] Variation 2/10 generated\n",
      "✅ [20/30] Variation 3/10 generated\n",
      "✅ [20/30] Variation 4/10 generated\n",
      "✅ [20/30] Variation 5/10 generated\n",
      "✅ [20/30] Variation 6/10 generated\n",
      "✅ [20/30] Variation 7/10 generated\n",
      "✅ [20/30] Variation 8/10 generated\n",
      "✅ [20/30] Variation 9/10 generated\n",
      "✅ [20/30] Variation 10/10 generated\n",
      "✅ [21/30] Variation 1/10 generated\n",
      "✅ [21/30] Variation 2/10 generated\n",
      "✅ [21/30] Variation 3/10 generated\n",
      "✅ [21/30] Variation 4/10 generated\n",
      "✅ [21/30] Variation 5/10 generated\n",
      "✅ [21/30] Variation 6/10 generated\n",
      "✅ [21/30] Variation 7/10 generated\n",
      "✅ [21/30] Variation 8/10 generated\n",
      "✅ [21/30] Variation 9/10 generated\n",
      "✅ [21/30] Variation 10/10 generated\n",
      "✅ [22/30] Variation 1/10 generated\n",
      "✅ [22/30] Variation 2/10 generated\n",
      "✅ [22/30] Variation 3/10 generated\n",
      "✅ [22/30] Variation 4/10 generated\n",
      "✅ [22/30] Variation 5/10 generated\n",
      "✅ [22/30] Variation 6/10 generated\n",
      "✅ [22/30] Variation 7/10 generated\n",
      "✅ [22/30] Variation 8/10 generated\n",
      "✅ [22/30] Variation 9/10 generated\n",
      "✅ [22/30] Variation 10/10 generated\n",
      "✅ [23/30] Variation 1/10 generated\n",
      "✅ [23/30] Variation 2/10 generated\n",
      "✅ [23/30] Variation 3/10 generated\n",
      "✅ [23/30] Variation 4/10 generated\n",
      "✅ [23/30] Variation 5/10 generated\n",
      "✅ [23/30] Variation 6/10 generated\n",
      "✅ [23/30] Variation 7/10 generated\n",
      "✅ [23/30] Variation 8/10 generated\n",
      "✅ [23/30] Variation 9/10 generated\n",
      "✅ [23/30] Variation 10/10 generated\n",
      "✅ [24/30] Variation 1/10 generated\n",
      "✅ [24/30] Variation 2/10 generated\n",
      "✅ [24/30] Variation 3/10 generated\n",
      "✅ [24/30] Variation 4/10 generated\n",
      "✅ [24/30] Variation 5/10 generated\n",
      "✅ [24/30] Variation 6/10 generated\n",
      "✅ [24/30] Variation 7/10 generated\n",
      "✅ [24/30] Variation 8/10 generated\n",
      "✅ [24/30] Variation 9/10 generated\n",
      "✅ [24/30] Variation 10/10 generated\n",
      "✅ [25/30] Variation 1/10 generated\n",
      "✅ [25/30] Variation 2/10 generated\n",
      "✅ [25/30] Variation 3/10 generated\n",
      "✅ [25/30] Variation 4/10 generated\n",
      "✅ [25/30] Variation 5/10 generated\n",
      "✅ [25/30] Variation 6/10 generated\n",
      "✅ [25/30] Variation 7/10 generated\n",
      "✅ [25/30] Variation 8/10 generated\n",
      "✅ [25/30] Variation 9/10 generated\n",
      "✅ [25/30] Variation 10/10 generated\n",
      "✅ [26/30] Variation 1/10 generated\n",
      "✅ [26/30] Variation 2/10 generated\n",
      "✅ [26/30] Variation 3/10 generated\n",
      "✅ [26/30] Variation 4/10 generated\n",
      "✅ [26/30] Variation 5/10 generated\n",
      "✅ [26/30] Variation 6/10 generated\n",
      "✅ [26/30] Variation 7/10 generated\n",
      "✅ [26/30] Variation 8/10 generated\n",
      "✅ [26/30] Variation 9/10 generated\n",
      "✅ [26/30] Variation 10/10 generated\n",
      "✅ [27/30] Variation 1/10 generated\n",
      "✅ [27/30] Variation 2/10 generated\n",
      "✅ [27/30] Variation 3/10 generated\n",
      "✅ [27/30] Variation 4/10 generated\n",
      "✅ [27/30] Variation 5/10 generated\n",
      "✅ [27/30] Variation 6/10 generated\n",
      "✅ [27/30] Variation 7/10 generated\n",
      "✅ [27/30] Variation 8/10 generated\n",
      "✅ [27/30] Variation 9/10 generated\n",
      "✅ [27/30] Variation 10/10 generated\n",
      "✅ [28/30] Variation 1/10 generated\n",
      "✅ [28/30] Variation 2/10 generated\n",
      "✅ [28/30] Variation 3/10 generated\n",
      "✅ [28/30] Variation 4/10 generated\n",
      "✅ [28/30] Variation 5/10 generated\n",
      "✅ [28/30] Variation 6/10 generated\n",
      "✅ [28/30] Variation 7/10 generated\n",
      "✅ [28/30] Variation 8/10 generated\n",
      "✅ [28/30] Variation 9/10 generated\n",
      "✅ [28/30] Variation 10/10 generated\n",
      "✅ [29/30] Variation 1/10 generated\n",
      "✅ [29/30] Variation 2/10 generated\n",
      "✅ [29/30] Variation 3/10 generated\n",
      "✅ [29/30] Variation 4/10 generated\n",
      "✅ [29/30] Variation 5/10 generated\n",
      "✅ [29/30] Variation 6/10 generated\n",
      "✅ [29/30] Variation 7/10 generated\n",
      "✅ [29/30] Variation 8/10 generated\n",
      "✅ [29/30] Variation 9/10 generated\n",
      "✅ [29/30] Variation 10/10 generated\n",
      "✅ [30/30] Variation 1/10 generated\n",
      "✅ [30/30] Variation 2/10 generated\n",
      "✅ [30/30] Variation 3/10 generated\n",
      "✅ [30/30] Variation 4/10 generated\n",
      "✅ [30/30] Variation 5/10 generated\n",
      "✅ [30/30] Variation 6/10 generated\n",
      "✅ [30/30] Variation 7/10 generated\n",
      "✅ [30/30] Variation 8/10 generated\n",
      "✅ [30/30] Variation 9/10 generated\n",
      "✅ [30/30] Variation 10/10 generated\n",
      "\n",
      "✅ All 300 variations saved to /tmp/complex@10_variations.yml\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import yaml\n",
    "from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "# === CONFIG ===\n",
    "code_file = \"/tmp/complex_codes.yml\"\n",
    "prompt_file = \"/tmp/complex_prompts.yml\"\n",
    "output_file = \"/tmp/complex@10_variations.yml\"\n",
    "num_variations_per_prompt = 10\n",
    "\n",
    "# === LOAD MODEL ===\n",
    "model_name = \"deepseek-ai/deepseek-coder-6.7b-base\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.float16,\n",
    "    trust_remote_code=True\n",
    ").to(\"cuda\")  # manually move to GPU 0\n",
    "\n",
    "print(\"✅ Using DeepSeek Coder 6.7B Base\")\n",
    "\n",
    "generator = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, device=0)\n",
    "\n",
    "# === LOAD INPUT DATA ===\n",
    "with open(code_file, \"r\") as f:\n",
    "    codes = yaml.safe_load(f)\n",
    "\n",
    "with open(prompt_file, \"r\") as f:\n",
    "    prompts = yaml.safe_load(f)\n",
    "\n",
    "assert len(codes) == len(prompts), \"❌ Code and prompt counts do not match!\"\n",
    "\n",
    "# === BUILD PROMPT BATCH ===\n",
    "batch_prompts = [\n",
    "    f\"### Base Verilog Code:\\n{c['code'].strip()}\\n### Variation Instruction:\\n{p['prompt'].strip()}\\n### Modified Code:\\n\"\n",
    "    for c, p in zip(codes, prompts)\n",
    "]\n",
    "\n",
    "# === RUN MULTIPLE VARIATIONS PER PROMPT ===\n",
    "outputs = []\n",
    "\n",
    "for i, prompt in enumerate(batch_prompts):\n",
    "    responses = generator(\n",
    "        prompt,\n",
    "        max_new_tokens=512,\n",
    "        do_sample=True,\n",
    "        temperature=0.8,\n",
    "        num_return_sequences=num_variations_per_prompt\n",
    "    )\n",
    "    for j, resp in enumerate(responses):\n",
    "        response = resp['generated_text']\n",
    "        variation = response.split(\"### Modified Code:\\n\")[-1].strip()\n",
    "        outputs.append({\"code\": variation})\n",
    "        print(f\"✅ [{i+1}/{len(batch_prompts)}] Variation {j+1}/{num_variations_per_prompt} generated\")\n",
    "\n",
    "# === SAVE OUTPUTS WITH BLOCK STYLE ===\n",
    "def str_presenter(dumper, data):\n",
    "    if '\\n' in data:\n",
    "        return dumper.represent_scalar('tag:yaml.org,2002:str', data, style='|')\n",
    "    return dumper.represent_scalar('tag:yaml.org,2002:str', data)\n",
    "\n",
    "yaml.add_representer(str, str_presenter)\n",
    "\n",
    "with open(output_file, \"w\") as f:\n",
    "    yaml.dump(outputs, f, sort_keys=False)\n",
    "\n",
    "print(f\"\\n✅ All {len(outputs)} variations saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58f2150c",
   "metadata": {},
   "source": [
    "## Batch Inferencing (AutoVCoder, simple@1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7275f08e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "# === CONFIG ===\n",
    "code_file = \"/tmp/simple_codes.yml\"\n",
    "prompt_file = \"/tmp/simple_prompts.yml\"\n",
    "output_file = \"/tmp/simple@1_variations.yml\"\n",
    "num_variations_per_prompt = 1\n",
    "\n",
    "# === LOAD MODEL ===\n",
    "model_name = \"Bin12345/AutoCoder_S_6.7B\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.float16,\n",
    "    trust_remote_code=True\n",
    ").to(\"cuda\")\n",
    "\n",
    "print(\"✅ Using AutoVCoder Coder 6.7B Base\")\n",
    "\n",
    "generator = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, device=0)\n",
    "\n",
    "# === LOAD INPUT DATA ===\n",
    "with open(code_file, \"r\") as f:\n",
    "    codes = yaml.safe_load(f)\n",
    "\n",
    "with open(prompt_file, \"r\") as f:\n",
    "    prompts = yaml.safe_load(f)\n",
    "\n",
    "assert len(codes) == len(prompts), \"❌ Code and prompt counts do not match!\"\n",
    "\n",
    "# === BUILD PROMPT BATCH ===\n",
    "batch_prompts = [\n",
    "    f\"### Base Verilog Code:\\n{c['code'].strip()}\\n### Variation Instruction:\\n{p['prompt'].strip()}\\n### Modified Code:\\n\"\n",
    "    for c, p in zip(codes, prompts)\n",
    "]\n",
    "\n",
    "# === RUN MULTIPLE VARIATIONS PER PROMPT ===\n",
    "outputs = []\n",
    "\n",
    "for i, prompt in enumerate(batch_prompts):\n",
    "    responses = generator(\n",
    "        prompt,\n",
    "        max_new_tokens=2048,\n",
    "        do_sample=True,\n",
    "        temperature=0.8,\n",
    "        num_return_sequences=num_variations_per_prompt\n",
    "    )\n",
    "    for j, resp in enumerate(responses):\n",
    "        response = resp['generated_text']\n",
    "        variation = response.split(\"### Modified Code:\\n\")[-1].strip()\n",
    "        outputs.append({\"code\": variation})\n",
    "        print(f\"✅ [{i+1}/{len(batch_prompts)}] Variation {j+1}/{num_variations_per_prompt} generated\")\n",
    "\n",
    "# === SAVE OUTPUTS WITH BLOCK STYLE ===\n",
    "def str_presenter(dumper, data):\n",
    "    if '\\n' in data:\n",
    "        return dumper.represent_scalar('tag:yaml.org,2002:str', data, style='|')\n",
    "    return dumper.represent_scalar('tag:yaml.org,2002:str', data)\n",
    "\n",
    "yaml.add_representer(str, str_presenter)\n",
    "\n",
    "with open(output_file, \"w\") as f:\n",
    "    yaml.dump(outputs, f, sort_keys=False)\n",
    "\n",
    "print(f\"\\n✅ All {len(outputs)} variations saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07a992e5",
   "metadata": {},
   "source": [
    "## Batch Inferencing (AutoVCoder, medium@1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6293d802",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "# === CONFIG ===\n",
    "code_file = \"/tmp/medium_codes.yml\"\n",
    "prompt_file = \"/tmp/medium_prompts.yml\"\n",
    "output_file = \"/tmp/medium@1_variations.yml\"\n",
    "num_variations_per_prompt = 1\n",
    "\n",
    "# === LOAD MODEL ===\n",
    "model_name = \"Bin12345/AutoCoder_S_6.7B\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.float16,\n",
    "    trust_remote_code=True\n",
    ").to(\"cuda\")\n",
    "\n",
    "print(\"✅ Using AutoVCoder Coder 6.7B Base\")\n",
    "\n",
    "generator = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, device=0)\n",
    "\n",
    "# === LOAD INPUT DATA ===\n",
    "with open(code_file, \"r\") as f:\n",
    "    codes = yaml.safe_load(f)\n",
    "\n",
    "with open(prompt_file, \"r\") as f:\n",
    "    prompts = yaml.safe_load(f)\n",
    "\n",
    "assert len(codes) == len(prompts), \"❌ Code and prompt counts do not match!\"\n",
    "\n",
    "# === BUILD PROMPT BATCH ===\n",
    "batch_prompts = [\n",
    "    f\"### Base Verilog Code:\\n{c['code'].strip()}\\n### Variation Instruction:\\n{p['prompt'].strip()}\\n### Modified Code:\\n\"\n",
    "    for c, p in zip(codes, prompts)\n",
    "]\n",
    "\n",
    "# === RUN MULTIPLE VARIATIONS PER PROMPT ===\n",
    "outputs = []\n",
    "\n",
    "for i, prompt in enumerate(batch_prompts):\n",
    "    responses = generator(\n",
    "        prompt,\n",
    "        max_new_tokens=2048,\n",
    "        do_sample=True,\n",
    "        temperature=0.8,\n",
    "        num_return_sequences=num_variations_per_prompt\n",
    "    )\n",
    "    for j, resp in enumerate(responses):\n",
    "        response = resp['generated_text']\n",
    "        variation = response.split(\"### Modified Code:\\n\")[-1].strip()\n",
    "        outputs.append({\"code\": variation})\n",
    "        print(f\"✅ [{i+1}/{len(batch_prompts)}] Variation {j+1}/{num_variations_per_prompt} generated\")\n",
    "\n",
    "# === SAVE OUTPUTS WITH BLOCK STYLE ===\n",
    "def str_presenter(dumper, data):\n",
    "    if '\\n' in data:\n",
    "        return dumper.represent_scalar('tag:yaml.org,2002:str', data, style='|')\n",
    "    return dumper.represent_scalar('tag:yaml.org,2002:str', data)\n",
    "\n",
    "yaml.add_representer(str, str_presenter)\n",
    "\n",
    "with open(output_file, \"w\") as f:\n",
    "    yaml.dump(outputs, f, sort_keys=False)\n",
    "\n",
    "print(f\"\\n✅ All {len(outputs)} variations saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f7e8d32",
   "metadata": {},
   "source": [
    "## Batch Inferencing (AutoVCoder, complex@1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "953aba2a",
   "metadata": {},
   "source": [
    "## Batch Inferencing (AutoVCoder, simple@5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd9d4319",
   "metadata": {},
   "source": [
    "## Batch Inferencing (AutoVCoder, medium@5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1820d9bf",
   "metadata": {},
   "source": [
    "## Batch Inferencing (AutoVCoder, complex@5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9719d6b2",
   "metadata": {},
   "source": [
    "## Batch Inferencing (AutoVCoder, simple@10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4000c33",
   "metadata": {},
   "source": [
    "## Batch Inferencing (AutoVCoder, medium@10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d1c94aa",
   "metadata": {},
   "source": [
    "## Batch Inferencing (AutoVCoder, complex@10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "def8207a",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fypvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
